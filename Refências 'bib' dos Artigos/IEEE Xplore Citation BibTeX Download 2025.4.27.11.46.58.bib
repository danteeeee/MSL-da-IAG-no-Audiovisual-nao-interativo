@INPROCEEDINGS{10445552,
  author={Krome, Niklas and Kopp, Stefan},
  booktitle={2024 IEEE International Conference on Artificial Intelligence and eXtended and Virtual Reality (AIxVR)}, 
  title={Minimal Latency Speech-Driven Gesture Generation for Continuous Interaction in Social XR}, 
  year={2024},
  volume={},
  number={},
  pages={236-240},
  abstract={Social XR applications usually require advanced tracking equipment to control one’s own avatar. We explore if AI-based co-speech gesture generation techniques can be employed to compensate for the lack of tracking hardware that many users face. One main challenge is to achieve convincing behavior quality without introducing too much latency. Previous work has shown that both depend – in opposite ways – on the length of the audio chunk the gestures are generated from, and that gesture quality of existing models declines with lower chunk sizes while still not reaching sufficiently low latency to enable fluent interaction. In this paper we present an approach that is able to generate continuous gesture trajectories frame by frame, minimizing latency and yielding delays well below buffer sizes of voice communication systems or video calls. A project page with videos of the generated gestures is available at https://nkrome.github.io/FrameCAGE.html.},
  keywords={Solid modeling;Webcams;Streaming media;Real-time systems;Behavioral sciences;Trajectory;X reality;extended reality;social interaction;animation;machine learning;gesture generation},
  doi={10.1109/AIxVR59861.2024.00038},
  ISSN={2771-7453},
  month={Jan},}@ARTICLE{10565871,
  author={Hu, Bin and Guo, Hui and Yang, Yuling and Tao, Xiongjie and He, Jie},
  journal={IEEE Access}, 
  title={Lightweight Physics-Based Character for Generating Sensible Postures in Dynamic Environments}, 
  year={2024},
  volume={12},
  number={},
  pages={89660-89678},
  abstract={Pose change and environmental interaction have been the primary directions of recent research in the field of physics-based characters. To address the problems of a high production threshold, high real-time arithmetic consumption when combined with deep reinforcement learning, and a lack of realism in existing solutions, in this paper, a real-time automatic motion control model is designed for virtual characters that provides posture control through physical response linkages and that can generate smooth and natural interactions with the environment through small amounts of body adjustments. The proposed model consists of a mixture of kinematic controllers and lightweight physical interaction modules. It uses a designed posture correction module to correct the posture of a hybrid model, automatically adjusting the body posture, and it can respond reasonably to the environment when the action generation encounters environmental obstacles. The designed constraint scheme can reduce the tuning of joint parameters while allowing a character to have realistic human-like motion perception. The proposed model is verified by experiments, and the experimental results demonstrate that the proposed virtual human model can be adjusted to achieve the human-like effect using real action data and keyframe animation. Ball bouncing and walking on irregular terrain experiments verify that the proposed virtual character model can interact with the environment effectively in real-time. As the carrier of character animation, the proposed driving framework model can rapidly and easily generate interactive motions based on given input parameters.},
  keywords={Kinematics;Animation;PD control;Real-time systems;Data models;Computational modeling;Heuristic algorithms;Physics-based character;inverse kinematics;hybrid modeling;joint restraint},
  doi={10.1109/ACCESS.2024.3417220},
  ISSN={2169-3536},
  month={},}@ARTICLE{9098914,
  author={Tian, Lei and Song, Limei and Zheng, Yu and Wang, Jinhai and Chen, Hongli},
  journal={IEEE Transactions on Magnetics}, 
  title={An Improved F/C Structure for Cell-Scale Micro-Magnetic Coil}, 
  year={2020},
  volume={56},
  number={8},
  pages={1-8},
  abstract={Cell-scale micro-magnetic stimulation is the process of stimulating neuronal tissues using a sub-millimeter coil. During the process, a time-varying current is fed to the micro-coil, and the micro-coil generates a dispersed magnetic field in the focal region of the tissue to create the effect of magnetic stimulation. The micro-magnetic coil has the drawbacks of small inductance, large power consumption, low-quality factor, and uneven distribution of magnetic induction. In this article, we designed an improved F/C structure, which was surrounded by a magnetic film/planar coil, and developed a method for determining the geometric parameters of the structure based on an investigation of how the pattern, thickness, spacing, and width of the magnetic shielding layer (MSL) affect the micro-coil inductance  $L$  and the magnetic induction  $B$ . The experimental results show: when the magnetic permeability  $\mu _{r}$  of the micro-magnetic coil with the improved F/C structure is 106 H/m, the inductance reaches 1149.3 nH, the maximum value of magnetic induction  $B$  on the  $Z = 800$  nm tangential surface reaches 11.33 mT, and the average value of  $B$  is 5.5 mT (the  $B$  value exceeds 4.28 mT in 92.6% of the area of the  $100\,\,\mu \text{m}\,\,\times 100\,\,\mu \text{m}$  micro-coil); the range of action of magnetic induction is approximately  $20~\mu \text{m}$  in the  $Z$ -direction. It can be concluded that the micro-magnetic coil with the improved F/C structure is superior to the existing micro-coils in terms of magnetic field uniformity, action strength, and inductance value, and the increased inductance value improves the quality factor  $Q$  of the coil and reduces the power consumption of the micro-magnetic coil.},
  keywords={Inductance;Magnetic films;Soft magnetic materials;Magnetic flux;Magnetic fields;Spirals;Inductors;F/C structure;magnetic shielding layer (MSL);micro-magnetic coil;uniform magnetic field},
  doi={10.1109/TMAG.2020.2996955},
  ISSN={1941-0069},
  month={Aug},}@INPROCEEDINGS{10128248,
  author={Vatsa, Arjit and Agarwal, Sanskar and Ranjani, M},
  booktitle={2023 International Conference on Computer Communication and Informatics (ICCCI)}, 
  title={Lane Coloration in Real Time Vehicular Ad-hoc Networks (VANETS)}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={With the advent of emerging technologies, it is important for every aspect of living to become fast-paced and safer. Working in correspondence to above idea, we are creating a system of Road Lane coloration and detection suitable for Indian Roads. For this we will be applying various ideas such as gamma - correction (for darker roads), addition of Kalman Filter, Canny Edge Detector (for edge detection)and DBSCAN (to cluster the detected lines). AI aspects have made it possible to build self-sufficient vehicles and GPS, but if there exists a system by which we can make this lane detection and self-driving car scenarios safer, wouldn’t it be better? Since our goal is to provide self-driving technology via a mobile device, we were looking for a solution that would be less computationally intensive and operate in real time from a phone. It was difficult for us to recognize road lanes during the day, so we started doing it at night. We are trying to solve this issue with the addition algorithms and by removal of as much noise. For our system, White Gaussian noise is used. We have estimated the state’s error using this noise model, and then we utilized the estimator to get the anticipated state using the measurement noise. The Kalman filter averages out variations in lines that are detected along the lanes by summing up measurement error and prior state. Since identified lane marker lines have a predictive feature from a prior state, they remain stable over time. Additionally, even under extremely low light conditions, it is still able to detect lanes by recalling previously recognized lanes from a previous video frame. We have begun to work on multi-lane detection, tracking, approximating road lane width, etc. You could soon be able to use your phone to access a high-end perception system.},
  keywords={Rain;Roads;Image edge detection;Vehicular ad hoc networks;Streaming media;Real-time systems;Reflection;Canny Edge Detection;Kalman Filter;Lane Coloration;Deep Learning;Indian Roads;DBSCAN},
  doi={10.1109/ICCCI56745.2023.10128248},
  ISSN={2473-7577},
  month={Jan},}@INPROCEEDINGS{10271800,
  author={Berthault, Alexandre and Kato, Takuma and Shirai, Akihiko},
  booktitle={2023 IEEE International Conference on Metaverse Computing, Networking and Applications (MetaCom)}, 
  title={Avatar Fusion Karaoke: Research and development on multi-user music play VR experience in the metaverse}, 
  year={2023},
  volume={},
  number={},
  pages={281-289},
  abstract={This paper contributes to building a standard process of research and development (R&D) for new user experiences (UX) in metaverse services. We tested this R&D process on a new UX proof of concept (PoC) for Meta Quest head-mounted display (HMDs) consisting of a school-life karaoke experience with the hypothesis that it is possible to design the avatars with only the necessary functions and rendering costs. The school life metaverse is a relevant subject for discovering issues and problems in this type of simultaneous connection. To qualitatively evaluate the potential of a multi-person metaverse experience, this study investigated subjects where each avatar requires expressive skills. While avatar play experiences feature artistic expressions, such as dancing, playing musical instruments, and drawing, and these can be used to evaluate their operability and expressive capabilities qualitatively, the Quest’s tracking capabilities are insufficient for full-body performance and graphical art expression. Considering such hardware limitations, this study evaluated the Quest, focusing primarily on UX simplicity using AI Fusion techniques and expressiveness in instrumental scenes played by approximately four avatars. To achieve our PoC, we first researched the limitations and challenges of such applications. In Meta Quest 1 and Meta Quest 2, the benchmark for simultaneous avatar rendering was 10 for Quest 1 and 22 for the Quest 2. The appropriate interpersonal distance (IPD) should be designed with culture in mind. We assume that various avatars communicate at a maximum distance of 1–2 meters while generating collisions for individual avatars and avoiding overlap. Considering the interference in stereophonic sound, we consider that 5–11 avatars can be displayed simultaneously while maintaining quality communication. Through our standard R&D process, we assessed the production needs for our PoC and obtained multiple ways to retrieve feedback. A notable effect was observed when other users entered the same session. When users waved and called out to one another (saluted), their participation time increased by 3 minutes (i.e., they played more than one piece of music). Most users did not perceive the AI-only piano-playing avatar as human (i.e., they perceived it as a non-player character [NPC] in the game). This research reported methods for multi-user metaverse communication and its supporting technologies, such as head-mounted devices and their graphics performance, special interaction techniques, and complementary tools and the importance of PoC development, its evaluation, and its iterations. The result is remarkable for further research; these expressive technologies in a multi-user context are directly related to the quality of communication within the metaverse and the value of the user-generated content (UGC) produced there.},
  keywords={Performance evaluation;Meters;Metaverse;Avatars;User-generated content;Music;Production;Metaverse;VR;Karaoke;Avatar;Real-time},
  doi={10.1109/MetaCom57706.2023.00058},
  ISSN={},
  month={June},}@INPROCEEDINGS{9059342,
  author={Cheng, Carl and Shen, Yu and Liu, Patrick},
  booktitle={2019 IEEE 16th International Conference on Mobile Ad Hoc and Sensor Systems Workshops (MASSW)}, 
  title={Optimizing Traditional Transforms for Vehicular Network Sign Classification}, 
  year={2019},
  volume={},
  number={},
  pages={127-132},
  abstract={In most cases, data is largely the limiting factor as to why a model is unable to perform well, either due to its quality or its amount. Traditional data augmentation is the by far the easiest and generally an efficient method to solve the issue of bad data, as it artificially manipulates the data in such a way that allows the model to generalize better. In this paper, we compare multiple forms of traditional data augmentation in order to see which traditional transforms have the largest impact on sign classification, and the extent to which a model can be improved using traditional transforms alone. Various combinations of transforms were applied to the GTSRB dataset, and evaluated by a CNN of ResNet-34 architecture. Via altering traditional transforms alone, the best combination of transforms achieved a 99.09% test set accuracy, a 0.62% improvement from the 98.47% accuracy of the control combination containing default transforms.},
  keywords={Transforms;Training;Machine learning;Data models;Convolutional neural networks;Gallium nitride;Artificial Intelligence;Autonomous Automobiles;Data Analysis;Convolutional Neural Networks;Big Data},
  doi={10.1109/MASSW.2019.00033},
  ISSN={},
  month={Nov},}@ARTICLE{8727523,
  author={Zhang, Min and Dong, Chao and Huang, Yang},
  journal={IEEE Access}, 
  title={FS-MAC: An Adaptive MAC Protocol With Fault-Tolerant Synchronous Switching for FANETs}, 
  year={2019},
  volume={7},
  number={},
  pages={80602-80613},
  abstract={Medium access control (MAC) is significant for guaranteeing the quality of service of Flying Ad-hoc NETworks (FANETs). The adaptive MAC protocol is recognized as a promising solution, which is able to improve the flexibility and robustness of FANETs. In this paper, we propose a fault-tolerant synchronous-MAC (FS-MAC) protocol that can switch between CSMA/CA and TDMA protocols for the FANETs. In FS-MAC, we propose a distributed Q-learning-based MAC switching scheme which contains a MAC pre-selection operation and a practical byzantine fault tolerance (PBFT)-based consensus decision procedure to produce a MAC switching decision. By the MAC pre-selection operation, each UAV can evaluate its own performance accurately and determine which MAC protocol is more appropriate. Then, all UAVs in FANETs can implement fault-tolerant synchronous switching with the help of the PBFT-based consensus decision procedure. The simulations are conducted to evaluate the various performance of the FS-MAC. It is shown that FS-MAC can significantly outperform the baseline protocols in terms of the average throughput, delay, and packet retransmission ratio performance.},
  keywords={Media Access Protocol;Switches;Ad hoc networks;Time division multiple access;Fault tolerance;Fault tolerant systems;Adaptive MAC protocol;consensus algorithm;Q-learning;FANETs},
  doi={10.1109/ACCESS.2019.2920175},
  ISSN={2169-3536},
  month={},}@ARTICLE{10632127,
  author={Salam, Mustafa Abdul and Aldawsari, Mohamed and Gamal, Mostafa and Hamed, Hesham F. A. and Sweidan, Sara},
  journal={IEEE Access}, 
  title={MSG-ATS: Multi-Level Semantic Graph for Arabic Text Summarization}, 
  year={2024},
  volume={12},
  number={},
  pages={118773-118784},
  abstract={Arabic language processing presents significant challenges due to its complex linguistic patterns and shortage of resources. This study describes MSG-ATS, a new technique to abstractive text summarization in Arabic that aims to overcome these issues. The key challenge is producing coherent and high-quality summaries given the Arabic language’s rich syntactic, semantic, and contextual elements. Traditional approaches, such as word2vec, frequently fail to capture these subtleties well. MSG-ATS uses multilevel semantic graphs and deep learning techniques to create a more thorough representation of Arabic text. This approach improves traditional text generation and embedding approaches by collecting syntactic, semantic, and contextual information fully. MSG-ATS uses a deep neural network to create high-quality summaries that are coherent and contextually appropriate. To verify MSG-ATS, we performed rigorous assessments that compared its performance to word2vec, a fundamental word embedding approach. These assessments employed a unique dataset created expressly for this study and included automated assessment using the ROUGE measure. The results are compelling: MSG-ATS outperformed the baseline model by 42.4% in precision, 23.8% in recall, and 38.3% overall. The outcomes of this study highlight MSG-ATS’s potential to considerably increase Arabic text summarization by providing a strong framework that solves the constraints of existing models while also laying the groundwork for future developments in the area.},
  keywords={Semantics;Text summarization;Measurement;Syntactics;Reviews;Long short term memory;Deep learning;Automation;Graph neural networks;Automatic text summarization;multi-level semantic graph;semantic graph embedding;graph neural networks;attention mechanisms},
  doi={10.1109/ACCESS.2024.3441489},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10544050,
  author={Selvaraj, Ram and Singh, Ayush and Kameel, Shafiudeen and Samal, Rahul and Agarwal, Pooja},
  booktitle={2024 IEEE 9th International Conference for Convergence in Technology (I2CT)}, 
  title={Vidgen: Long-Form Text-to-Video Generation with Temporal, Narrative and Visual Consistency for High Quality Story-Visualisation Tasks}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Generating long-form videos conditioned on large story based text input is a new and relatively unexplored task. Current text-to-video models are designed to generate short video clips conditioned on small input texts, which while temporally consistent, are severely lacking in narrative and visual consistency which are key elements of good story visualisation. This paper presents Vidgen, a pipeline that ensures Long-Form Text-to-Video Generation with Temporal, Narrative and Visual Consistency. To address the above issues Vidgen employs a new approach which leverages a Large-Language- Model (LLM) for the pre-processing of input story/script, which extracts key actions and story elements from the given script, and converts it to a standardised format which will be read and understood by the text-to-video model. The paper also proposes fixing "embeddings" (spatial-information about the appearance of the character) using a new, faster and improved textual- inversion approach with pre-trained weights, to ensure consistent looking characters. The work done in this paper will allow any traditional text-to-video model to accept large input text in the form of a story and generate high quality, temporally consistent arbitrary-length videos that have consistent looking characters for the entire duration of the video.},
  keywords={Training;Visualization;Generative AI;Pipelines;Entertainment industry;Memory modules;Generators;Generative Artificial Intelligence;Latent Diffusion Models;Large Language Models;Text-to-Video Generation;Textual Inversion;Story Visualisation},
  doi={10.1109/I2CT61223.2024.10544050},
  ISSN={},
  month={April},}@INPROCEEDINGS{10645358,
  author={Wang, Yuchen and Lyu, Ruimin},
  booktitle={2024 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)}, 
  title={Characteristics of Visual Complexity: Calligraphic Fonts vs. Printed Fonts}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The Chinese characters possess diverse calligraphic styles and corresponding calligraphers. These styles are evidently manifested on a visual level, stemming from certain visual disparities between calligraphic characters and common characters. We aim to explore the specific visual disparities that distinguish calligraphic characters from standard ones, endowing them with higher levels of recognition difficulty and artistic value. This can help evaluate to what extent AI-generated calligraphic characters match a specific calligraphy style. To achieve this, we leveraged research findings on visual complexity to measure various aspects of complexity for a certain number of calligraphic characters and printed characters. We found that the significant differences in visual complexity between the two lie in perimeter complexity, terminations, and intersections. Based on the findings, we evaluated the quality of calligraphic characters generated by Midjourney. They basically possess the common characteristics of calligraphic characters, but do not quite match the specified calligraphic style.},
  keywords={Visualization;Multimedia systems;Conferences;Market research;Skeleton;Complexity theory;Character recognition;Visual complexity;writing systems;Chinese calligraphic;AI calligraphic},
  doi={10.1109/ICMEW63481.2024.10645358},
  ISSN={2995-1429},
  month={July},}@ARTICLE{9819910,
  author={Rehman, Abdul and Hassan, Mohd Fadzil and Hooi, Yew Kwang and Qureshi, Muhammad Aasim and Shukla, Saurabh and Susanto, Erwin and Rubab, Saddaf and Abdel-Aty, Abdel-Haleem},
  journal={IEEE Access}, 
  title={CTMF: Context-Aware Trust Management Framework for Internet of Vehicles}, 
  year={2022},
  volume={10},
  number={},
  pages={73685-73701},
  abstract={Secure communication is the top concern of the Internet of Vehicles (IoV). The trust between nodes can have a considerable impact on ensuring IoV security. Therefore, the trustworthiness of a received message must be evaluated before acting upon it. A malicious node can broadcast bogus events to obtain network control. False reports and malicious vehicles render the network unreliable during emergencies. In this study, a unique trust framework is presented that considers most of the aspects of trust in IoV to accurately identify malicious nodes and events. Previous studies have proposed some trust models for VANETs, which have many deficiencies in serving IoV. In particular, they lack dynamism and practical implementations. All the existing models have two things in common, first they work on fixed parameters, and second, they use static scenarios. In contrast, the proposed framework is based on a context-awareness cognitive approach with artificial intelligence (AI) properties. The framework cognitively learns the environment from the received report and creates a context around an event. In addition to trust management (TM), the proposed framework offers a novel process for detecting and screening malicious nodes using anomaly outliers. The performance of the framework was examined using an experimental simulation. The proposed framework was compared with top benchmarks in the field. The results show inclining performance indicators. The proposed trust-management framework has the potential to serve as a component of IoV security.},
  keywords={Security;Context modeling;Data models;Blockchains;Vehicular ad hoc networks;Trust management;Computational modeling;Internet of Vehicles (IoV);trust management (TM);vehicular ad hoc network (VANET);context awareness},
  doi={10.1109/ACCESS.2022.3189349},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9466022,
  author={Hossain, Md. Iqbal and Rahman, Maqsudur and Ahmed, Md. Tofael and Rahman, Md. Saifur and Islam, A Z M Touhidul},
  booktitle={2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS)}, 
  title={Rating Prediction of Product Reviews of Bangla Language using Machine Learning Algorithms}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={The only way to provide feedback about a product is through reviews. When a new shopper proceeds to an online shop to purchase a product but does not have adequate time to study the reviews provided by other shoppers to get an opinion about the product, the shopper determines whether to buy the product or not on the number rating. Through reviews, shoppers can acquaint everyone about the product's quality and the manufacturer can enhance their products and business by interpreting that review. However, manufacturers demand a number review more than a text review for business analysis. This paper represents a machine learning based model for predicting the number rating from written text for Bangla product review. This study performs on a dataset collected manually from Daraz.com.bd, a Bangladeshi leading e-commerce shop. We have implemented Support Vector Machine (SVM), Random Forest, XGBoost, and Logistic Regression with Term Frequency-Inverse Document Frequency (TF-IDF) Vectorizer on our collected dataset and record all the performance metrics like accuracy, precision, recall and f1-score. From these above four algorithms, SVM showed more outstanding results than others in terms of performance metrics. SVM achieved 90% accuracy on the applied dataset. The other SVM performance metrics are 0.90, 0.92, and 0.91 for precision, recall and f1-score respectively.},
  keywords={Support vector machines;Measurement;Machine learning algorithms;Mechatronics;Predictive models;Prediction algorithms;Product design;Bangla Rating Prediction;Machine Learning;SVM;TF-IDF;Reviews Rating Prediction},
  doi={10.1109/AIMS52415.2021.9466022},
  ISSN={},
  month={April},}@INPROCEEDINGS{9156871,
  author={Chen, Zhuo and Wang, Chaoyue and Yuan, Bo and Tao, Dacheng},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={PuppeteerGAN: Arbitrary Portrait Animation With Semantic-Aware Appearance Transformation}, 
  year={2020},
  volume={},
  number={},
  pages={13515-13524},
  abstract={Portrait animation, which aims to animate a still portrait to life using poses extracted from target frames, is an important technique for many real-world entertainment applications. Although recent works have achieved highly realistic results on synthesizing or controlling human head images, the puppeteering of arbitrary portraits is still confronted by the following challenges: 1) identity/personality mismatch; 2) training data/domain limitations; and 3) low-efficiency in training/fine-tuning. In this paper, we devised a novel two-stage framework called PuppeteerGAN for solving these challenges. Specifically, we first learn identity-preserved semantic segmentation animation which executes pose retargeting between any portraits. As a general representation, the semantic segmentation results could be adapted to different datasets, environmental conditions or appearance domains. Furthermore, the synthesized semantic segmentation is filled with the appearance of the source portrait. To this end, an appearance transformation network is presented to produce fidelity output by jointly considering the wrapping of semantic features and conditional generation. After training, the two networks can directly perform end-to-end inference on unseen subjects without any retraining or fine-tuning. Extensive experiments on cross-identity/domain/resolution situations demonstrate the superiority of the proposed PuppetterGAN over existing portrait animation methods in both generation quality and inference speed.},
  keywords={Training;Animation;Face;Semantics;Videos;Strain;Image segmentation},
  doi={10.1109/CVPR42600.2020.01353},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{9644322,
  author={Chang, Miao-Chi Liu and Huang, Yu-Hsiung and Lin, Wei-Chih and Sun, Shih-Wei},
  booktitle={2021 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)}, 
  title={Digital Fabrication: Machine Learning-based Immersive Experiencing for the Virtual Space in a Future Museum}, 
  year={2021},
  volume={},
  number={},
  pages={102-105},
  abstract={In this paper, “Digital Fabrication” artwork is developed to be exhibited in a future museum. According to a built-in RGB camera, we adopt “Teachable Machine” (Google's AI tool) to train pose classifiers. A game-like sci-fi story script is created for providing immersive user experience. The avatar in the virtual space is interactively fabricated based on the recognized pose belonging to a user. The proposed on-going “Digital Fabrication” artwork is online exhibited on a computer screen.},
  keywords={Fabrication;Conferences;Avatars;Cameras;User experience;Internet;Artificial intelligence;Machine Learning;Avatar;Virtual Space;Pose Recognition;Future Museum},
  doi={10.1109/AIVR52153.2021.00024},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10945273,
  author={Pietri, Marcello and Mamei, Marco and Colajanni, Michele},
  booktitle={2024 22nd International Symposium on Network Computing and Applications (NCA)}, 
  title={Evaluating Technical Countermeasures for Telecom Spam and Scams in the AI Era}, 
  year={2024},
  volume={},
  number={},
  pages={270-277},
  abstract={This paper addresses the enduring issue of spam, scams, and robocalls within the telecommunications sector. The diffusion of generative AI technologies has escalated these challenges, as advancements in natural language processing and related tools enhance the sophistication of scams, facilitating the implementation of convincing social engineering attacks. The economic impact of these nefarious activities is significant, as evidenced by the vast number of spam calls and robocalls generated every day that lead to significant financial losses. Although technologies such as blocklists, STIR/SHAKEN, and Caller ID Verification methods are being implemented, the adoption of these solutions by phone companies remains slow due to industry barriers and varied regulatory frameworks. This paper evaluates the effectiveness of current anti-spam countermeasures and highlights the practical limits of these solutions, underscoring the need for improved decision-making tools.},
  keywords={Technological innovation;Limiting;Authentication;Standardization;Telephony;Time division multiplexing;Telecommunications;Fraud;Registers;Security;Caller ID Spoofing;Public Switched Telephone Network;Voice-over-IP;STIR/SHAKEN;Caller ID Verification;Telecom fraud},
  doi={10.1109/NCA61908.2024.00047},
  ISSN={2643-7929},
  month={Oct},}@INPROCEEDINGS{9373139,
  author={Jeon, Hyun-Kyu and Cheong, Yun-Gyung},
  booktitle={2021 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={A Peer Learning Method for Building Robust Text Classification Models}, 
  year={2021},
  volume={},
  number={},
  pages={321-324},
  abstract={Classification is an essential task in many practical problems. A machine learning based classification model is built to minimize the error between actual labels and predicted labels generated by the model. When the model depends on only actual labels during the training, it can generate monotonous distributional predictions. In order to make a robust model, it needs to use other sources of information in addition to the original labels. To address this issue, we propose a peer learning method that enables the target model to reference multiple peer models and that can control the impact of peers on the target model during the training phase. The experiment results indicate that the proposed method is promising.},
  keywords={Training;Learning systems;Conferences;Text categorization;Machine learning;Predictive models;Task analysis;peer learning;classification;deep learning;label refinery},
  doi={10.1109/BigComp51126.2021.00069},
  ISSN={2375-9356},
  month={Jan},}@INPROCEEDINGS{10941577,
  author={Sheikh, Mohammed Firdos Alam and Bhuyan, Ajatray Swagat and Dhiman, Ankita and Sharma, Chandni and Diya and Joshi, Mayurika and Ritu},
  booktitle={2024 Asian Conference on Intelligent Technologies (ACOIT)}, 
  title={Preserving Information Integrity: Analyzing the Impact of Deepfake Videos on Social Media Trust and Security}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={The proliferation of deepfake videos on social media has raised huge issues about disinformation, identity manipulation, and fraud. Advanced AI techniques now enable the creation of pretty convincing fake films, posing threats to character and countrywide security, eroding consider in digital media, and undermining online discourse. This research paper aims to develop a strong deepfake detection set of rules that leverages superior laptop vision and device getting to know strategies to accurately pick out manipulated films on social media. We will look at the unfold of deepfakes throughout social media platforms, examine their effect on on line discourse, and evaluate the effectiveness of our detection set of rules on numerous platforms. Furthermore, we will provide insights and recommendations for social media organizations, policymakers, and users to mitigate the dangers associated with deepfakes. Our last intention is to make contributions to a safer and greater honest on line surroundings with the aid of combating the spread of deepfakes and disinformation. The study will include a comprehensive literature evaluation on deepfake detection and social media vulnerabilities, experiments and analyses on a massive dataset of deepfake and real motion pictures, visualizations and information illustrating the spread and effect of deepfakes, and an assessment of our detection algorithm’s performance the use of metrics such as accuracy, precision, consider, and F1-score. Relevant legal guidelines, rules, and ethical guidelines related to deepfakes and social media may also be cited.},
  keywords={Deepfakes;Visualization;Portable computers;Social networking (online);Films;Virtual environments;Security;Reliability;Artificial intelligence;Guidelines;deepfake detection;social media;identity manipulation;artificial intelligence;computer vision},
  doi={10.1109/ACOIT62457.2024.10941577},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9882881,
  author={Corcuera Bárcena, José Luis and Ducange, Pietro and Ercolani, Alessio and Marcelloni, Francesco and Renda, Alessandro},
  booktitle={2022 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)}, 
  title={An Approach to Federated Learning of Explainable Fuzzy Regression Models}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Federated Learning (FL) has been proposed as a privacy preserving paradigm for collaboratively training AI models: in an FL scenario data owners learn a shared model by aggregating locally-computed partial models, with no need to share their raw data with other parties. Although FL is today extensively studied, a few works have discussed federated approaches to generate explainable AI (XAI) models. In this context, we propose an FL approach to learn Takagi-Sugeno-Kang Fuzzy Rule-based Systems (TSK-FRBSs), which can be considered as XAI models in regression problems. In particular, a number of independent data owner nodes participate in the learning process, where each of them generates its own local TSK-FRBS by exploiting an ad-hoc defined procedure. Then, these models are forwarded to a server that is responsible for aggregating them and generating a global TSK-FRBS, which is sent back to the nodes. An appropriate aggregation strategy is proposed to preserve the explainability of the global TSK-FRBS. A thorough experimental analysis highlights that the proposed approach brings benefits, in terms of accuracy, to data owners participating in the federation preserving the privacy of the data. Indeed, the accuracy achieved by the global TSK-FRBS is higher than the ones of the TSK-FRBSs learned by exploiting only local training data.},
  keywords={Fuzzy logic;Training;Data privacy;Training data;Collaborative work;Data models;Takagi-Sugeno model;TSK fuzzy system;federated learning;explainability;regression},
  doi={10.1109/FUZZ-IEEE55066.2022.9882881},
  ISSN={1558-4739},
  month={July},}@ARTICLE{9947337,
  author={Jia, Jun and Gao, Zhongpai and Zhu, Dandan and Min, Xiongkuo and Hu, Menghan and Zhai, Guangtao},
  journal={IEEE Transactions on Multimedia}, 
  title={RIVIE: Robust Inherent Video Information Embedding}, 
  year={2023},
  volume={25},
  number={},
  pages={7364-7377},
  abstract={Imagine an interesting situation when watching a movie, we can scan the screen using our smartphones to get some extra information about this movie such as the cast, the release date, the movie's homepage, etc. Our prospect is a world where each video contains invisible information that can be delivered to us through mobile devices with cameras. This paper proposes the first deep learning-based information hiding method for videos to achieve information transmission from screens to cameras. Compared with hiding information in single images, the methods for videos need to maintain visual quality in both spatial and temporal domains. Furthermore, the training of video models builds on a large video dataset, which needs much more computational resources than training models for images. To reduce the computational complexity, we propose to simulate data on-the-fly to generate simulated sequences from single images. Then, we use the simulated data to train a spatio-temporal generator that hides information in videos while maintaining visual quality. During training, a temporal loss function based on the simulated data is exploited to ensure the temporal consistency of generated videos. After embedding, we use a decoder to recover the hidden information. To simulate the imaging pipeline from screens to cameras in the real world, we insert a distortion network between the generator and decoder. The distortion network is based on differentiable 3D rendering to cover possible distortions introduced in the procedure of camera imaging. Experimental results show that the hidden information in videos can be extracted by cameras without impacting the visual quality. Our work can be applied to many fields, such as advertisement, entertainment, and education.},
  keywords={Cameras;Watermarking;Decoding;Steganography;Distortion;Motion pictures;Training;Data hiding;display-camera communication;temporal consistency;3D rendering;adversarial training},
  doi={10.1109/TMM.2022.3221894},
  ISSN={1941-0077},
  month={},}@INPROCEEDINGS{10624254,
  author={Addo, Daniel and Al-Antari, Mugahed A. and Zhou, Shijie and Ashalley, Eric and Muoka, Gladys Wavinya and Nartey, Obed T.},
  booktitle={2024 2nd International Conference on Intelligent Perception and Computer Vision (CIPCV)}, 
  title={Enhancing Alzheimer Disease Diagnosis: Integrating Gabor Convolutional Neural Network with Conventional CNNs}, 
  year={2024},
  volume={},
  number={},
  pages={147-151},
  abstract={Alzheimer’s disease (AD) presents a formidable challenge in healthcare, characterized by its progressive nature and the absence of definitive diagnostic markers. This study introduces an innovative method to enhance AD detection by integrating advanced technology with established techniques. We have developed a hybrid model that combines Gabor Convolutional Neural Networks (GCNN) with traditional Convolutional Neural Networks (CNNs) to improve feature extraction from MRI images, thereby achieving a more precise diagnosis of AD. The efficacy of our model is evidenced by its high-performance metrics: an accuracy of $98.58 \%$, a sensitivity of $\mathbf{9 8 . 1 7 \%}$, a precision of $97.11 \%$, and robust empirical support. This approach benefits from the use of augmented data and the unique capabilities of GCNN. Comprehensive evaluations, including confusion matrix analysis and ROC curve assessments, confirm the model’s proficiency in accurately classifying various stages of dementia. By enhancing diagnostic accuracy, this innovative approach holds the potential to improve patient outcomes, facilitate better care, and enable more timely interventions in the treatment of Alzheimer’s disease.},
  keywords={Analytical models;Accuracy;Sensitivity;Computational modeling;Merging;Predictive models;Data models;Alzheimer’s disease;Deep learning;Convolutional neural network;Depthwise separable convolution;Gabor convolutional neural network},
  doi={10.1109/CIPCV61763.2024.00033},
  ISSN={},
  month={May},}@INPROCEEDINGS{9689472,
  author={Huang, Huirong and Wu, Zhiyong and Kang, Shiyin and Dai, Dongyang and Jia, Jia and Fu, Tianxiao and Tuo, Deyi and Lei, Guangzhi and Liu, Peng and Su, Dan and Yu, Dong and Meng, Helen},
  booktitle={2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)}, 
  title={Speaker Independent and Multilingual/Mixlingual Speech-Driven Talking Head Generation Using Phonetic Posteriorgrams}, 
  year={2021},
  volume={},
  number={},
  pages={1433-1437},
  abstract={Generating 3D speech-driven talking head has re-ceived more and more attention in recent years. Recent approaches mainly have following limitations: 1) most speaker-independent methods need handcrafted features that are time-consuming to design or unreliable; 2) there is no convincing method to support multilingual or mixlingual speech as input. In this work, we propose a novel approach using phonetic posteriorgrams (PPG) extracted from input speech to predict facial animation parameters. In this way, our method doesn't need hand-crafted features and is more robust to noise compared to recent approaches. Furthermore, our method can support multilingual speech as input by building a universal phoneme space, which expands the phoneme space of PPG to support any new language. As far as we know, our model is the first to support multilingual/mixlingual speech as input with convincing results. Objective and subjective experiments have shown that our model can generate high quality animations given speech from unseen languages or speakers and is robust to noise.},
  keywords={Training;Three-dimensional displays;Architecture;Buildings;Information processing;Phonetics;Feature extraction},
  doi={},
  ISSN={2640-0103},
  month={Dec},}@INPROCEEDINGS{10887885,
  author={Li, Guohao and Yang, Hongyu and Men, Yifang and Huang, Di and Li, Weixin and Yang, Ruijie and Wang, Yunhong},
  booktitle={ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Generating Editable Head Avatars with 3D Gaussian GANs}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Generating animatable and editable 3D head avatars is essential for various applications in computer vision and graphics. Traditional 3D-aware generative adversarial networks (GANs), often using implicit fields like Neural Radiance Fields (NeRF), achieve photo-realistic and view-consistent 3D head synthesis. However, these methods face limitations in deformation flexibility and editability, hindering the creation of lifelike and easily modifiable 3D heads. We propose a novel approach that enhances the editability and animation control of 3D head avatars by incorporating 3D Gaussian Splatting (3DGS) as an explicit 3D representation. This method enables easier illumination control and improved editability. Central to our approach is the Editable Gaussian Head (EG-Head) model, which combines a 3D Morphable Model (3DMM) with texture maps, allowing precise expression control and flexible texture editing for accurate animation while preserving identity. To capture complex non-facial geometries like hair, we use an auxiliary set of 3DGS and tri-plane features. Extensive experiments demonstrate that our approach delivers high-quality 3D-aware synthesis with state-of-the-art controllability. Our code and models are available at https://github.com/liguohao96/EGG3D.},
  keywords={Training;Solid modeling;Three-dimensional displays;Head;Accuracy;Avatars;Superresolution;Animation;Neural radiance field;Generative adversarial networks;3D-aware GAN;Image and video synthesis},
  doi={10.1109/ICASSP49660.2025.10887885},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10911297,
  author={Garg, Avni and J, Jaisudha and Sood, Gourav and J, Gowrishankar and Dongre, Deepika and Ravivarman, G.},
  booktitle={2024 IEEE 4th International Conference on ICT in Business Industry & Government (ICTBIG)}, 
  title={Advanced Liveness Detection in Facial Recognition with Disguises}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={In recent decades, technology for recognising faces has advanced, making it popular in online communities and complex recognition systems. Face recognition is harder when people conceal themselves. Masked face recognition performs poorly in restricted spaces and big datasets. Face recognition systems must recognise disguised faces for security and dependability. This applies especially when individuals intentionally exploit the system. Ignoring disguised faces as actual faces is a major issue. Masks appear genuine because of their unintentional nature. Disguised faces are changed for style, culture, or personal preference, unlike spoofed ones. These individuals actively participate in recognition, making their masked faces true representations. Masked faces are treated as actual faces, authenticating people by their existence. By considering disguised identities as live faces, aliveness detection algorithms must handle their difficulties. Detecting spoofing became less important than proving identification, even with disguises. New techniques and algorithms are needed to accurately gather and analyse facial features from masked people to determine individual identification. We create an ensemble model to identify and distinguish real and counterfeit faces to address this issue. The ensemble model enhances face recognition by combining RGB and LBP photo texture information. Traditional methods are less accurate and reliable than the ensemble model, which is more resilient.},
  keywords={Accuracy;Face recognition;Films;Government;Force;Real-time systems;Security;Reliability;Faces;Testing;Artificial Intelligence;Biometrics;Face recognition;LBP;MTCNN;ReLU;CNN;HSV(Hue;Saturation;Value)},
  doi={10.1109/ICTBIG64922.2024.10911297},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9619094,
  author={Perez-Liebana, Diego and Guerrero-Romero, Cristina and Dockhorn, Alexander and Xu, Linjie and Hurtado, Jorge and Jeurissen, Dominik},
  booktitle={2021 IEEE Conference on Games (CoG)}, 
  title={Generating Diverse and Competitive Play-Styles for Strategy Games}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={Designing agents that are able to achieve different play-styles while maintaining a competitive level of play is a difficult task, especially for games for which the research community has not found super-human performance yet, like strategy games. These require the AI to deal with large action spaces, long-term planning and partial observability, among other well-known factors that make decision-making a hard problem. On top of this, achieving distinct play-styles using a general algorithm without reducing playing strength is not trivial. In this paper, we propose Portfolio Monte Carlo Tree Search with Progressive Unpruning for playing a turn-based strategy game (Tribes) and show how it can be parameterized so a quality-diversity algorithm (MAP-Elites) is used to achieve different play-styles while keeping a competitive level of play. Our results show that this algorithm is capable of achieving these goals even for an extensive collection of game levels beyond those used for training.},
  keywords={Training;Monte Carlo methods;Conferences;Decision making;Games;Planning;Task analysis},
  doi={10.1109/CoG52621.2021.9619094},
  ISSN={2325-4289},
  month={Aug},}@ARTICLE{10643257,
  author={Park, Jeongeun and Jeong, Taemoon and Kim, Hyeonseong and Byun, Taehyun and Shin, Seungyoun and Choi, Keunjun and Kwon, Jaewoon and Lee, Taeyoon and Pan, Matthew and Choi, Sungjoon},
  journal={IEEE Robotics and Automation Letters}, 
  title={Towards Embedding Dynamic Personas in Interactive Robots: Masquerading Animated Social Kinematic (MASK)}, 
  year={2024},
  volume={9},
  number={10},
  pages={8826-8833},
  abstract={This letter presents the design and development of an innovative interactive robotic system to enhance audience engagement using character-like personas. Built upon the foundations of persona-driven dialog agents, this work extends the agent's application to the physical realm, employing robots to provide a more captivating and interactive experience. The proposed system, named the Masquerading Animated Social Kinematic (MASK), leverages an anthropomorphic robot which interacts with guests using non-verbal interactions, including facial expressions and gestures. A behavior generation system based upon a finite-state machine structure effectively conditions robotic behavior to convey distinct personas. The MASK framework integrates a perception engine, a behavior selection engine, and a comprehensive action library to enable real-time, dynamic interactions with minimal human intervention in behavior design. Throughout the user subject studies, we examined whether the users could recognize the intended character in both personality- and film-character-based persona conditions. We conclude by discussing the role of personas in interactive agents and the factors to consider for creating an engaging user experience.},
  keywords={Databases;User experience;Kinematics;Gesture recognition;Facial features;Human factors;Social robots;Kinematics;Social HRI;gesture;posture and facial expressions;design and human factors},
  doi={10.1109/LRA.2024.3447470},
  ISSN={2377-3766},
  month={Oct},}@INPROCEEDINGS{10446049,
  author={Liu, Tao and Du, Chenpeng and Fan, Shuai and Chen, Feilong and Yu, Kai},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={DiffDub: Person-Generic Visual Dubbing Using Inpainting Renderer with Diffusion Auto-Encoder}, 
  year={2024},
  volume={},
  number={},
  pages={3630-3634},
  abstract={Generating high-quality and person-generic visual dubbing remains a challenge. Recent innovation has seen the advent of a two-stage paradigm, decoupling the rendering and lip synchronization process facilitated by intermediate representation as a conduit. Still, previous methodologies rely on rough landmarks or are confined to a single speaker, thus limiting their performance. In this paper, we propose DiffDub: Diffusion-based dubbing. We first craft the Diffusion auto-encoder by an inpainting renderer incorporating a mask to delineate editable zones and unaltered regions. This allows for seamless filling of the lower-face region while preserving the remaining parts. Throughout our experiments, we encountered several challenges. Primarily, the semantic encoder lacks robustness, constricting its ability to capture high-level features. Besides, the modeling ignored facial positioning, causing mouth or nose jitters across frames. To tackle these issues, we employ versatile strategies, including data augmentation and supplementary eye guidance. Moreover, we encapsulated a conformer-based reference encoder and motion generator fortified by a cross-attention mechanism. This enables our model to learn person-specific textures with varying references and reduces reliance on paired audio-visual data. Our rigorous experiments comprehensively highlight that our ground-breaking approach outpaces existing methods with considerable margins and delivers seamless, intelligible videos in person-generic and multilingual scenarios.},
  keywords={Training;Visualization;Technological innovation;Semantics;Synchronization;Speech processing;Videos;Talking Face;Diffusion;Face Animation;Dubbing},
  doi={10.1109/ICASSP48485.2024.10446049},
  ISSN={2379-190X},
  month={April},}@ARTICLE{9767852,
  author={Wang, Zong-Sheng and Song, Chang Geun and Lee, Jung and Kim, Jong-Hyun and Kim, Sun-Jeong},
  journal={IEEE Access}, 
  title={Controllable Swarm Animation Using Deep Reinforcement Learning With a Rule-Based Action Generator}, 
  year={2022},
  volume={10},
  number={},
  pages={48472-48485},
  abstract={The swarm behavior in nature is a fascinating and complex phenomenon that has been studied extensively for decades. Visually natural swarm animation can be produced by the state-of-the-art rule-based method; however, it still suffers from the drawbacks of low control accuracy and instability in swarm behavior quality when controlled by the user. This study proposes a deep reinforcement learning (DRL) based approach to generate swarm animation that reacts to real-time user control with high quality. A rule-based action generator (RAG) adapted to the actor-critic DRL method is presented to enhance DRL’s action exploration strategy. Various practical dynamic reward functions are also designed for DRL to train agents by rewarding swarm behaviors and penalizing misbehavior. The user controls the swarm by interacting with the swarm’s leader agent, for example by directly changing its speed or orientation, or by specifying a path consisting of waypoints. The second aim of this study is to improve the scalability of the trained policy. This study introduces a new state observation quantity of DRL called the embedded features of swarm (EFS) for allowing the trained policy scaling to a more extensive system than it has been trained on. In the experiments, four different scenarios have been designed to evaluate the control accuracy and quality of the generated swarm behavior by metrics and visualization. Additionally, the experiment has compared the performance of the proposed dynamic reward functions with fixed reward functions. Experimental results show that the proposed approach outperforms state-of-the-art methods in terms of swarm behavior quality and control accuracy. Moreover, the proposed dynamic reward functions are more effective than the existing reward functions.},
  keywords={Animation;Reinforcement learning;Computational modeling;Real-time systems;Generators;Scalability;Insects;Computer graphics;swarm animation;deep reinforcement learning;actor-critic;path planning},
  doi={10.1109/ACCESS.2022.3172492},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10134683,
  author={Varshney, Neeraj and Kumar, Gaurav and Kumar, Ankit and Pandey, Saroj Kumar and Singh, Teekam and Singh, Kamred Udham},
  booktitle={2023 IEEE 12th International Conference on Communication Systems and Network Technologies (CSNT)}, 
  title={AI-Enable Generating Human Faces using Deep Learning}, 
  year={2023},
  volume={},
  number={},
  pages={256-262},
  abstract={Lately, sensible Image processing using deep neural networks has become a fervently discussed issue in machine learning and computer vision. Image can be made at the pixel level by learning from a gigantic variety of pictures. Learning to make splendid movement pictures from highdifference draws is not only a captivating investigation issue yet also a reasonable application in innovative delight. In this research, we research the sketch-to-picture mix issue by using prohibitive generative poorly arranged networks. The model can normally deliver reasonable shadings for a sketch. The new model is not only prepared for painting hand-drawn sketches with real tones, yet also allows customers to exhibit supported tones. Test results on two sketch datasets show that the autopainter performs better contrasted with existing picture-topicture methodologies. With creating interest in the development of film, the interest in building a computerized structure to change over the authentic video into action is higher than at some other time. The edge-by-diagram modification of the action age measure is costly and dreary. To help with moving quickly and with no issue in a robotized collaboration we proposed a generative model that changes over genuine pictures into contrasting energy pictures without losing critical nuances of the source picture. We used an assortment of the generative hostile association as a fundamental plan with the custom incident ability to ensure the substance of the source picture, which changed over to an exuberance image.},
  keywords={Deep learning;Computer vision;Communication systems;Computational modeling;Image processing;Neural networks;Buildings;component;formatting;style;styling;insert (key words)},
  doi={10.1109/CSNT57126.2023.10134683},
  ISSN={2473-5655},
  month={April},}@INPROCEEDINGS{8935856,
  author={Iwashita, Motoi},
  booktitle={2019 20th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)}, 
  title={Trend of Influencer Marketing and Future Required Functions}, 
  year={2019},
  volume={},
  number={},
  pages={2-2},
  abstract={The development of ICT such as broadband networks, cloud computing, and mobile gadgets has enabled the wide use of not only information browsing (news sites, blogs, etc.) but also Consumer Generated Media (CGM). These multimedia services have dramatically changes our modern lifestyle. They give large impact for individuals to obtain and share information with ease. Individuals follow and make decision by consulting word-by-mouth information, comments and contents by influencers. Especially, mobile gadgets, such as smartphones and tablet PCs, have high usability and performance, and can provide easy access to required information at any time and place.Influencer marketing becomes popular recently according to these tendencies. Influencer marketing is one of the marketing method in which the information of products/services is scattered through influencers with social impact for intending the increase of awareness and interest in the market. The new work style, YouTuber, was born these days. Therefore, video is one of the strong media for advertising. As a result, the market of video advertisement gradually grows year by year. Lots of enterprises want to use video advertisement, and the needs of making video contents by influencers will increase.In this talk, I describe several issues concerned with influencer marketing. The result whether influencer marketing grows is deeply related to players behaviours, i.e., behaviours of consumers, providers, influencers and so on. Key features are constructing a new service to compensate the drawbacks of the conventional influencer marketing based on players behaviours, and realizing required functions for this mechanism. I outline some of the consequent functions that will be needed in this setting.},
  keywords={Telecommunications;Media;Laboratories;Information science;Usability;Tablet computers;Smart phones},
  doi={10.1109/SNPD.2019.8935856},
  ISSN={},
  month={July},}@INPROCEEDINGS{10348907,
  author={Hsu, Yi-Chung and Huang, Wong-Shian},
  booktitle={2023 18th International Microsystems, Packaging, Assembly and Circuits Technology Conference (IMPACT)}, 
  title={A Precision Enhancement Deep Learning Framework for Package Substrate Defect Detection}, 
  year={2023},
  volume={},
  number={},
  pages={174-177},
  abstract={In the manufacturing industry, producing a large quantity of highquality products every day is the top priority. Recently, the integration of Automated Optical Inspection (AOI) equipment and Deep Neural Network (DNN) defect image predictions has demonstrated a significant reduction in manual effort required for defect inspection in manufacturing industrial applications. However, during substrate manufacturing, changes in in-process conditions can lead to unexpected variations in defects, such as combinations of multiple defects or anomalies that humans are not familiar with. Consequently, mainstream DNN-based object detection (OD) methods have yet to achieve the desired accuracy for detecting these defects images in production line, and hence human visual inspections remain essential. In this paper, we propose a defect detection framework to both reduce the need for human examination and maintain the high quality of normal products for downstream processes. This method integrates three DNN methods: OD, anomaly detection (AD), and auxiliary classification (AC). OD constructs hyperplanes for bounding-box objects and classes. AD measures distances between embedding vectors trained from normal samples to detect anomalies. AC operates by centering the object proposed by OD within a cropped image and re-judging the object's class, which can refine OD's anchor-based training. We conducted an ASE manufacturing dataset experiment, and our method reduced human examination by 28%, with only a 2.8% underkill rate. Combining OD and AC achieved 93.21% defect classification accuracy, surpassing mainstream accuracy 89.74%. With the assistance of our new framework, we are able to simultaneously reduce the manual efforts for examination and increase the accuracy of defect classification. When applied in MFG production lines, this technology can surely assist our factory operation, enabling us to better meet a smart factory requirements.},
  keywords={Manufacturing industries;Training;Productivity;Visualization;Manuals;Artificial neural networks;Packaging},
  doi={10.1109/IMPACT59481.2023.10348907},
  ISSN={2150-5942},
  month={Oct},}@INPROCEEDINGS{10241690,
  author={Percassi, Francesco and Bhatnagar, Saumya and Guo, Rongge and Mccabe, Keith and Mccluskey, Thomas L. and Vallati, Mauro},
  booktitle={2023 8th International Conference on Models and Technologies for Intelligent Transportation Systems (MT-ITS)}, 
  title={An Efficient Heuristic for AI-based Urban Traffic Control}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={There is a growing interest in the use of Artificial Intelligence (AI) techniques for urban traffic control, with a particular focus on traffic signal optimisation. AI-based approaches demonstrated to be capable of dealing in real-time with unexpected or unusual traffic conditions, as well as with the usual traffic patterns. To effectively perform their task, AI approaches require the design of ad-hoc techniques, usually under the form of heuristics to guide the search process. In this paper, leveraging on the successful application of automated planning to urban traffic control, we introduce an innovative heuristic and test it using real-world historical data. The experimental analysis shows that the proposed heuristic allows to quickly generate high-quality traffic signal strategies, that outperform those generated by the SCOOT framework currently deployed in the considered region.},
  keywords={Analytical models;Atmospheric modeling;Traffic control;Throughput;Data models;Planning;Artificial intelligence;Urban Traffic Control;Traffic Signal optimisation;Artificial Intelligence},
  doi={10.1109/MT-ITS56129.2023.10241690},
  ISSN={},
  month={June},}@INPROCEEDINGS{10219594,
  author={Xu, Jingning and Tang, Benlai and Wang, Mingjie and Li, Minghao and Ma, Meirong},
  booktitle={2023 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={CPNet: Exploiting CLIP-based Attention Condenser and Probability Map Guidance for High-fidelity Talking Face Generation}, 
  year={2023},
  volume={},
  number={},
  pages={240-245},
  abstract={Recently, talking face generation has drawn ever-increasing attention from the research community in computer vision due to its arduous challenges and widespread application scenarios, e.g. movie animation and virtual anchor. Although persevering efforts have been undertaken to enhance the fidelity and lip-sync quality of generated talking face videos, there is still large room for further improvements of synthesis quality and efficiency. Actually, these attempts somewhat ignore the explorations of fine-granularity feature extraction/integration and the consistency between probability distributions of landmarks, thereby recurring the issues of local details blurring and degraded fidelity. To mitigate these dilemmas, in this paper, a novel CLIP-based Attention and Probability Map Guided Network (CPNet) is delicately designed for inferring high-fidelity talking face videos. Specifically, considering the demands of fine-grained feature recalibration, a clip-based attention condenser is exploited to transfer knowledge with rich semantic priors from the prevailing CLIP model. Moreover, to guarantee the consistency in probability space and suppress the landmark ambiguity, we creatively propose the density map of facial landmark as auxiliary supervisory signal to guide the landmark distribution learning of generated frame. Extensive experiments on the widely-used benchmark dataset demonstrate the superiority of our CPNet against state of the arts in terms of image and lip-sync quality. In addition, a cohort of studies are also conducted to ablate the impacts of the individual pivotal components.},
  keywords={Computer vision;Semantics;Benchmark testing;Feature extraction;Motion pictures;Animation;Probability distribution;Talking Face Generation;CLIP;Channel-wise Recalibration;Density Map;Probability Space},
  doi={10.1109/ICME55011.2023.00049},
  ISSN={1945-788X},
  month={July},}@INPROCEEDINGS{9395990,
  author={Minu, M. S. and Canessane, R. Aroul},
  booktitle={2021 International Conference on Artificial Intelligence and Smart Systems (ICAIS)}, 
  title={An Efficient Squirrel Search Algorithm based Vector Quantization for Image Compression in Unmanned Aerial Vehicles}, 
  year={2021},
  volume={},
  number={},
  pages={789-793},
  abstract={Unmanned aerial vehicles (UAVs) typically fly at low altitudes for capturing high-resolution images covering smaller areas. Since short flights also and high-resolution cameras lead to the generation of massive gigabytes (GBs) of data regions, image compression is essential to compress the data to a compact form resulted in shorter file size without any loss of quality. The vector quantization (VQ) is an effective type of image compression and the conventionally employed technique namely Linde-Buzo-Gray (LBG) algorithm continually created local optimal codebook. The codebook design process can be considered as a high dimensional optimization problem and can be resolved by the use of swarm intelligence algorithms. This paper designs a novel squirrel search algorithm (SSA) with LBG based image compression technique, called SSA-LBG for UAVs. The SSA is applied for the construction of codebooks for VQ and it makes use of LBG model as the initialization of the SSA for VQ. The application of SSA-LBG results in effective compression with low computation time (CT) and high peak signal to noise ratio (PSNR). An extensive set of simulations were performed on benchmark test images and the results are examined with respect to CT and PSNR undervarying bit rates and codebook sizes.},
  keywords={Image coding;PSNR;Computed tomography;Vector quantization;Computational modeling;Unmanned aerial vehicles;Particle swarm optimization;Compression ratio Codebook generation;Image compression;LBG model;Squirrel search algorithm Unmanned aerial vehicle;Vector quantization},
  doi={10.1109/ICAIS50930.2021.9395990},
  ISSN={},
  month={March},}@INPROCEEDINGS{10771097,
  author={Lee, Seongmin and Hoover, Benjamin and Strobelt, Hendrik and Wang, Zijie J. and Peng, ShengYun and Wright, Austin and Li, Kevin and Park, Haekyu and Yang, Haoyang and Chau, Duen Horng Polo},
  booktitle={2024 IEEE Visualization and Visual Analytics (VIS)}, 
  title={Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion}, 
  year={2024},
  volume={},
  number={},
  pages={96-100},
  abstract={Diffusion-based generative models’ impressive ability to create convincing images has garnered global attention. However, their complex structures and operations often pose challenges for non-experts to grasp. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion’s complex structure with explanations of the underlying operations. By comparing image generation of prompt variants, users can discover the impact of keyword changes on image generation. A 56-participant user study demonstrates that Diffusion Explainer offers substantial learning benefits to non-experts. Our tool has been used by over 10,300 users from 124 countries at https://poloclub.github.io/diffusion-explainer/.},
  keywords={Visualization;Image synthesis;Visual analytics;Blogs;Text to image;Transforms;Animation;Generative AI;Machine Learning;Interactive visualization;Text-to-image generative AI;Artificial Intelligence;User study},
  doi={10.1109/VIS55277.2024.00027},
  ISSN={2771-9553},
  month={Oct},}@ARTICLE{10781389,
  author={Wang, Fuchun and Wang, Kesheng and Song, Lei},
  journal={IEEE Access}, 
  title={Channel Graph Convolutional Networks for Animation Image Super-Resolution}, 
  year={2024},
  volume={12},
  number={},
  pages={197577-197588},
  abstract={Animation images exhibit rich edge characteristics, distinct from those found in naturalistic images. Since general image super-resolution reconstruction techniques are not explicitly designed to handle animation image features, especially edges, these methods may not be suitable for animation image reconstruction. To address this limitation, we have curated a large-scale, high-quality color animation dataset encompassing various contents and styles to investigate this phenomenon. In response to the unique challenges posed by animation images, we propose Channel Graph Convolutional Networks (CGC-Nets), which are fast, accurate, and lightweight for animated image super-resolution. These networks demonstrate efficient memory usage and runtime performance across different scaling factors. Specifically, CGC-Nets consist of two dedicated sections. The Back-Projection block is used in the first section to maintain and modify the animation picture content (i.e., enhance the animation image’s low-frequency information). The second step focuses on improving the edge information of animated images by transforming feature maps obtained from gradient maps into graph features. Graph features encapsulate the topological relationships between each channel in the feature generated by convolution. This structuring of details and edge information (i.e., high-frequency information) in animation images makes it easier to capture and reconstruct such information effectively. Our proposed CGC-Nets method significantly enhances the performance of anime image super-resolution tasks. Extensive experiments validate the superiority of the CGC-Nets network over state-of-the-art methods in super-resolution reconstruction.},
  keywords={Animation;Image edge detection;Superresolution;Image resolution;Image reconstruction;Convolution;Training;Kernel;Graph convolutional networks;Visualization;Image super-resolution;animation image;lightweight;graph features;gradient},
  doi={10.1109/ACCESS.2024.3512796},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9781486,
  author={Bozyel, Ibrahim and Endes, Alper and Akkoca, Aybuke and Yuksekkaya, Baris and Gokcen, Dincer},
  booktitle={2022 IEEE International Conference on Flexible and Printable Sensors and Systems (FLEPS)}, 
  title={AI-based Liquid Classification with Laser-Induced Graphene Flex-Sensor}, 
  year={2022},
  volume={},
  number={},
  pages={1-4},
  abstract={Flexible sensors have a great impact in removing the barriers of the electronic components caused by their rigid shape. This study introduces optimal artificial intelligence algorithms for the classification of high precision flex-sensor outputs in sensing various liquids. The composite-based sensor was realized by combining polydimethylsiloxane (PDMS) and laser-induced graphene formed on polyimide (PI). PI substrate was engraved by blue laser to produce graphene sheets over the surface, while this approach decreases cost of sensor production, reliability of mass production was improved with less process steps. The recorded capacitance values were used to classify various liquids dropped over the sensor, then more than 90% accuracy, precision, and recall results were obtained under the scope of this study.},
  keywords={Training;Liquids;Graphene;Training data;Polyimides;Sensor systems and applications;Capacitive sensors;Flexible sensor;artificial intelligence;liquid classification;laser-induced graphene;flexible composite},
  doi={10.1109/FLEPS53764.2022.9781486},
  ISSN={},
  month={July},}@ARTICLE{9994681,
  author={Bigioi, Dan and Jordan, Hugh and Jain, Rishabh and McDonnell, Rachel and Corcoran, Peter},
  journal={IEEE Access}, 
  title={Pose-Aware Speech Driven Facial Landmark Animation Pipeline for Automated Dubbing}, 
  year={2022},
  volume={10},
  number={},
  pages={133357-133369},
  abstract={A novel neural pipeline allowing one to generate pose aware 3D animated facial landmarks synchronised to a target speech signal is proposed for the task of automatic dubbing. The goal is to automatically synchronize a target actors’ lips and facial motion to an unseen speech sequence, while maintaining the quality of the original performance. Given a 3D facial key point sequence extracted from any reference video, and a target audio clip, the neural pipeline learns how to generate head pose aware, identity aware landmarks and outputs accurate 3D lip motion directly at the inference stage. These generated landmarks can be used to render a photo-realistic video via an additional image to image conversion stage. In this paper, a novel data augmentation technique is introduced that increases the size of the training dataset from N audio/visual pairs up to NxN unique pairs for the task of automatic dubbing. The trained inference pipeline employs a LSTM-based network that takes Mel-coefficients as input from an unseen speech sequence, combined with head pose, and identity parameters extracted from a reference video to generate a new set of pose aware 3D landmarks that are synchronized with the unseen speech.},
  keywords={Synchronization;Machine learning;Three-dimensional displays;Artificial intelligence;Magnetic heads;Task analysis;Animation;Speech recognition;Machine learning;computer vision;lip synchronization;talking head generation;automatic dubbing;audio driven deep fakes;artificial intelligence},
  doi={10.1109/ACCESS.2022.3231137},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9935364,
  author={Syambas, Nana Rachmana and Juhana, Tutun and Hendrawan and Mulyana, Eueung and Edward, Ian Joseph Matheus and Situmorang, Hamonangan and Mayasari, Ratna and Negara, Ridha Muldina and Yovita, Leanna Vidya and Wibowo, Tody Ariefianto and Ahdan, Syaiful and Nurkahfi, Galih Nugraha and Nurhayati, Ade and Mulya, Hafiz and Budiana, Mochamad Soebagja},
  booktitle={2022 8th International Conference on Wireless and Telematics (ICWT)}, 
  title={Research Progress On Name Data Networking To Achieve A Superior National Product In Indonesia}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Global traffic data are proliferating, including in Indonesia. The number of internet users in Indonesia reached 205 million in January 2022. This data means that 73.7% of Indonesia’s population has used the internet. The median internet speed for mobile phones in Indonesia is 15.82 Mbps, while the median internet connection speed for Wi-Fi in Indonesia is 20.13 Mbps. As predicted by many, real-time traffic such as multimedia streaming dominates more than 79% of traffic on the internet network. This condition will be a severe challenge for the internet network, which is required to improve the Quality of Experience (QoE) for user mobility, such as reducing delay, data loss, and network costs. However, IP-based networks are no longer efficient at managing traffic. Named Data Network (NDN) is a promising technology for building an agile communication model that reduces delays through a distributed and adaptive name-based data delivery approach. NDN replaces the ‘where’ paradigm with the concept of ‘what’. User requests are no longer directed to a specific IP address but to specific content. This paradigm causes responses to content requests to be served by a specific server and can also be served by the closest device to the requested data. NDN router has CS to cache the data, significantly reducing delays and improving the internet network’s quality of Service (QoS). Motivated by this, in 2019, we began intensive research to achieve a national flagship product, an NDN router with different functions from ordinary IP routers. NDN routers have cache, forwarding, and routing functions that affect data security on name-based networks. Designing scalable NDN routers is a new challenge as NDN requires fast hierarchical name-based lookups, perpackage data field state updates, and large-scale forward tables. We have a research team that has conducted NDN research through simulation, emulation, and testbed approaches using virtual machines to get the best NDN router design before building a prototype. Research results from 2019 show that the performance of NDN-based networks is better than existing IP-based networks. The tests were carried out based on various scenarios on the Indonesian network topology using NDNsimulator, MATLAB, Mininet-NDN, and testbed using virtual machines. Various network performance parameters, such as delay, throughput, packet loss, resource utilization, header overhead, packet transmission, round trip time, and cache hit ratio, showed the best results compared to IP-based networks. In addition, NDN Testbed based on open source is free, and the flexibility of creating topology has also been successfully carried out. This testbed includes all the functions needed to run an NDN network. The resource capacity on the server used for this testbed is sufficient to run a reasonably complex topology. However, bugs are still found on the testbed, and some features still need improvement. The following exploration of the NDN testbed will run with more new strategy algorithms and add Artificial Intelligence (AI) to the NDN function. Using AI in cache and forwarding strategies can make the system more intelligent and precise in making decisions according to network conditions. It will be a step toward developing NDN router products by the Bandung Institute of Technology (ITB) Indonesia.},
  keywords={Network topology;Heuristic algorithms;Quality of service;Virtual machining;Delays;Topology;Servers;Name Data Networking;Internet Network;IP-based Network;NDN Router;NDN Testbed},
  doi={10.1109/ICWT55831.2022.9935364},
  ISSN={},
  month={July},}@INPROCEEDINGS{10828545,
  author={Yusri, Muhammed Wafi Bin and Masrie, Marianah and Badaruddin, Siti Aishah Mohamad and Burham, Norhafizah and Janin, Zuriati and Saad, Hasnida},
  booktitle={2024 IEEE 7th International Conference on Electrical, Electronics and System Engineering (ICEESE)}, 
  title={Image Classification of Graphene Oxide Thin Films’ Sheet Resistance using a Convolution Neural Network}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This study focuses on developing a CNN model, VGG-16, to classify microscopy images of graphene oxide thin films produced by two machines; Atomizer 2 and Atomizer 3 based on the sheet resistance values. The methodology begins with preparing microscopic images of graphene oxide thin films dataset. The dataset undergoes preprocessing to enhance image quality. It is then divided into training (80%) and testing (20%) sets. Data augmentation techniques were applied to improve the model’s generalization capabilities. The core of this research involves constructing a CNN model using the VGG-16 architecture, which is trained on the preprocessed dataset. Training and validation results are obtained to assess the model’s performance. Subsequently, a separate test model evaluates the accuracy of the image classification process. The results indicate an accuracy of 76.7% for images from Atomizer 2 and 92.37% for images from Atomizer 3, demonstrating the effectiveness of the developed AI program in classifying graphene oxide thin films microscopic images based on sheet resistance values.},
  keywords={Resistance;Training;Image quality;Accuracy;Graphene;Neural networks;Systems engineering and theory;Electron microscopy;Image classification;Testing;CNN;VGG-16;image classification;Graphene Oxide;Reduced Graphene Oxide;sheet resistance},
  doi={10.1109/ICEESE62315.2024.10828545},
  ISSN={2770-9787},
  month={Nov},}@ARTICLE{9249235,
  author={Liang, Haitao and Chen, Xiaodong and Xu, Huaiyuan and Ren, Siyu and Cai, Huaiyu and Wang, Yi},
  journal={IEEE Access}, 
  title={Local Foreground Removal Disocclusion Filling Method for View Synthesis}, 
  year={2020},
  volume={8},
  number={},
  pages={201286-201299},
  abstract={View synthesis is an effective method to generate the contents of multiple views based on a limited number of reference views, which can be used in 2D to 3D conversion, free viewpoint video and multiview video rendering. Depth-image-based rendering (DIBR) is a practical technique to generate virtual view by using a 2D reference view and its depth image. However, a critical problem in DIBR process is that disocclusions might be produced in the synthesized image because the background occluded by the foreground objects in the reference view may be exposed in the virtual view. In this paper, a local foreground removal method is proposed for disocclusion filling. Morphology-based depth image preprocessing is performed before DIBR, aiming to correct the depth value of the ghosts and remove ghost artifacts. In the synthesized virtual image, pixels on the disocclusion edge are identified and classified. Then they are positioned in the reference image by inverse 3D warping. Local foreground regions that occlude the corresponding background are removed from both the reference image and its depth image based on the disocclusion edge pixels. Removed region is filled with surrounding background contents, and depth information is used in this process to prevent foreground penetration. The predicted background contents are warped to the disocclusion region, thereby achieving the hole filling. Experimental results show that the proposed method performs better than the other methods in disocclusion filling, and improves the subjective and objective quality of the synthesized view. In the evaluation results of PSNR, SSIM, FSIMc and VSI, our method improves by 0.32-2.43dB, 0.0036-0.0155, 0.0041-0.0198 and 0.0012-0.0057 respectively compared with competitive methods.},
  keywords={Image edge detection;Three-dimensional displays;Filling;Two dimensional displays;Rendering (computer graphics);Reliability;Laplace equations;Free viewpoint video;depth-image-based rendering;disocclusion filling;local foreground removal;view synthesis},
  doi={10.1109/ACCESS.2020.3036053},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10182597,
  author={G, Ashalatha. and Modi, Sanjay and Dixit, Anil Kumar and Kumar, R. Ravi and Al-Chilibi, Hayder and Alazzam, Malik Bader},
  booktitle={2023 3rd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)}, 
  title={Ensuring Security in Internet of Everything (IoT) With Smart Sensor Networking and Mobile Ad Hoc Network}, 
  year={2023},
  volume={},
  number={},
  pages={1677-1681},
  abstract={A new age of connection is being ushered in by the Internet of Things (IoT), one that relies on a wide variety of objects and several modulation schemes to communicate. The followings will define this new era of correspondence: The IoT presents a substantially higher number of concerns, restrictions, and problems than do conventional computers since all of its items are interconnected and operate in unprotected contexts. This is because there are fewer linked elements in conventional computer systems. As a result, it is essential that cybersecurity be given priority in a new strategy, although that is not presently true of regular computer platforms. Two key elements in the development of an Internet of Everything system are the Smart Sensor Networking and also the Mobile Ad hoc Network. Sensing, pollution prevention, data collection, multimodal telecommunication networks, and data management are only a few of the many operations that employ this technology. IoT is vulnerable to the same types of safety concerns that other networks face since it combines elements of both Mobile ad hoc networks and Embedded system. A Suspension of Operation is a subtype of an onslaught known as a Delegates Entities Operation. The intruder delivers an unacceptably large number of command sessions that seem genuine. An SD operation is one of the various types of DoS attacks that are possible. This makes it far more difficult to identify this kind of assault than a straightforward one that drains the capacitors capability. The necessity to improve energy conservation and extend the life of Iot networks is one of the other major issues that occur in a networking throughout an SD attack. This is another important problem that develops in a network after an assault. Utilizing a Rng Generation with Heterogeneous Idss, often known as RNGHID, is advised. There are several distinct industries and regions that will make up the Web of Things economy. In order to detect any nodes within the network that are acting abnormally, the software has been divided into two entities known as the Delegation Item and the Central Organization. The Delegation Domain and the Central Organization are the names of these two entities, correspondingly. It will be feasible to locate the location of the assault, torment, and other harmful acts after the anomaly have been located. The Attacker Node Notification System generates an alert notification that is disseminated out across networks to let other nodes know that the connection is being attacked. According to the results of an algorithms that makes use of learning algorithms, this statement categorizes the numerous types of assaults. The protocol demonstrates a number of desirable characteristics, including the ability to execute undivided identification, quick identity verification, and little expense in the both communication and storage. These are only a handful of the appealing qualities.},
  keywords={Computers;Protocols;Surveillance;Organizations;Software;Safety;Internet of Things;Cyber Security;ML;IoT;SD Attacks;RNGHID System;PE;NODES;MANET;DEA;MNA System;IP Address and IPv6},
  doi={10.1109/ICACITE57410.2023.10182597},
  ISSN={},
  month={May},}@INPROCEEDINGS{10374064,
  author={Oliveira, Luciana Pereira and do Nascimento, Genildo Alves},
  booktitle={2023 29th International Conference on Telecommunications (ICT)}, 
  title={A Systematic Literature Review on Asterisk: Teach More than VoIP Communication}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Asterisk is a free and open-source software for telecommunication scenarios. It was created in 1999 and used by organizations of various types and sizes worldwide. It is software that can serve as the business infrastructure for telephony with different phone systems. This paper examines the relevance of this tool for teaching electrical engineering, computer science, and other areas. The study investigated the evolution, technologies, the benefits of learning it for students, and its applicability in practical activities to teach new concepts. The methodology of this study is a systematic review of searches in electronic repositories until 2020. The results found 3,221 documents, selecting 179 to extract data. The study identified several scenarios in hands-on exercises to improve knowledge skills in security, artificial intelligence, network quality, measurement, and programming. Furthermore, this research suggests that teachers can use the tool to motivate and engage students in learning about the latest network concepts, such as Software Defined Networks (SDN), Virtualization, the Internet of Things (IoT), and Blockchain.},
  keywords={Systematics;DevOps;Education;Telephony;Communications technology;Security;Internet of Things;Asterisk;review;education;telecommunication;open-source},
  doi={10.1109/ICT60153.2023.10374064},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10943555,
  author={Cheng, Yongkang and Liang, Mingjiang and Huang, Shaoli and Han, Gaoge and Ning, Jifeng and Liu, Wei},
  booktitle={2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Conditional GAN for Enhancing Diffusion Models in Efficient and Authentic Global Gesture Generation from Audios}, 
  year={2025},
  volume={},
  number={},
  pages={2164-2173},
  abstract={Audio-driven simultaneous gesture generation is vital for human-computer communication, AI games, and film pro-duction. While previous research has shown promise, there are still limitations. Methods based on VAEs are accompa-nied by issues of local jitter and global instability, whereas methods based on diffusion models are hampered by low generation efficiency. This is because the denoising process of DDPM in the latter relies on the assumption that the noise added at each step is sampled from a unimodal distribution, and the noise values are small. DDIM bor-rows the idea from the Euler method for solving differential equations, disrupts the Markov chain process, and increases the noise step size to reduce the number of denoising steps, thereby accelerating generation. However, simply increasing the step size during the step-by-step denoising process causes the results to gradually deviate from the original data distribution, leading to a significant drop in the quality of the generated actions and the emergence of unnatural artifacts. In this paper, we break the assumptions of DDPM and achieves breakthrough progress in denoising speed and fidelity. Specifically, we introduce a conditional GAN to capture audio control signals and implicitly match the multimodal denoising distribution between the diffusion and denoising steps within the same sampling step, aiming to sample larger noise values and apply fewer denoising steps for high-speed generation. In addition, to enable the model to generate high-fidelity global gestures and avoid artifacts, we introduce an explicit motion geometric loss to enhance the quality and global stability of the generated gestures. Numerous qualitative and quantitative experiments show that compared to contemporary diffusion-based methods, our method offers faster generation speed and higher fidelity, and compared to non-diffusion methods, it provides a more stable global effect and a more natural user experience.},
  keywords={Computer vision;Noise reduction;Noise;Games;Differential equations;Jitter;Diffusion models;User experience;Stability analysis;Artificial intelligence},
  doi={10.1109/WACV61041.2025.00217},
  ISSN={2642-9381},
  month={Feb},}@ARTICLE{10948188,
  author={Haddad, Nabeel Mahdy and Mustafa, Mustafa Sabah and Salih, Hayder Sabah and Jaber, Mustafa Musa and Ali, Mohammed Hasan},
  journal={Journal of Cyber Security and Mobility}, 
  title={Analysis of the Security of Internet of Multimedia Things in Wireless Environment}, 
  year={2024},
  volume={13},
  number={1},
  pages={161-191},
  abstract={The Internet of Things (IoT) and real-time flexibility improve people's lives, and IoT applications rely heavily on multimedia sensors and devices. An interconnected network of IoT multimedia devices has made the Internet of Medical Things (IoMT). It creates massive data distinct from what the Internet of Things (IoT) produced. Smart traffic monitoring and smart hospitals are only a few examples of real-time deployment applications. IoMT data and decision-making must be made quickly since it directly impacts human life. The security heterogeneity of optimization issues is a significant challenge for enabling multimedia applications on the IoT. The IoMT has difficulty achieving low-cost data collecting while maintaining data security. An Internet of Multimedia Things in a wireless environment (IoMT-WE) system decreases the bandwidth and privacy risk caused by the revocation list, ensures the integrity of batch verification information, and corresponds with Vehicular ad hoc network (VANET) security performance. The proposed method uses random subsampling and chaotic convolution to collect numerous images. The sampling method is safe since the measurement matrix is controlled by chaos. As part of the IoMT architecture, wireless multimedia sensor nodes can be more easily deployed over the long term for real-time multimedia. The Wireless Multimedia Sensor Network (WMSN) comprises nodes that can capture both multimedia and non-multimedia data. The IoMT-WE system has been tested and found to be secure and effective.},
  keywords={Wireless communication;Wireless sensor networks;Privacy;Chaotic communication;Vehicular ad hoc networks;Real-time systems;Internet of Things;Communication system security;Multimedia communication;Optimization;Wireless environment;multimedia applications;internet of multimedia things;VANET;chaotic convolution;sampling},
  doi={10.13052/jcsm2245-1439.1316},
  ISSN={2245-4578},
  month={January},}@INPROCEEDINGS{10678183,
  author={Lv, Jiaxi and Huang, Yi and Yan, Mingfu and Huang, Jiancheng and Liu, Jianzhuang and Liu, Yifan and Wen, Yafei and Chen, Xiaoxin and Chen, Shifeng},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={GPT4Motion: Scripting Physical Motions in Text-to-Video Generation via Blender-Oriented GPT Planning}, 
  year={2024},
  volume={},
  number={},
  pages={1430-1440},
  abstract={Recent advances in text-to-video generation have harnessed the power of diffusion models to create visually compelling content conditioned on text prompts. However, they usually encounter high computational costs and often struggle to produce videos with coherent physical motions. To tackle these issues, we propose GPT4Motion, a training-free framework that leverages the planning capability of large language models such as GPT, the physical simulation strength of Blender, and the excellent image generation ability of text-to-image diffusion models to enhance the quality of video synthesis. Specifically, GPT4Motion employs GPT-4 to generate a Blender script based on a user textual prompt, which commands Blender’s built-in physics engine to craft fundamental scene components that encapsulate coherent physical motions across frames. Then these components are inputted into Stable Diffusion to generate a video aligned with the textual prompt. Experimental results on three basic physical motion scenarios, including rigid object drop and collision, cloth draping and swinging, and liquid flow, demonstrate that GPT4Motion can generate high-quality videos efficiently in maintaining motion coherency and entity consistency. GPT4Motion offers new insights in text-to-video research, enhancing its quality and broadening its horizon for future explorations. Our homepage website is https://GPT4Motion.github.io.},
  keywords={Three-dimensional displays;Image synthesis;Large language models;Text to image;Fluid flow;Manuals;Diffusion models},
  doi={10.1109/CVPRW63382.2024.00150},
  ISSN={2160-7516},
  month={June},}@ARTICLE{10679101,
  author={Sukumarran, Dhevisha and Sam Loh, Ee and Salwa Mohd Khairuddin, Anis and Ngui, Romano and Yusoff Wan Sulaiman, Wan and Vythilingam, Indra and Cliff Simon Divis, Paul and Hasikin, Khairunnisa},
  journal={IEEE Access}, 
  title={Automated Identification of Malaria-Infected Cells and Classification of Human Malaria Parasites Using a Two-Stage Deep Learning Technique}, 
  year={2024},
  volume={12},
  number={},
  pages={135746-135763},
  abstract={The gold standard for diagnosing malaria remains microscopic examination; however, its application is frequently impeded by the lack of a standardized framework that guarantees uniformity and quality, particularly in scenarios with limited resources and high volume. This study suggests a novel and highly effective automated diagnostic approach that employs deep-learning object detectors to improve the accuracy and efficiency of malaria-infected cell detection and Plasmodium species classification to overcome these challenges. Plasmodium parasites were detected within thin blood stain images using the YOLOv4 and YOLOv5 models, which were optimized for this purpose. YOLOv5 obtains a slightly higher accuracy on the source dataset (mAP@ $0.5=96$ %) than YOLOv4 (mAP@ $0.5=89$ %), but YOLOv4 exhibits superior robustness and generalization across diverse datasets, as demonstrated by its performance on an independent validation set (mAP@ $0.5=90$ %). This robustness emphasizes the dependability of YOLOv4 for deployment in a variety of clinical settings. Furthermore, an automated process was implemented to produce bound single-cell images from YOLOv4’s localization outputs, thereby eradicating the necessity for conventional and time-consuming segmentation methods. The DenseNet-121 model, which was optimized for species identification, obtained an impressive overall accuracy of 95.5% in the subsequent classification stage, indicating excellent generalization across all malaria species. Accurate classification of Plasmodium species on microscopically thin blood films is essential for guiding appropriate therapy and preventing unnecessary anti-malarial treatments, which can lead to adverse effects and contribute to drug resistance. This research contributes to the field of automated malaria diagnosis by offering a comprehensive framework that substantially improves clinical decision-making, particularly in resource-limited environments.},
  keywords={Malaria;Microscopy;Deep learning;Cells (biology);Accuracy;Support vector machines;Films;Machine learning;Surveillance;Artificial intelligence;Detection algorithms;Machine learning;deep learning;biosurveillance;AI-monitoring;detection},
  doi={10.1109/ACCESS.2024.3459411},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9587584,
  author={Abdiyeva-Aliyeva, Gunay and Hematyar, Mehran and Bakan, Sefa},
  booktitle={2021 2nd Global Conference for Advancement in Technology (GCAT)}, 
  title={Development of System for Detection and Prevention of Cyber Attacks Using Artifıcial Intelligence Methods}, 
  year={2021},
  volume={},
  number={},
  pages={1-5},
  abstract={Artificial intelligence (AI) technologies have given the cyber security industry a huge leverage with the possibility of having significantly autonomous models that can detect and prevent cyberattacks – even though there still exist some degree of human interventions. AI technologies have been utilized in gathering data which can then be processed into information that are valuable in the prevention of cyberattacks. These AI-based cybersecurity frameworks have commendable scalability about them and are able to detect malicious activities within the cyberspace in a prompter and more efficient manner than conventional security architectures. However, our one or two completed studies did not provide a complete and clear analyses to apply different machine learning algorithms on different media systems. Because of the existing methods of attack and the dynamic nature of malware or other unwanted software (adware etc.) it is important to automatically and systematically create, update and approve malicious packages that can be available to the public. Some of Complex tests have shown that DNN performs maybe can better than conventional machine learning classification. Finally, we present a multiple, large and hybrid DNN torrent structure called Scale-Hybrid-IDS-AlertNet, which can be used to effectively monitor to detect and review the impact of network traffic and host-level events to warn directly or indirectly about cyber-attacks. Besides this, they are also highly adaptable and flexible, with commensurate efficiency and accuracy when it comes to the detection and prevention of cyberattacks.There has been a multiplicity of AI-based cyber security architectures in recent years, and each of these has been found to show varying degree of effectiveness. Deep Neural Networks, which tend to be more complex and even more efficient, have been the major focus of research studies in recent times. In light of the foregoing, the objective of this paper is to discuss the use of AI methods in fighting cyberattacks like malware and DDoS attacks, with attention on DNN-based models.},
  keywords={Deep learning;Scalability;Computer architecture;Telecommunication traffic;Malware;Real-time systems;Computer crime;Black hole;Rushing;Flooding;Wormhole;Neighbor attacks;ANN;GAIS;DNN-based models},
  doi={10.1109/GCAT52182.2021.9587584},
  ISSN={},
  month={Oct},}@ARTICLE{10679777,
  author={Shi, Jiahao and Chen, Cihai and Li, Xiuyan and Chen, Huanting and Lin, Huichuan and Chen, Zhixiang},
  journal={IEEE Transactions on Electron Devices}, 
  title={Optoelectronic Synaptic Transistors via Adding Insulator Into Semiconductor for Brain-Inspired Computing}, 
  year={2024},
  volume={71},
  number={11},
  pages={6989-6995},
  abstract={Recent developments in artificial intelligence (AI) have triggered growing studies in artificial synaptic transistors. However, achieving tunable synaptic behaviors and improving synaptic performance by simple processes is still challenging. Herein, a facile regulation strategy by introducing a polymer insulator to a semiconductor was proposed, for the first time, to implement an optoelectronic synaptic transistor (OST) using a solution-based method. The organic OST comprises a biodegradable polyvinyl alcohol (PVA) electret with hydroxyl group dipoles, enabling the effectively coordinated regulation of synaptic weight and the enhancement of synaptic properties. Essential synaptic characteristics were realized, such as paired-pulse facilitation (PPF), excitatory postsynaptic currents (EPSCs), long-term potentiation, and depression (LTP/D). The results indicate that enhanced synaptic performance should be associated with the improvement of thin film quality and surface morphologies due to blending insulators, as well as the carrier migration between semiconductors and PVA. More importantly, the controllable light logical operation was perfectly simulated. Pattern recognition test of MNIST handwritten digits was also researched based on artificial neural networks (ANNs), a high recognition accuracy of 91.38% was achieved using appropriate insulator blending ratios. Therefore, our work should open a new pathway for designing optoelectronic synapse devices and expand their further application in brain-inspired computing.},
  keywords={Transistors;Insulators;Biology;Synapses;Logic gates;Indexes;Surface morphology;Insulator doping;light logic operation;neuromorphic computing;optoelectronic synaptic transistor (OST);organic semiconductor},
  doi={10.1109/TED.2024.3454033},
  ISSN={1557-9646},
  month={Nov},}@INPROCEEDINGS{10156685,
  author={Bahiya, Ashraf Mustafa Kadhim and Ibrahim, Abdullahi Abdu},
  booktitle={2023 5th International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA)}, 
  title={Efficient Routing in Manet AdHoc Networks Using Metaheuristic Optimization Algorithm}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={In recent years there has been a major shift in the information society, driven by the commercialization and emergence of inexpensive and widely available communications devices (telephones, laptops, etc.) and the evolution of fixed and mobile networks. Nowadays, the exchange of information at any time has generated the need for more mobility, which has made the notion of networks without infrastructure, or Ad Hoc networks, very widespread. The general objective of this thesis is to propose a routing protocol for MANETs networks based on the behavior of fungal colonies, test it in different scenarios and situations, as well as compare its performance with competing protocols. The protocol aims to operate with sufficient functionality and performance for the network to provide quality services for low-traffic scenarios. Due to the frequent topology changes due to the mobility of the nodes, the protocol proposes efficient mechanisms for: discovery, maintenance and failure recovery. In addition, the protocol must have an efficient heuristic for selecting the best route, and a path optimization algorithm. In this new approach, we sought to discover which information within the universe of fungi could be useful to compose a new protocol for networks. The nutrient search mechanism, the construction of hyphae and the mass flow in the fungal colony, inspire the design of a working model of protocol operation.},
  keywords={Fungi;Technological innovation;Routing;Telephone sets;Ad hoc networks;Routing protocols;Organisms;ADHOC;MANET;AI;Fungi Optimization},
  doi={10.1109/HORA58378.2023.10156685},
  ISSN={},
  month={June},}@ARTICLE{10586988,
  author={Dias, Duarte and Luís, Miguel and Rito, Pedro and Sargento, Susana},
  journal={IEEE Access}, 
  title={Estimating Quality-of-Service in Urban Vehicular Networks Through Machine Learning}, 
  year={2024},
  volume={12},
  number={},
  pages={96449-96461},
  abstract={Machine Learning (ML) has emerged as a promising tool for addressing complex challenges in multiple domains. In the context of Vehicular Ad-Hoc Networks (VANETs), ML has gained much more attention due to its ability to solve major known problems in areas such as traffic management, road safety and communication infrastructure management. In a VANET, vehicles generate a significant amount of data, which can be explored to, for example, enhance the network management regarding the connectivity between the vehicles and the infrastructure. This work studies the performance of ML models regarding the estimation of the Quality-of-Service of different network access technologies (ITS-G5 and 5G) in urban vehicular environments. To this end, data collection campaigns were carried out throughout the city of Aveiro, Portugal, which included vehicular and network performance data for ITS-G5 and 5G cellular technologies. After an initial characterization of the data collected, several ML algorithms were trained, considering different combinations of features (represented by the collected metrics). The results have shown that, for the same configurations, similar estimation errors were obtained by the Random Forest Regression and the Extreme Gradient Boosting algorithms, with the last one presenting a shorter estimation time. The results also show that location-independent configurations, i.e., when no geographic positions are used in the ML model, are slightly worse than GPS-based ML models, creating the possibility of being applied in different urban environments, making them quite versatile.},
  keywords={Urban areas;Measurement;Quality of service;Data collection;5G mobile communication;Maximum likelihood estimation;Predictive models;Intelligent transportation systems;Machine learning;Performance evaluation;Intelligent transport systems;machine learning;vehicular networks;ITS-G5 and 5G;vehicle data collection;network performance},
  doi={10.1109/ACCESS.2024.3424304},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10018734,
  author={Sokol, Oleksandra and Naumenko, Heorhii and Derkach, Viacheslav and Kuznetsov, Vasyl and Progonov, Dmytro and Husiev, Volodymyr},
  booktitle={2022 12th International Conference on Dependable Systems, Services and Technologies (DESSERT)}, 
  title={Automatic Speaker Verification on Compressed Audio}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={Voice-based phishing and wire-fraud attacks have become a topical problem in the recent years due to the emergence of advanced AI-based speech synthesis models. These models can generate realistic speech signal of a known target that is difficult to differentiate from a bonafide voice of real human. This proved to be an issue by the recent security reports related to vishing attacks on bank call centers, fraud or pranking of public figures, and spoofing of voice authentication systems. Current approaches to address voice fraud issue are based on applying an Automatic Speaker Verification (ASV) system. In most cases, these systems are tuned on datasets that consist of wideband quality bonafide and spoofed voice samples. This makes ASV systems vulnerable to speech signal degradation caused by voice encoding in cellular network and Voice over IP (VoIP). However, performance evaluation of ASV (namely Equal Error Rate (EER) estimation) is almost exclusively available only for the cellular networks. Thus, performance of ASV systems for modern VoIP applications remains unclear. In this paper, we evaluate the modern ASV systems on audio compressed with codecs used in both cellular networks (AMR and GSM codecs) and VoIP applications (G.711, G.722, AAC, Lyra and Opus codecs). In addition, ASV performance was tested using popular VoIP application (Discord). Obtained results have shown that codec application results in considerable (up to two times) EER increase compared to the baseline results. Moreover, we observed up to three times increase in EER on data transmitted using Discord. We propose to apply hard samples mining to the training process in order to improve the accuracy of ASV systems on compressed voice samples. It allows to reduce EER from 21% down to 16% even for the most distorted samples obtained after aggressive voice compression by GSM codecs. Note, that improvement for real VoIP application is even higher - with EER on Discord data decrease from 35% to 20%.},
  keywords={Cellular networks;Training;Performance evaluation;GSM;Degradation;Codecs;Error analysis;Automatic Speaker Verification;VoIP;Hard Samples Mining},
  doi={10.1109/DESSERT58054.2022.10018734},
  ISSN={},
  month={Dec},}@ARTICLE{8573574,
  author={Khan, Muhammad Fahad and Aadil, Farhan and Maqsood, Muazzam and Bukhari, Syed Hashim Raza and Hussain, Maqbool and Nam, Yunyoung},
  journal={IEEE Access}, 
  title={Moth Flame Clustering Algorithm for Internet of Vehicle (MFCA-IoV)}, 
  year={2019},
  volume={7},
  number={},
  pages={11613-11629},
  abstract={A network of wirelessly connected vehicles by using any mean of connectivity is termed as the Internet of Vehicle (IoV). Managing this type of network is a challenging task. Clustering is a technique to efficiently manage resources in this type of network. In a cluster, all inter/intra cluster communication is managed by a cluster head (CH). Load on each CH, the lifetime of the cluster and the total number of clusters in a network are some parameters to measure the efficiency of the network. In this paper, a novel technique based on moth flame clustering algorithm for IoV (MFCA-IoV) is proposed. Moth flame optimizer is a nature-inspired algorithm. MFCA-IoV generates optimized clusters for robust transmission and is evaluated experimentally with renowned techniques. These techniques are Grey-Wolf-optimization-based method used for the clustering called as GWOCNETs, multi-objective particle-swarm-optimization (MOPSO), clustering algorithm based on Ant colony optimization for vehicular ad-hoc networks termed as CACONET and comprehensive learning particle-swarm-optimization (CLPSO). To assess the comparative efficiency of these algorithms, numerous experiments are performed. The parameters like network grid-size, number of nodes, speed, direction, and transmission-range of the nodes are considered for optimized clustering. The results indicate, MFCA-IoV is showing 73% nodes, which are not selected as a cluster head while existing techniques are providing 57%, 50%, 51%, and 58% for GWOCNETs, CLPSO, MOPSO, and CACONET, respectively. Hence, lesser the nodes are selected as CH, the more optimal result will be considered.},
  keywords={Clustering algorithms;Ad hoc networks;Protocols;Computer science;Fires;Measurement;Internet;Internet of Vehicle (IoV);vehicular ad-hoc networks (VANETs);intelligent transportation system (ITS);Ant-colony-optimization (ACO);particle swarm optimization (PSO);MFO;clustering;meta-heuristic algorithms;population-based algorithm},
  doi={10.1109/ACCESS.2018.2886420},
  ISSN={2169-3536},
  month={},}@ARTICLE{9432814,
  author={Kwon, Hyun},
  journal={IEEE Access}, 
  title={Friend-Guard Textfooler Attack on Text Classification System}, 
  year={2025},
  volume={13},
  number={},
  pages={3841-3848},
  abstract={Deep neural networks provide good performance for image classification, text classification, speech classification, and pattern analysis. However, such neural networks are vulnerable to adversarial examples. An adversarial example is a sample created by adding a little noise to the original sample data and that, although presenting no change identifiable to human perception, will be misclassified by a deep neural network. Most studies on adversarial examples have focused on images, but research is expanding to include the field of text. Textual adversarial examples can be useful in certain situations, such as when models of both friend and enemy coexist, as in a military scenario. Here, a specific message may be generated as an adversarial example such that no grammatical or semantic problems are apparent to human perception and it will be correctly classified by the friend model but incorrectly classified by the enemy model. In this paper, I propose a “friend-guard” textual adversarial example for a text classification system. Unlike the existing methods for generating image adversarial examples, the proposed method creates adversarial examples designed to be misclassified by an enemy model and correctly classified by a friend model while retaining the meaning and grammar of the original sentence by replacing words of importance with substitutions. Experiments were conducted using a movie review dataset and the TensorFlow library. The experimental results show that the proposed method can generate an adversarial example that will be correctly classified with 88.2% accuracy by the friend model and 26.1% accuracy by the enemy model.},
  keywords={Bit error rate;Neural networks;Grammar;Text recognition;Distortion;Context modeling;Tokenization;Machine learning;text classification;text adversarial example;evasion attack;deep neural network (DNN)},
  doi={10.1109/ACCESS.2021.3080680},
  ISSN={2169-3536},
  month={},}@ARTICLE{10061675,
  author={Dong, Nannan and Yin, Hao and Ren, Baoquan and Li, Hongjun and Gong, Xiangwu and Zhong, Xudong and Han, Junmei and Lyu, Jiazheng},
  journal={China Communications}, 
  title={Research on network cognition model and mechanism of intelligent information network}, 
  year={2023},
  volume={20},
  number={2},
  pages={257-277},
  abstract={Intellectualization has been an inevitable trend in the information network, allowing the network to achieve the capabilities of self-learning, self-optimization, and self-evolution in the dynamic environment. Due to the strong adaptability to the environment, the cognitive theory methods from psychology gradually become an excellent approach to construct the intelligent information network (IIN), making the traditional definition of the intelligent information network no longer appropriate. Moreover, the thinking capability of existing IINs is always limited. This paper redefines the intelligent information network and illustrates the required properties of the architecture, core theory, and critical technologies by analyzing the existing intelligent information network. Besides, we innovatively propose a novel network cognition model with the network knowledge to implement the intelligent information network. The proposed model can perceive the overall environment data of the network and extract the knowledge from the data. As the model's core, the knowledge guides the model to generate the optimal decisions adapting to the environmental changes. At last, we present the critical technologies needed to accomplish the proposed network cognition model.},
  keywords={Artificial intelligence;Optimization;Knowledge engineering;Wireless networks;Internet;Electromagnetics;Decision making;intelligent information network;network cognition;multi-domain perception;network knowledge},
  doi={10.23919/JCC.2023.02.017},
  ISSN={1673-5447},
  month={Feb},}@INPROCEEDINGS{9223595,
  author={Gao, Xiaofeng and Gong, Ran and Zhao, Yizhou and Wang, Shu and Shu, Tianmin and Zhu, Song-Chun},
  booktitle={2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)}, 
  title={Joint Mind Modeling for Explanation Generation in Complex Human-Robot Collaborative Tasks}, 
  year={2020},
  volume={},
  number={},
  pages={1119-1126},
  abstract={Human collaborators can effectively communicate with their partners to finish a common task by inferring each other’s mental states (e.g., goals, beliefs, and desires). Such mind-aware communication minimizes the discrepancy among collaborators’ mental states, and is crucial to the success in human ad-hoc teaming. We believe that robots collaborating with human users should demonstrate similar pedagogic behavior. Thus, in this paper, we propose a novel explainable AI (XAI) framework for achieving human-like communication in human-robot collaborations, where the robot builds a hierarchical mind model of the human user and generates explanations of its own mind as a form of communications based on its online Bayesian inference of the user’s mental state. To evaluate our framework, we conduct a user study on a real-time human-robot cooking task. Experimental results show that the generated explanations of our approach significantly improves the collaboration performance and user perception of the robot. Code and video demos are available on our project website: https://xfgao.github.io/xCookingWeb/.},
  keywords={Codes;Conferences;Collaboration;Streaming media;Real-time systems;Cognitive science;Bayes methods},
  doi={10.1109/RO-MAN47096.2020.9223595},
  ISSN={1944-9437},
  month={Aug},}@INPROCEEDINGS{10690124,
  author={Vimaladevi, M. and Thangamani, R. and S, Priyadharshini and B, Varshini and A, Tarun},
  booktitle={2024 5th International Conference on Electronics and Sustainable Communication Systems (ICESC)}, 
  title={Prediction of Alzheimer's Disease by Analyzing Handwriting Dynamics Using Machine Learning Algorithms}, 
  year={2024},
  volume={},
  number={},
  pages={1298-1304},
  abstract={Alzheimer's disease is a degenerative neurological condition that impairs everyday functioning and quality of life. Handwritten data scripts are related in the determination of Alzheimer's disease with the use of handwriting patterns, which can reveal early signs of cognitive decline and motor impairments. Changes in handwriting, such as tremors, irregularities, and slower writing speeds, can be indicative of the onset and progression of Alzheimer's, aiding in early diagnosis and monitoring of the disease. Traditionally, statistical tests or conventional classification models have been used in handwriting analysis studies. Machine learning techniques have only lately been investigated, and they were mostly used to identify Parkinson's disease rather than Alzheimer's. This research work aims to close this gap and investigate this by developing a machine learning model that uses handwriting data to categorize an individual as an Alzheimer patient. This research aims to advance the science of Alzheimer's disease prediction by examining the use of handwritten data as a unique medium for early diagnosis. PCA is one of the data preprocessing parameters that are employed, while the linear support vector classifier, random forest classifier, and XGBoost, together with their accuracy, are the model training parameters. Each participant's handwriting sample yielded 451 handwriting features, such as air_time1, disp_index1, gmrt_in_air1, gmrt_on_paper1, mean_gmrt1, etc., which are considered as the input parameters. A binary class label indicating whether a person is healthy or may have Alzheimer's disease is considered as the expected output.},
  keywords={Training;Parkinson's disease;Writing;Motors;Data models;Vectors;Alzheimer's disease;Random forests;Diseases;Principal component analysis;Neurological conditions;Health information;Alzheimer's disease prognosis;Analysis of handwriting;Categorization;Merging techniques},
  doi={10.1109/ICESC60852.2024.10690124},
  ISSN={2996-5357},
  month={Aug},}@INPROCEEDINGS{10024464,
  author={Bordier, Jean-Baptiste and Mirabile, Anthony and Courant, Robin and Christie, Marc},
  booktitle={2022 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)}, 
  title={Smart Motion Trails for Animating in VR}, 
  year={2022},
  volume={},
  number={},
  pages={64-72},
  abstract={The artistic crafting of 3D animations by designers is a complex and iterative process. While classical animation tools have brought significant improvements in creating and manipulating shapes over time, most approaches rely on classical 2D input devices to create 3D contents. With the advent of virtual reality technologies and their ability to dive the users in their 3D worlds and to precisely track devices in 6 dimensions (position and orientation), a number of VR creative tools have emerged such as Quill, AnimVR, Tvori, Tiltbrush or MasterPieceVR. While these tools provide intuitive means to directly design in the 3D space by exploiting both the 6D tracking capacity of the hand devices and the stereoscopic perception by the user, the animation capacities or such tools remain strongly limited, and often reproduce classical 2D manipulators in VR. In this work, we propose the design of smart interactive manipulators which leverage on the specificity of VR to animate poly-articulated animations. We then perform a user study to evaluate the benefits of such manipulators over traditional 2D tools for three groups of users: beginner, intermediate, and professional artists. We build on this user to discuss how smart tools (e.g. using a variety of AI techniques) can be coupled with VR technologies to improve content creation.},
  keywords={Performance evaluation;Three-dimensional displays;Shape;Stereo image processing;Input devices;Virtual reality;Animation;computer animation;VR;user evaluation},
  doi={10.1109/AIVR56993.2022.00016},
  ISSN={2771-7453},
  month={Dec},}@INPROCEEDINGS{10212171,
  author={N, Nimitha and Bala, Saroj and Satheeswaran, V and Janani, M and Selvanayaki, S. and Ramasami, S.},
  booktitle={2023 2nd International Conference on Edge Computing and Applications (ICECAA)}, 
  title={Deep Learning based Sentiment Analysis on Images}, 
  year={2023},
  volume={},
  number={},
  pages={568-575},
  abstract={Sentiment analysis is a powerful tool that has been gaining popularity among businesses in recent years. It involves analyzing text, speech, or images to understand the emotions or opinions conveyed by them. This process can provide valuable insights into customer preferences, behavior, and satisfaction levels. By leveraging sentiment analysis, businesses can gather customer feedback on their products and services, which can help them identify areas for improvement and develop stronger customer relationships. Furthermore, sentiment analysis can also be used to support emotional marketing campaigns, allowing businesses to create more targeted and personalized messaging that resonates with their audience. For example, by analyzing social media posts and comments, businesses can gain a better understanding of how their brand is perceived and adjust their marketing strategies accordingly. One popular method for this is transfer learning, where pre-trained models are used to analyze new datasets. Deep learning algorithms, such as convolutional neural networks (CNNs), have been particularly successful in achieving accurate results in a variety of applications, including image sentiment analysis. However, analyzing emotions conveyed through images is a complex task, and there is still much room for improvement. The Inception-v3 technique is a notable development in this field, as it can easily identify key parts of the body, such as the face, which is essential for accurately detecting emotions. The results of this model were compared to those of various other machine learning approaches, and the suggested model showed superior accuracy levels of up to 99.5%. This research demonstrates the potential of using deep learning algorithms and transfer learning methods to improve image sentiment analysis. For example, in the field of Healthcare, image sentiment analysis can be used to detect emotional expressions of patients, which can be valuable for assessing their mental and emotional state. In Sales and Marketing, it can be used to evaluate customers' emotional responses to different products or campaigns, allowing businesses to tailor their strategies accordingly. Overall, the use of image sentiment analysis can provide valuable insights to businesses across a wide range of industries.},
  keywords={Deep learning;Sentiment analysis;Analytical models;Social networking (online);Computational modeling;Transfer learning;Robustness;Sentiment Analysis;CNN;Transfer Learning;Image processing},
  doi={10.1109/ICECAA58104.2023.10212171},
  ISSN={},
  month={July},}@INPROCEEDINGS{10972954,
  author={Akmatbekov, Ulan and Sprenger, Janis and Xu, Rui and Slusallek, Philipp},
  booktitle={2025 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Simulating Body Movements for Multiple Agent Avoidance}, 
  year={2025},
  volume={},
  number={},
  pages={730-735},
  abstract={Animation of virtual characters in a crowd is one of the unavoidable hurdles artists and developers encounter throughout their work in different fields, such as cinematography, simulations, and game development. Depending on the requirements for the quality of motions, various techniques are used to overcome this constantly rising problem, each with its own shortcomings. Ignoring the interaction between individuals by using simple hitboxes to avoid collisions produces unrealistic movement patterns, while the manual animation of characters is a tedious and costly endeavor. This work discusses the implementation of an automated system capable of realistic human motion synthesis for multiple avatars, focusing on the interaction between individuals. The method is based on an autoregressive, data-driven conditional variational autoencoder and an additional control policy providing the latent input vector for the motion network trained with reinforcement learning. Different virtual sensor types that perceive a variable number of individuals and obstacles around the agent are proposed and evaluated. The system provides the simulation of the body movements in a multi-agent environment. It can operate at runtime with an adequate frame rate in environments containing more than 20 characters.},
  keywords={Solid modeling;Three-dimensional displays;Runtime;Soft sensors;Autoencoders;Reinforcement learning;User interfaces;Animation;Motion capture;Vectors;Data-Driven Animation;Motion Capture;Reinforcement Learning;Crowd Simulation},
  doi={10.1109/VRW66409.2025.00148},
  ISSN={},
  month={March},}@ARTICLE{10220432,
  author={Kang, Hosan and Yang, Jinseong and Ko, Beom-Seok and Kim, Bo-Seong and Song, Oh-Young and Choi, Soo-Mi},
  journal={IEEE Computer Graphics and Applications}, 
  title={Integrated Augmented and Virtual Reality Technologies for Realistic Fire Drill Training}, 
  year={2024},
  volume={44},
  number={2},
  pages={89-99},
  abstract={In this article, we propose a novel fire drill training system designed specifically to integrate augmented reality (AR) and virtual reality (VR) technologies into a single head-mounted display device to provide realistic as well as safe and diverse experiences. Applying hybrid AR/VR technologies in fire drill training may be beneficial because they can overcome limitations such as space–time constraints, risk factors, training costs, and difficulties in real environments. The proposed system can improve training effectiveness by transforming arbitrary real spaces into real-time, realistic virtual fire situations, and by interacting with tangible training props. Moreover, the system can create intelligent and realistic fire effects in AR by estimating not only the object type but also its physical properties. Our user studies demonstrated the potential of integrated AR/VR for improving training and education.},
  keywords={Training;Portals;Object recognition;X reality;Three-dimensional displays;Artificial intelligence;Visualization;Fire safety;Emergency services},
  doi={10.1109/MCG.2023.3303028},
  ISSN={1558-1756},
  month={March},}@ARTICLE{8585041,
  author={Huang, Siyu and Li, Xi and Zhang, Zhongfei and Wu, Fei and Han, Junwei},
  journal={IEEE Transactions on Image Processing}, 
  title={User-Ranking Video Summarization With Multi-Stage Spatio–Temporal Representation}, 
  year={2019},
  volume={28},
  number={6},
  pages={2654-2664},
  abstract={Video summarization is a challenging task, mainly due to the difficulties in learning complicated semantic structural relations between videos and summaries. In this paper, we present a novel supervised video summarization scheme based on three-stage deep neural networks. The scheme takes a divide-and-conquer strategy to resolve the complicated task of 3D video summarization into a set of easy and flexible computational subtasks, and then to sequentially perform 2D CNNs, 1D CNNs, and long short-term memory to address the subtasks in an hierarchical fashion. The hierarchical modeling of spatio-temporal structure leads to high performance and efficiency. In addition, we propose a simple but effective user-ranking method to cope with the labeling subjectivity problem of user-created video summarization, leading to the labeling quality refinement for robust supervised learning. Experimental results show that our approach outperforms the state-of-the-art video summarization methods on two benchmark datasets.},
  keywords={Two dimensional displays;Context modeling;Three-dimensional displays;Computational modeling;Supervised learning;Convolution;Task analysis;Video summarization;recurrent neural network;convolutional neural network;multi-user inconsistency;user ranking},
  doi={10.1109/TIP.2018.2889265},
  ISSN={1941-0042},
  month={June},}@INPROCEEDINGS{10391508,
  author={Kumar, Ankit and Singh, Kamred Udham and Kumar, Gaurav and Choudhury, Tanupriya and Kotecha, Ketan},
  booktitle={2023 7th International Symposium on Innovative Approaches in Smart Technologies (ISAS)}, 
  title={Combating VoIP Spam: A Real-Time Algorithmic Approach}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Spam calls, formerly a relatively insignificant issue due to the restricted usage of VOIP (Voice over Internet Protocol) services, are now on the verge of becoming a serious problem because of the growing use of internet-based telephone systems. Because it provides phone service over an internet connection rather than conventional phone lines, VOIP (Voice over Internet Protocol) has cheaper prices and more features than traditional PSTN phones, which has led to its growing acceptance alongside PSTN phones. However, this development has generated new security and quality concerns, such as call eavesdropping, service theft, call manipulation, and notably spamming. Spamming is particularly problematic because of its widespread nature. Because each VoIP account is tied to an IP address, spam in VOIP poses a particularly serious threat. Spammers are able to flood voicemail systems with messages, which are often accompanied by viruses and malware. These may lower the quality of calls or even cause VoIP servers to collapse because they use an excessive amount of bandwidth. At this moment, there is no technology that is both perfect and real-time that can efficiently filter spam calls out of VoIP traffic. In this research, I provide a revolutionary real-time algorithm that utilises a mix of techniques in order to get rid of spam calls. These techniques include verifying fuzzy RTP media parameters, evaluating fuzzy MOS value, performing Turing tests, and analysing specific call information. The effectiveness of this system is proved by its capacity to monitor and evaluate the operation of VOIP systems in real time. It is able to reject calls that are thought to be spam at different stages of the verification process. The aforementioned algorithm, which has been developed for incorporation into VOIP systems, provides a potentially useful solution for the rejection of VOIP spam calls in real time.},
  keywords={Unsolicited e-mail;Media;Telephone sets;Information filters;Real-time systems;Security;Communication networks;VoIP;Real-Time Systems;Fuzzy Logic;Spam Detection;MATLAB Simulations;Speech Quality;Measurement;Network Security},
  doi={10.1109/ISAS60782.2023.10391508},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10889186,
  author={Zhang, Zeren and Qin, Haibo and Huang, Jiayu and Cheng, Jo-Ku and Li, Yixin and Lin, Hui and Duan, Yitao and Ma, Jinwen},
  booktitle={ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={SwapTalk: Audio-Driven Talking Face Generation with One-Shot Customization in Latent Space}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Combining face-swapping with lip synchronization offers a cost-effective solution for generating customized talking faces. However, directly cascading existing models can introduce significant interference and reduce video clarity due to limited interaction space in the low-level RGB domain. To solve this, we propose SwapTalk, a unified framework that performs face-swapping and lip synchronization within the same latent VQ-embedding space, known for its editability and fidelity. We enhance generalization to unseen identities with identity loss in the face-swapping module and improve synchronization quality with expert discriminator supervision. To better approximate real-world applications, we expand the evaluation scope to asynchronous audio-video scenarios. Furthermore, we introduce a novel identity consistency metric to more comprehensively assess the identity consistency over time series in generated facial videos. Experiments on HDTF show that SwapTalk outperforms existing methods in video quality, lip synchronization accuracy, face-swapping fidelity, and identity consistency.},
  keywords={Measurement;Visualization;Accuracy;Lips;Time series analysis;Interference;Synchronization;Speech processing;Faces;Video recording;face-swapping;talking face generation;audio driven animation},
  doi={10.1109/ICASSP49660.2025.10889186},
  ISSN={2379-190X},
  month={April},}@ARTICLE{9815036,
  author={Gaba, Priyanka and Raw, Ram Shringar and Mohammed, Mazin Abed and Nedoma, Jan and Martinek, Radek},
  journal={IEEE Access}, 
  title={Impact of Block Data Components on the Performance of Blockchain-Based VANET Implemented on Hyperledger Fabric}, 
  year={2022},
  volume={10},
  number={},
  pages={71003-71018},
  abstract={Blockchain, a vital technology in today’s era, changed the way we share, manage and exchange our data in a centralized way to decentralized architecture. With the increasing demand for Blockchain, various platforms are available to implement public, private, consortium, or permissioned, permissionless Blockchain. Hyperledger, an open-source, permissioned, distributed ledger-based Blockchain, was hosted by Linux. This paper explores Hyperledger Fabric Private Blockchain Network (HFPBN). The architecture of HFPBN with its components and transaction flow is explored in detail. The Blockchain in HFPBN comprises multiple blocks that are linked to each other. The block elements are discussed in detail with their type and size, and after that, the total size of the block depending upon various parameters is calculated. Further, one application of Blockchain, i.e., Vehicular Ad-hoc Networks (VANETs), is discussed in this paper as a case study. The VANET application is implemented on the Hyperledger Fabric platform. Formulas showing the dependency of various parameters like endorsement policy, number of transactions, and number of reads and writes on block size are derived and shown in their relationship through the graph for the VANET system. The impact of block size on various performance parameters like throughput, latency, memory, and CPU utilization for the VANET system is then analyzed using Hyperledger Caliper. An optimal required value of throughput and latency is achieved for Blockchain-based VANET. Also, the Hyperledger Fabric platform seems suitable for many applications as it creates separate Blockchain for different applications.},
  keywords={Blockchains;Vehicular ad hoc networks;Distributed ledger;Fabrics;Peer-to-peer computing;Smart contracts;Throughput;Blockchain;Hyperledger Caliper;Hyperledger fabric;VANET},
  doi={10.1109/ACCESS.2022.3188296},
  ISSN={2169-3536},
  month={},}@ARTICLE{9444363,
  author={Antico, M. and Little, J. P. and Jennings, H. and Askin, G. and Labrom, R. D. and Fontanarosa, D. and Pivonka, P.},
  journal={IEEE Access}, 
  title={Deep Learning-Based Automatic Segmentation for Reconstructing Vertebral Anatomy of Healthy Adolescents and Patients With Adolescent Idiopathic Scoliosis (AIS) Using MRI Data}, 
  year={2021},
  volume={9},
  number={},
  pages={86811-86823},
  abstract={MRI is a non-ionising imaging modality that could be used as an alternative to Xray-based imaging methods to accurately assess the 3D morphology of the vertebral anatomy of scoliosis patients. However, a major caveat in utilising MRI is the significant amount of time required to manually segment the anatomy of interest. To overcome this limitation, we implemented a fully automatic method for the 3D segmentation of thoracic vertebrae, including vertebral body and posterior elements, of healthy adolescents and patients with Adolescent Idiopathic Scoliosis (AIS) using MRI data. 62 MRI scans were obtained from 3 healthy volunteers and 25 patients with AIS. A state-of-the-art deep-learning network for segmentation was trained using image patches of the apical vertebra (T7, T8, T9 or T10) extracted from 20 AIS patient MRIs. Ad-hoc data augmentation was adopted to represent the unlabeled vertebral levels in the dataset (T5-T6, T11-T12). The vertebral levels T5-T12 were then segmented for the remaining MRI datasets by feeding to the network the MRI patches generated by translating a window of fixed size and stride onto the MRI volume. The mean dice score coefficient for the AIS patient vertebral levels T5-T12 was of 87% ± 4.3%, which was comparable to the performance achieved by two experts. On average, 93% and 97% of the MRI segmented slices were considered clinically acceptable morphological reconstructions of AIS and healthy volunteer vertebrae, respectively. The proposed method can be considered as the first step towards more routine MRI-based imaging of AIS osseous deformities, reducing the cumulative exposure of young patients to ionising radiation.},
  keywords={Magnetic resonance imaging;Artificial intelligence;Three-dimensional displays;Image segmentation;Training;Surgery;Pathology;Deep learning;CNN;segmentation;scoliosis;MRI;vertebral body;posterior process;AIS},
  doi={10.1109/ACCESS.2021.3084949},
  ISSN={2169-3536},
  month={},}@ARTICLE{9656902,
  author={Nurlan, Zhanserik and Zhukabayeva, Tamara and Othman, Mohamed and Adamova, Aigul and Zhakiyev, Nurkhat},
  journal={IEEE Access}, 
  title={Wireless Sensor Network as a Mesh: Vision and Challenges}, 
  year={2022},
  volume={10},
  number={},
  pages={46-67},
  abstract={Nowadays, network technologies are developing very rapidly. The growing volume of transmitted information (video, data, VoIP, etc.), the physical growth of networks, and inter-network traffic are forcing manufacturers to produce more powerful and “smart” devices that use new methods of transferring and sorting data. Such connected smart devices (IoT) are used in intelligently controlled traffic for self-driving vehicles in Vehicle Adhoc Networks (VANET), in electricity and water in smart cities, and in-home automation in smart homes. These types of connected Internet of Things (IoT) devices are used to leverage different types of network structures. Such IoT sensor devices can be deployed as a wireless sensor network (WSN) in a mesh topology. Both WSNs and Wireless Mesh Networks (WMNs) are easy to organize as well as to deploy. In this case, there are many reasons for combining these different types of networks. In particular, the detailed sensory capabilities of sensor networks may be improved by increasing bandwidth, reliability and power consumption in the mesh topology. However, there are currently only a handful of studies devoting to integrate these two different types of networks. In addition, there is no systematic review of existing interconnection methods. That is why in this article we explore the existing methods of these two networks and provide an analytical basis for their relationship. We introduce the definition of WSN and WMN and then look at some case studies. Afterward, we present several challenges and opportunities in the area of combined Wireless Mesh Sensor Network (WMSN) followed by a discussion on this interconnection through literature review and hope that this document will attract the attention of the community and inspire further research in this direction.},
  keywords={Wireless sensor networks;Internet of Things;Topology;Receivers;Network topology;Monitoring;Mesh networks;IoT;wireless mesh network;wireless sensor network;wireless mesh sensor network},
  doi={10.1109/ACCESS.2021.3137341},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10873968,
  author={Rosyadi, Mohammad As’ad and Purnomo, Muhammad Fauzan Edy and Sakti, Setyawan Purnomo},
  booktitle={2023 International Conference on Artificial Intelligence Robotics, Signal and Image Processing (AIRoSIP)}, 
  title={Smart Hydroponic-Based IoT using Fuzzy Bayes to Control Vapor Pressure Deficit (VPD) in Lettuce Plant}, 
  year={2023},
  volume={},
  number={},
  pages={16-21},
  abstract={Vegetable crops can be affected by a Vapor Pressure Deficit (VPD). VPD value affects plant growth and development. There are many Internet of Things (IoT) devices that can help in the agricultural sector, but there is a lack of devices that can detect, especially VPD problems. For this reason, it is necessary to calibrate the VPD value using the fuzzy algorithm and the Bayes method via IoT. In this study, we used temperature, humidity, water quality, pH, soil moisture, and oxygen sensors to determine the VPD value and control growth. The readings from all sensors are processed by ESP 32 and sent to the MySQL database, then the process of fuzzy Bayes runs on the server. Based on that study, based on this study, lettuce plants that underwent control produced taller plants, more leaves, and higher fresh weight. In addition, this method achieves a success rate of up to 87% and is faster than manual calculations.},
  keywords={Temperature sensors;Image processing;Soil moisture;Water quality;Bayes methods;Sensors;Temperature control;Internet of Things;Servers;Robots;Bayes;Fuzzy;IoT;Lettuce plants;VPD},
  doi={10.1109/AIRoSIP58759.2023.10873968},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10930025,
  author={Choi, Hyoungki and Kim, Seungwoo and Jang, Jinbeum and Paik, Joonki},
  booktitle={2025 IEEE International Conference on Consumer Electronics (ICCE)}, 
  title={Pose-Guided Person Image Synthesis with Hybrid Appearance Encoding}, 
  year={2025},
  volume={},
  number={},
  pages={1-4},
  abstract={Despite significant advancements in pose-guided image synthesis techniques using diffusion models, many results still appear unrealistic and unnatural due to insufficient semantic understanding of the input person images by the synthesis models. This paper introduces an enhanced deep neural network for pose-guided person image synthesis. The proposed network integrates both global and local information by employing Swin Transformers and a CNN-based Fourier convolution block during the encoding process of a person image. This approach generates more natural and refined images across various poses and appearances. Furthermore, we improve the appearance encoding process by utilizing VQ-VAE (Vector Quantized Variational AutoEncoder) to compress appearance information. Our method demonstrates outstanding results in preserving details and reducing overfitting. Additionally, the generated images can be effectively used as training data for applications such as object detection, object re-identification, and abnormal behavior analysis.},
  keywords={Image coding;Image synthesis;Convolution;Semantics;Training data;Object detection;Transformers;Vectors;Hybrid power systems;Overfitting;pose;person image synthesis;Swin transformer;Fourier convolution;VQ-VAE;image generation},
  doi={10.1109/ICCE63647.2025.10930025},
  ISSN={2158-4001},
  month={Jan},}@INPROCEEDINGS{10776736,
  author={Marzougui, Maryem and Rabeh, Mohamed Ben and Kanzari, Mounir},
  booktitle={2024 IEEE International Conference on Artificial Intelligence & Green Energy (ICAIGE)}, 
  title={Analysis of the Influence of Annealing on Crystallite Sizes in Na-Doped Cu2ZnSnS4 Using X-Ray Diffraction}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper investigates the impact of thermally annealed and Na incorporated Cu2ZnSnS4 (CZTS) material. Particle size is an important determinant of material properties. It can be increased by thermal treatment and or by the addition of additives that promote grain size expansion in poly-crystalline materials. Sodium-induced CZTS films produced by thermal evaporation were investigated in this work. Thin samples were produced by evaporation on glass heated to 150°C then annealed at 400°C under two different atmospheres: (1) a vacuum heating atmosphere (Tvacuum) and (2) a sulfur heating atmosphere (Tsulfur). X-ray diffraction (XRD) was employed to determine structural properties like crystallite size and lattice strain using Debye-Scherrer (D-S), Williamson-Hall (W-H), Size-Strain Plot (SSP), Halder-Wagner (H-W)and strain distribution (S-D) models. Acceptable results were obtained with all of these models. The XRD intensities of two different atmospheres were then utilized to determine the degree of crystallinity and the texture coefficient In accordance with the results obtained, the largest particle size, high degree of crystallinity and texture coefficient were found in samples annealed in a vacuum atmosphere and with sodium doping, in particular with 1% doping.},
  keywords={Heating systems;Annealing;X-ray scattering;Atmospheric modeling;Sodium;Doping;X-ray diffraction;Mathematical models;Semiconductor process modeling;Strain;CZTS;Sodium-doping;thermal evaporation;X-ray Diffraction;vacuum annealing;sulfur annealing},
  doi={10.1109/ICAIGE62696.2024.10776736},
  ISSN={},
  month={Oct},}@ARTICLE{9446993,
  author={Kim, Jeong-Jae and On, Byung-Won and Lee, Ingyu},
  journal={IEEE Access}, 
  title={High-Quality Train Data Generation for Deep Learning-Based Web Page Classification Models}, 
  year={2021},
  volume={9},
  number={},
  pages={85240-85254},
  abstract={The current deep learning models detecting relevant web pages show low accuracy because of the poor quality of the training data. In this paper, we propose a novel algorithm to automatically generate high-quality training data based on the frequency of the document including the entity of interest. Our experimental results with movies and cellphones data sets show that the average F1-score of the deep learning models (FNN, CNN, Bi-LSTM, and SeqGAN) trained with our proposed algorithm shows up to 0.9992 in F1-score.},
  keywords={Web pages;Deep learning;Data models;Training data;Data mining;Complexity theory;Training;Text classification;deep learning;automatic labelling},
  doi={10.1109/ACCESS.2021.3086586},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9283870,
  author={Chamberlain, Luanne Burns and Davis, Lauren Eisenberg and Stanley, Martin and Gattoni, Brian R.},
  booktitle={2020 IEEE Security and Privacy Workshops (SPW)}, 
  title={Automated Decision Systems for Cybersecurity and Infrastructure Security}, 
  year={2020},
  volume={},
  number={},
  pages={196-201},
  abstract={This paper describes and discusses the impact of using automated decision systems (ADS), or decision automation, on the spectrum from decision support systems (DSS), where a human makes decisions based on analytics generated by the system, to intelligent decision systems based on analytics performed by Artificial Intelligence (AI) and Machine Learning (ML), and further, to fully autonomous intelligent decision systems, where a machine independently makes decisions based on its AI and ML capabilities. Specifically, we examine the use of decision automation in cybersecurity and infrastructure security and present a methodology for determining which decisions should be automated and at which level of autonomy.},
  keywords={Decision support systems;Privacy;Automation;Conferences;Machine learning;Learning (artificial intelligence);Computer security;Artificial intelligence;decision automation;machine learning},
  doi={10.1109/SPW50608.2020.00048},
  ISSN={},
  month={May},}@INPROCEEDINGS{10698327,
  author={Blancaflor, Eric and Balmocena, Alex Chester},
  booktitle={2024 International Conference on Electrical, Computer and Energy Technologies (ICECET}, 
  title={BuyLow: A Platform Design for Online Goods and Services Trading Platform in the Philippines}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={This study focuses on the business design model covering the creation and developing the BuyLow: Goods and Services Trading Platform in the Philippines. BuyLow aims to provide an alternative e-commerce approach to typical marketplace where customers meet to exchange their goods and services without any currency. In a common scenario the amount goods and services purchased are also paid by manufactured goods or offered services. BuyLow offers exchange of goods and services in a much safer and reliable way. Buylow also offers a platform where people can trade not just physical product but also digital services such as animations, video editing, digital art and other modern digital products that can be stored electronically. BuyLow can be a game changer in the e-commerce industry in the Philippines because it utilizes unused item from each user in exchange for possible useful items from other users. The BuyLow business model is based on subscription and advertisements. For free users they can only post up to 4 items per day and can transact up to 8 trades per day, after the quota (for free users) certain features will be disabled such as ability to post item for trade, accept trade proposal or submit trade proposal to other users, but can also do monitoring of exisiting trades. BuyLow can be a game changer in the e-commerce industry in the Philippines because it utilizes unused item from each user in exchange for possible useful items from other users. With the use of AI and Machine Learning the success of the system can be greatly improve by producing new and accurate data about the system environment.},
  keywords={Industries;Accuracy;Digital art;Games;Machine learning;Electronic commerce;Proposals;Reliability;Monitoring;Business;Online Marketing;Information System;E- barter;digital products},
  doi={10.1109/ICECET61485.2024.10698327},
  ISSN={},
  month={July},}@INPROCEEDINGS{10763899,
  author={Das, Rajdeep and Sharma, Vaishnavi and Kumari, Priyanka and Raj, Abhishek and Das, Sudipta and De, Indrajit},
  booktitle={2024 IEEE International Conference on Communication, Computing and Signal Processing (IICCCS)}, 
  title={Vyoma-ADS : An AI-Driven Tool for Adaptive Monitoring and Anomaly Detection for Space Agriculture}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={As humanity ventures further into space exploration, securing sustainable resources becomes paramount. The success of space farming depends not only on the ability to cultivate crops but also on the effective management of environmental conditions. Introducing Vyoma-ADS: an innovative system that integrates IoT technology with an adaptive Anomaly Detection System, specifically designed for space agriculture. Our study underscores the crucial role of environmental monitoring within space shuttles, where even minor fluctuations can have a significant impact on plant growth and productivity. To tackle these challenges, we designed a compact IoT-enabled system to monitor the moisture content in the soil, the temperature and humidity of the surrounding environment, and the ppm value of unwanted gases in the atmosphere. Through rigorous testing and analysis, we established optimal thresholds conducive to plant growth in the unique conditions of space. Recognizing the inherent variability and occasional anomalies in environmental data, we investigated three distinct machine learning models for anomaly detection: Isolation Forest, One-Class Support Vector Machines (OCSVM) and Density-Based Spatial Clustering of Applications with Noise (DBSCAN). Our findings reveal that DBSCAN outperforms other techniques in identifying anomalies amidst fluctuating data. Additionally, we detail our approach to data visualization using a SaaS cloud platform, which enhances the comprehensibility of the study and facilitates better decision-making processes. This paper demonstrates how the integration of IoT monitoring and advanced anomaly detection methods can significantly improve the reliability and efficiency of space agriculture. By leveraging these technologies, we aim to create a robust system capable of maintaining optimal growing conditions for plants in space.},
  keywords={Temperature sensors;Cloud computing;Atmospheric modeling;Space shuttles;Data visualization;Machine learning;Humidity;Agriculture;Sensors;Anomaly detection;Machine Learning;ADS;Internet of Things;Pattern Recognition},
  doi={10.1109/IICCCS61609.2024.10763899},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10561275,
  author={Li, Jian and Yu, Xinge},
  booktitle={2024 IEEE 3rd International Conference on Micro/Nano Sensors for AI, Healthcare, and Robotics (NSENS)}, 
  title={An active interface pressure adapter for enhancing sensing capacity and stability of wearable electronics}, 
  year={2024},
  volume={},
  number={},
  pages={45-48},
  abstract={Wearable hybrid systems offer the potential for continuous and reliable on-skin monitoring of physiological indicators. These systems typically consist of sensing modules, signal processing modules, and protective encapsulation modules, shielding the systems from external environments to enhance their robustness, which in turn compromises interfacial stability. To address this challenge, we introduce a novel interface pressure adapter designed to significantly improve the compliance and stability between the user’s skin and the wearable system. This adapter incorporates a micro-airbag, a pressure regulation one-way valve, and a micro-pump, working in synergy to provide sufficient pressure support to the sensing module, thereby enhancing the sensing stability and robustness of wearable systems. As proof of concept, a maximum pressure of 16.2 kPa was generated and regulated with 5 pumping phases to provide back pressure to a pressure sensor for pulse wave measurement. The proposed adapter demonstrated remarkable improvements in interfacial stability and measurement robustness against various joint deformation and motion artifacts. This design presents a promising solution for enhancing the performance and interface stability of contact-based sensing systems.},
  keywords={Signal processing;Robot sensing systems;Valves;Robustness;Skin;Regulation;Sensors},
  doi={10.1109/NSENS62142.2024.10561275},
  ISSN={},
  month={March},}@INPROCEEDINGS{9092392,
  author={Haneche, Abderrahim and Lachachi, Yazid and Niar, Smail and Ouarnoughi, Hamza},
  booktitle={2020 28th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)}, 
  title={A GPU enhanced LIDAR Perception System for Autonomous Vehicles}, 
  year={2020},
  volume={},
  number={},
  pages={232-236},
  abstract={Environment vision and understanding is a crucial task in Autonomous Driving (AD) context. This mainly needs image processing approaches such as Convolutional Neural Networks (CNN). Nevertheless, cameras have shown their limits for such a task, especially in dealing with difficult light conditions. LIDAR is a powerful and widely used sensor for AD. Indeed, LIDAR can then cope with the lack of information gathered from cameras. For AD, data processing from the sensors is the key function to obtain a high quality perception. For this, Graphics Processing Unit (GPU) platforms show great performances and outperform other processing platforms such as FPGA and Multi-cores. This work presents a new approach to produce multiple 2D representation from 3D points cloud coming from LIDAR. The 2D representation can therefore be used by any efficient image processing applications. Our approach uses only LIDAR sensor and exploits the high GPU parallelism for its implementation. The resulting 2D representations are then used by CNN for AD applications such as image classification and segmentation. Finally, our contributions have been evaluated using the KITTI road benchmark and showed encouraging results.},
  keywords={Laser radar;Graphics processing units;Two dimensional displays;Three-dimensional displays;Task analysis;Cameras;Rendering (computer graphics);GPU;LIDAR;Autonomous Driving;Artificial Intelligence},
  doi={10.1109/PDP50117.2020.00043},
  ISSN={2377-5750},
  month={March},}@INPROCEEDINGS{9816576,
  author={Chowdhury, Prabudhya Roy and Sakuma, Katsuyuki and Raghavan, Sathya and Bergendahl, Marc and Sikka, Kamal and Kohara, Sayuri and Hisada, Takashi and Mori, Hiroyuki and Taneja, Divya and De Sousa, Isabel},
  booktitle={2022 IEEE 72nd Electronic Components and Technology Conference (ECTC)}, 
  title={Thermo-mechanical Analysis of Thermal Compression Bonding Chip-Join Process}, 
  year={2022},
  volume={},
  number={},
  pages={579-585},
  abstract={Heterogeneous Integration (HI) Packaging has enabled high-bandwidth, low-latency communication between package components such as CPUs, GPUs and memory units for AI workloads. Examples of enabling HI packaging technologies include Si interposers, high-density laminates with thin films, silicon bridges and stacked chips.Thermo-compression bonding (TCB) chip-join process is achieved by the application of pressure and temperature to the mating surfaces which can be silicon-silicon or silicon-laminate substrate. A silicon-silicon structure has minimal thermal expansion mismatch while there is a large thermal expansion mismatch between silicon and laminate substrate. Also, the pressure and temperature capabilities of the TCB tools can be vastly different for different manufacturers. As a result, it is important to understand the effects of varying the TCB process parameters on the package components and interconnect quality.In this study, transient finite element models were created for the TCB process for a silicon-bridge structure with direct bonded heterogenous integration (DBHi) packaging. The package model consists of processor and bridge chips, interconnects between the chips, and underfill around the interconnects. In addition, our models also included TCB tool components in direct contact with the package, such as the ceramic heater, nozzle and heated pedestal. Appropriate thermal and mechanical boundary conditions were applied to the models based on the assembly process parameters for the different packages. The quality of the bonds was assessed from the resultant temperature profiles, stress distribution and warpage in the package components and interconnects. The effect of process parameters including bond temperature, bond force and temperature ramping rate was also studied to provide guidance toward improving the TCB bond profiles. Our work presents the first extensive modeling effort for thermal compression bonding of HI package structures to complement our prior thermo-mechanical analysis of the isolated package systems.},
  keywords={Bridges;Heating systems;Temperature distribution;Thermal expansion;Thermomechanical processes;Force;Packaging;Direct Bonded Heterogeneous Integration;Si bridge;DBHi;Thermal compression bonding;Thermo-mechanical modeling},
  doi={10.1109/ECTC51906.2022.00097},
  ISSN={2377-5726},
  month={May},}@ARTICLE{8767960,
  author={Zhang, Ling and Chen, Guo and Ai, Xing and He, Xiao-Shan and He, Zhi-Bing and Tang, Yong-Jian},
  journal={IEEE Transactions on Plasma Science}, 
  title={Effects of Gas Pressure on the Plasma Properties in a Radial Small Size Inductively Coupled Plasma Reactor}, 
  year={2019},
  volume={47},
  number={8},
  pages={4026-4031},
  abstract={To improve the technology of glow discharge polymer (GDP) thin film deposition and provide theoretical support for the deposition mechanism of GDP thin films, the plasma properties in a special cylinder (radial small size) inductively coupled plasma (ICP) reactor were investigated. High electron energy was observed in this reactor in contrast to radial large reactors. The electron energy relaxation length λε in the elastic range was calculated to determine the electron kinetics transition pressure. The electron energy probability functions were found to change from a bi-Maxwellian distribution to a Maxwellian distribution and then into a Druyvesteyn-like distribution. Furthermore, we observed the association ions Ar+(+Ar) generated by ion-molecule association reactions. In addition, the profiles of plasma parameters, such as the plasma potential Vp, the electron density ne, and the effective electron temperature Teff, were experimentally investigated with increasing gas pressure.},
  keywords={Plasmas;Inductors;Kinetic theory;Heating systems;Discharges (electric);Radio frequency;Economic indicators;Gas pressure;heating mode transition;inductively coupled plasma (ICP);plasma properties},
  doi={10.1109/TPS.2019.2925568},
  ISSN={1939-9375},
  month={Aug},}@INPROCEEDINGS{10674478,
  author={Islam, Junaidul and Furqon, Elvin Nur and Farady, Isack and Rani Alex, John Sahaya and Kuo, Chia-Chen and Lin, Chih-Yang},
  booktitle={2024 International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)}, 
  title={Integrating Stable diffusion for Improved Alzheimer's Disease Classification: Insights from MRI Images and YOLOv8}, 
  year={2024},
  volume={},
  number={},
  pages={303-304},
  abstract={Alzheimer's disease (AD) is the most common form of dementia, characterized by progressive neurodegeneration. Structural changes in the brain associated with AD can be visualized using magnetic resonance imaging (MRI). However, acquiring a sufficient number of MRI images from patients with Alzheimer's disease is challenging. In this study, we employ stable diffusion to simulate MRI scans of patients with AD. We train the stable diffusion model from scratch to generate new MRI images as part of data augmentation alongside the original images. The model is capable of producing synthetic MRI images of AD. We employ t-distributed Stochastic Neighbor Embedding (t-SNE) visualization to evaluate the quality and diversity of the generated images by comparing their distribution to that of actual MRI scans. Subsequently, a modified YOLOv8-cls model is retrained using the generated images for AD classification. Our findings demonstrate that this approach effectively produces realistic synthetic biological images suitable for training deep learning models with 10% improvement in accuracy.},
  keywords={Deep learning;Training;Solid modeling;Accuracy;Three-dimensional displays;Generative AI;Magnetic resonance imaging;Generative model;stable diffusion;Alzheimer’s disease;MRI image;deep learning},
  doi={10.1109/ICCE-Taiwan62264.2024.10674478},
  ISSN={2575-8284},
  month={July},}@INPROCEEDINGS{10888660,
  author={Trioux, Anthony and Gao, Yusong and Song, Jiarun and Wu, Wenjie and Ma, Faming and Yang, Fuzheng},
  booktitle={ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Subjective Fidelity Assessment of Audio- and Video-Driven Talking Head Generation Methods}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Audio- and Video-Driven Talking Head Generation methods have attracted considerable research interest due to recent advances in Artificial Intelligence Generated Content (AIGC) technologies. In such approaches, a single image is artificially animated by leveraging audio and/or motion features extracted from video sources. Despite notable progress, current performance assessments rely primarily on traditional objective metrics, often neglecting subjective evaluation aspects. To address this issue, we propose in this paper a subjective fidelity assessment of recent Audio- and/or Video-Driven Talking Head Generation methods. This study aims to assess how accurately and convincingly the generated video reproduces the visual and behavioral characteristics of a real human face, as well as how closely the video aligns with expected natural human expressions, movements, and/or audio synchronization. In order to provide a detailed assessment of the fidelity in the context of talking heads, our study focuses on six key criteria: Overall Fidelity, Gaze Fidelity, Audio-Video Sync Fidelity, Head Pose Fidelity, Expression Fidelity, and Overall Visual Quality. Experiments results reveal a nuanced picture of the fidelity in this context, where the performance varies significantly depending on the video content itself as well as how the animation is generated, highlighting the needs for further research. This research represents an initial step towards the evaluation of Audio- and Video-Driven generative image animation methods for Talking heads while offering insights for improving the accuracy and realism of those techniques. The dataset and corresponding results are available at https://github.com/a-trioux/Subjective-Fidelity-Assessment-Talking-Head.},
  keywords={Measurement;Visualization;Head;Signal processing;Feature extraction;Animation;Behavioral sciences;Synchronization;Speech processing;Faces;Audio- and Video-Driven Talking Head Generation;Multimodal Fidelity Assessment;AIGC;Generative models;Video conferencing},
  doi={10.1109/ICASSP49660.2025.10888660},
  ISSN={2379-190X},
  month={April},}@ARTICLE{10945331,
  author={Bezziane, Mohamed Ben and Abbas, Messaoud and Alfraihi, Hessa and Brik, Bouziane and Khaldi, Yacine and Aiadi, Oussama and Siham, Hasan},
  journal={IEEE Access}, 
  title={Deep Learning Empowered Service Selection in FANETs: A 5G-Enabled UAV-Cloud Architecture}, 
  year={2025},
  volume={13},
  number={},
  pages={63877-63894},
  abstract={Flying Ad-Hoc Networks (FANETs), commonly referred to as drones or Unmanned Aerial Vehicles (UAVs), are increasingly transforming various industries by supporting numerous applications. In this context, a UAV can function as a cloud service provider, offering services such as memory, computing, and network connectivity to consumer UAVs. This work introduces an innovative 5G-enabled UAV-Cloud Architecture empowered by deep learning for service selection. The architecture integrates two main modules: (i) A game theory-based module, which establishes an interactive game between provider and consumer UAVs, generating datasets to guide consumer UAVs in selecting suitable providers; and (ii) A deep learning-based module, leveraging the generated datasets to predict optimal service providers. Numerical simulations validate the proposed architecture, highlighting its superior prediction accuracy, reduced time and message complexity, and minimized service discovery and consumption delays compared to existing methods. These advancements are attributed to the synergy of game theory, deep learning, and 5G communication capabilities.},
  keywords={Drones;Autonomous aerial vehicles;Computer architecture;Cloud computing;Game theory;Games;Deep learning;5G mobile communication;Ad hoc networks;Quality of service;UAV-cloud architecture;service selection;deep learning;game theory and 5G},
  doi={10.1109/ACCESS.2025.3555871},
  ISSN={2169-3536},
  month={},}@ARTICLE{8472113,
  author={Liang, Le and Ye, Hao and Li, Geoffrey Ye},
  journal={IEEE Internet of Things Journal}, 
  title={Toward Intelligent Vehicular Networks: A Machine Learning Framework}, 
  year={2019},
  volume={6},
  number={1},
  pages={124-135},
  abstract={As wireless networks evolve toward high mobility and providing better support for connected vehicles, a number of new challenges arise due to the resulting high dynamics in vehicular environments and thus motive rethinking of traditional wireless design methodologies. Future intelligent vehicles, which are at the heart of high mobility networks, are increasingly equipped with multiple advanced onboard sensors and keep generating large volumes of data. Machine learning, as an effective approach to artificial intelligence, can provide a rich set of tools to exploit such data for the benefit of the networks. In this paper, we first identify the distinctive characteristics of high mobility vehicular networks and motivate the use of machine learning to address the resulting challenges. After a brief introduction of the major concepts of machine learning, we discuss its applications to learn the dynamics of vehicular networks and make informed decisions to optimize network performance. In particular, we discuss in greater detail the application of reinforcement learning in managing network resources as an alternative to the prevalent optimization approach. Finally, some open issues worth further investigation are highlighted.},
  keywords={Machine learning;Vehicle dynamics;Quality of service;Resource management;Tools;Wireless networks;High mobility;Internet of Intelligent Vehicles;machine learning;vehicular networks},
  doi={10.1109/JIOT.2018.2872122},
  ISSN={2327-4662},
  month={Feb},}@INPROCEEDINGS{10723576,
  author={Kaveh, Amin and Rohner, Christian and Johnsson, Andreas},
  booktitle={2024 IEEE 21st International Conference on Mobile Ad-Hoc and Smart Systems (MASS)}, 
  title={Impact of Attack Variations and Topology on IoT Intrusion Detection Model Generalizability}, 
  year={2024},
  volume={},
  number={},
  pages={364-370},
  abstract={Intrusion Detection Systems (IDS) play a critical role in safeguarding loT networks, especially in sectors like healthcare, manufacturing, and smart cities where safety is paramount. Machine learning (ML) holds significant promise for training IDS models, leveraging data from past attacks. However, the effectiveness of these models are dependent on the quality and diversity of training data, which is often limited from the perspective of a single network operator. This paper delves into the challenges of ML-based IDS model generalization across loT network scenarios with expected distributional shifts in the data. We examine variations in known attack patterns and changes in loT network configurations, quantifying their impact on model generalizability. These shifts originates from when multiple network operators seek to share knowledge to enhance their respective IDS capabilities, when a new attack variation is launched, or when an operator reconfigure its network. We explore two approaches to address these chal-lenges: namely data sharing and horizontal federated learning for privacy preservation. While data sharing proves effective across scenarios, it relies on mutual trust among network operators. In contrast, federated learning preserves privacy but is less effective, especially when the network topology is the primary driver of distributional shifts in the train and test data. To facilitate our study, we implemented Blackhole attack variation strategies within the Cooja network simulator. Our objective was to generate a large dataset enabling comprehensive analysis of attack variations across diverse set of network configurations to study the impact on ML-based IDS for loT networks.},
  keywords={Training;Data privacy;Network topology;Federated learning;Smart cities;Intrusion detection;Training data;Smart systems;Data models;Topology;Internet of Things;Blackhole Attacks;Intrusion Detection Systems;Machine Learning;Federated Learning},
  doi={10.1109/MASS62177.2024.00055},
  ISSN={2155-6814},
  month={Sep.},}@ARTICLE{9869784,
  author={Shojae Chaeikar, Saman and Jolfaei, Alireza and Mohammad, Nazeeruddin},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={AI-Enabled Cryptographic Key Management Model for Secure Communications in the Internet of Vehicles}, 
  year={2023},
  volume={24},
  number={4},
  pages={4589-4598},
  abstract={Recent advancements in the Internet of Vehicles (IoV) technology have pathed the way for the use of various smart services for the management of urban traffic, including authentication and key management. Key management protocols are an important means of addressing security and privacy concerns. However, they can be resource-intensive in terms of network traffic and workload management, particularly at times of traffic congestion, which in turn can increase the ECU and RSU processing load and adversely impact network communications. This paper introduces a more efficient key management method, named AI-enabled and Layered Key Management (ALKM), which uses an Artificial Intelligence (AI) approach and a layered workflow to reduce network traffic and workload. Specifically, the ALKM distributes dynamic synchronous time-dependent keys among Road Side Units (RSUs) rather than static cryptographic keys. It provides three layers of secure communications: public, tunnel, and hierarchy. The public layer creates a flat secure layer between the Traffic Management Center (TMC) and all RSUs. Using AI-enabled features, the tunnel layer predicts short-term and long-term congestion areas by analysis of the acquired trajectory data, and then establishes a secure communication channel between the selected RSUs and the TMC. In the hierarchy layer, in multiple tiers, the TMC and higher-level RSUs assist lower RSUs in message decryption (but not vice-versa). Our extensive analysis shows that the ALKM generates overall between 48% and 99% less network traffic per generated key than the Master Key method, depending on the operational lifetime of the keys used.},
  keywords={Security;Trajectory;Authentication;Encryption;Road side unit;Internet of Vehicles;Automobiles;Cybersecurity;key management;Internet of Vehicles;road side unit;traffic management center;trajectory data analysis},
  doi={10.1109/TITS.2022.3200250},
  ISSN={1558-0016},
  month={April},}@INPROCEEDINGS{10714771,
  author={Jadhav, Dhiraj and Agrawal, Sakshee and Jagdale, Sakshi and Salunkhe, Pranav and Salunkhe, Raj},
  booktitle={2024 8th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)}, 
  title={AI-Driven Text-to-Multimedia Content Generation: Enhancing Modern Content Creation}, 
  year={2024},
  volume={},
  number={},
  pages={1610-1615},
  abstract={This research investigates the use of artificial intelligence for the creation of multimedia content from textual sources, such as press articles, stories, and scripts. The system utilizes Natural Language Processing and image generation techniques to create relevant visuals and voiceovers, enhancing content creation efficiency without sacrificing quality. The system processes various input formats, using Optical Character Recognition (OCR) for non-text formats and advanced large language models (LLMs) like LLaMA2 and Gemini for text summarization. It automatically extracts keywords and uses them to generate synchronized visual and audio content, utilizing tools like Google Text-to-Speech (GTTS) and AI-based image generation models such as DALL-E and Stable Diffusion. The multimedia content is assembled and rendered using MoviePy, ensuring seamless integration of text, audio, visuals, and subtitles. Applications in journalism, marketing, and education are examined to showcase how this technology improves accessibility and audience engagement. Overall, the study demonstrates the potential of AI in automating media content creation, streamlining the process while maintaining narrative coherence and enhancing the user experience.},
  keywords={Visualization;Presses;Accuracy;Image synthesis;Optical character recognition;Media;Streaming media;Journalism;Real-time systems;Synchronization;Media Content Creation;Audience Engagement;Text-to-Video Generation;Natural Language Processing (NLP);Computer Vision;Optical Character Recognition (OCR);Large Language Models (LLMs)},
  doi={10.1109/I-SMAC61858.2024.10714771},
  ISSN={2768-0673},
  month={Oct},}@INPROCEEDINGS{10393371,
  author={Xing, Yuan},
  booktitle={2023 IEEE 3rd International Conference on Data Science and Computer Application (ICDSCA)}, 
  title={Research on Intelligent System of 3D Software Cultural and Creative Product Design Based on Computer Artificial Intelligence Technology}, 
  year={2023},
  volume={},
  number={},
  pages={1151-1155},
  abstract={This paper proposes a development scheme of 3D animation visual mobile application based on virtual reality technology. Use camera equipment to collect image data. The image-real technology is used to process the images and other contents that need to be made. The processed data is established, combined with sound effects and other technologies, and input into the Unity3D engine to generate a 3D virtual picture at last. Establish a database of cultural and creative pictures. Quantitative description of the artistic style of cultural and creative products is carried out, and artificial features such as composition and splicing edge features are extracted. The advanced semantic properties are studied by simulated annealing method. Research the stitching method of artistic cultural and creative commodity pictures. Simulation experiments show that the proposed method can effectively complete the intelligent design of three-dimensional creative products.},
  keywords={Visualization;Three-dimensional displays;Simulated annealing;Virtual reality;Feature extraction;Cultural differences;Data mining;Image generation;cultural and creative products;3D design;virtual reality technology;simulated annealing algorithm},
  doi={10.1109/ICDSCA59871.2023.10393371},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10887687,
  author={Li, Boyuan and Wang, Xihua and Song, Ruihua and Huang, Wenbing},
  booktitle={ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Two-in-One: Unified Multi-Person Interactive Motion Generation by Latent Diffusion Transformer}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Multi-person interactive motion generation, a critical yet under-explored domain in computer character animation, poses significant challenges such as intricate modeling of inter-human interactions beyond individual motions and generating two motions with huge differences from one text condition. Current research often employs separate module branches for individual motions, leading to a loss of interaction information and increased computational demands. To address these challenges, we propose a novel, unified approach that models multi-person motions and their interactions within a single latent space. Our approach streamlines the process by treating interactive motions as an integrated data point, utilizing a Variational AutoEncoder (VAE) for compression into a unified latent space, and performing a diffusion process within this space, guided by the natural language conditions. Experimental results demonstrate our method’s superiority over existing approaches in generation quality, performing text condition in particular when motions have significant asymmetry, and accelerating the generation efficiency while preserving high quality.},
  keywords={Computational modeling;Autoencoders;Natural languages;Diffusion processes;Signal processing;Transformers;Diffusion models;Animation;Acoustics;Speech processing;Latent Diffusion Model;Multi-modalities;Hu-man interaction generation},
  doi={10.1109/ICASSP49660.2025.10887687},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10630767,
  author={Phummapooti, Ratchatida and Thanyaphongphat, Jirapipat and Panjaburee, Patcharin},
  booktitle={2024 5th Technology Innovation Management and Engineering Science International Conference (TIMES-iCON)}, 
  title={AI-Driven Recommendations for Digital Majors Based on Learning Styles}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={In response to industry demands, universities offer diverse digital technology majors, some focusing purely on technology while others integrate with domains like arts, business, or science. These interdisciplinary programs aim to equip students with the skills necessary to thrive in a digitally driven world. However, matching students with the right major can be challenging, especially given the diverse learning preferences among students. The VARK framework categorizes students into visual, aural, reading/writing, and kinesthetic learners, while the Kolb model classifies learners based on their experiences as diverger, assimilators, converger, or accommodators. These frameworks offer insights into how students prefer to acquire and retain information. The resulting predictive models will recommend the most suitable majors for students based on their learning styles. These recommendations will consider the compatibility between students' preferred learning methods with digital major. Moreover, leveraging machine learning models can predict students' suitability for digital majors based on learning styles. Analyzing student profiles and outcomes facilitates informed decisions, benefiting both students and universities in shaping educational pathways and career trajectories. Through the application of Support Vector Classifier, this study seeks to provide personalized academic guidance to students, helping them make informed decisions about their educational pathways. By considering students' individual learning styles, universities can enhance the effectiveness of their digital maj or offerings, ultimately addressing the shortage of skilled digital professionals in the workforce. The study findings showcase the proposed approach as highly promising, achieving outstanding classification accuracy. Specifically, the Area Under the Curve (AUC) accuracy values are notable: Animation and Visual Effects with an AUC of 0.89, Business Computer with an AUC of 0.97, and Computer Engineering with an AUC of 0.96.},
  keywords={Support vector machines;Accuracy;Statistical analysis;Sociology;Focusing;Predictive models;Visual effects;digital major;learning style;machine learning model},
  doi={10.1109/TIMES-iCON61890.2024.10630767},
  ISSN={},
  month={June},}@INPROCEEDINGS{10722064,
  author={Ramagundam, Shashishekhar and Karne, Niharika},
  booktitle={2024 5th International Conference on Smart Electronics and Communication (ICOSEC)}, 
  title={Enhancing Viewer Engagement: Exploring Generative Long Short Term Memory in Dynamic Ad Customization for Streaming Media}, 
  year={2024},
  volume={},
  number={},
  pages={251-259},
  abstract={In Ad customization, several approaches are employed to improve the relevance level of the message for monitoring the user performance according to the prior search history (e.g., retargeting) or by gathering particular information related to the customer (e.g., micro-targeting). Occasionally, the consumers view the ad developed by the other user, in this situation, the ads are viewed as something that can be related by the user. The quantity of information that is gathered openly or secretly can affect the user choice for the content displayed. However, the users remain as recipients of a strategy, that decides the best ad content. Therefore, an exploring Generative Artificial Intelligence-based Dynamic Ad Customization model for enhancing viewer engagement from streaming media is developed. In this paper, the Generative Long Short-Term Memory (GenLSTM) model is developed for examining customized advertising then includes the customization of specific message elements by individuals, variation in the level of ad customization, and in some cases perceived collaboration with others. Here, the customization classification is defined based on the modification in the marketing mix element that is created by the customer proactively as opposed to the company. In regards to customized advertising, “ad is something that happens with the viewer”. The customization increases based on the sense of task engagement, strength of the perceived customer relationship with the company, attitude toward the ad, and perceptions of company trust/integrity. Finally, the experimental analysis is conducted based on the relationship with the company, company care, company trust, and task engagement to attain the overall impact on ad customization.},
  keywords={Supply chains;Collaboration;Companies;Streaming media;Data models;History;Advertising;Long short term memory;Monitoring;Consumer electronics;Ad Customization;User Behavior;Viewer Engagement;Generative Long Short Term Memory},
  doi={10.1109/ICOSEC61587.2024.10722064},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10656828,
  author={Zhao, Yian and Li, Kehan and Cheng, Zesen and Qiao, Pengchong and Zheng, Xiawu and Ji, Rongrong and Liu, Chang and Yuan, Li and Chen, Jie},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={GraCo: Granularity-Controllable Interactive Segmentation}, 
  year={2024},
  volume={},
  number={},
  pages={3501-3510},
  abstract={Interactive Segmentation (IS) segments specific objects or parts in the image according to user input. Current IS pipelines fall into two categories: single-granularity out-put and multi-granularity output. The latter aims to allevi-ate the spatial ambiguity present in the former. However, the multi-granularity output pipeline suffers from limited interaction flexibility and produces redundant results. In this work, we introduce Granularity-Controllable Interactive Segmentation (GraCo), a novel approach that allows precise control of prediction granularity by introducing ad-ditional parameters to input. This enhances the customization of the interactive system and eliminates redundancy while resolving ambiguity. Nevertheless, the exorbitant cost of annotating multi-granularity masks and the lack of avail-able datasets with granularity annotations make it difficult for models to acquire the necessary guidance to control out-put granularity. To address this problem, we design an any-granularity mask generator that exploits the semantic property of the pre-trained IS model to automatically gen-erate abundant mask-granularity pairs without requiring additional manual annotation. Based on these pairs, we propose a granularity-controllable learning strategy that efficiently imparts the granularity controllability to the IS model. Extensive experiments on intricate scenarios at ob-ject and part levels demonstrate that our GraCo has signifi-cant advantages over previous methods. This highlights the potential of GraCo to be a flexible annotation tool, capable of adapting to diverse segmentation scenarios. The project page: https://zhao-yian.github.io/GraCo.},
  keywords={Image segmentation;Adaptation models;Annotations;Interactive systems;Pipelines;Semantics;Redundancy;Interactive Segmentation;Granularity-Controllable},
  doi={10.1109/CVPR52733.2024.00336},
  ISSN={2575-7075},
  month={June},}@ARTICLE{8884164,
  author={Zhang, Jun and Letaief, Khaled B.},
  journal={Proceedings of the IEEE}, 
  title={Mobile Edge Intelligence and Computing for the Internet of Vehicles}, 
  year={2020},
  volume={108},
  number={2},
  pages={246-261},
  abstract={The Internet of Vehicles (IoV) is an emerging paradigm that is driven by recent advancements in vehicular communications and networking. Meanwhile, the capability and intelligence of vehicles are being rapidly enhanced, and this will have the potential of supporting a plethora of new exciting applications that will integrate fully autonomous vehicles, the Internet of Things (IoT), and the environment. These trends will bring about an era of intelligent IoV, which will heavily depend on communications, computing, and data analytics technologies. To store and process the massive amount of data generated by intelligent IoV, onboard processing and cloud computing will not be sufficient due to resource/power constraints and communication overhead/latency, respectively. By deploying storage and computing resources at the wireless network edge, e.g., radio access points, the edge information system (EIS), including edge caching, edge computing, and edge AI, will play a key role in the future intelligent IoV. EIS will provide not only low-latency content delivery and computation services but also localized data acquisition, aggregation, and processing. This article surveys the latest development in EIS for intelligent IoV. Key design issues, methodologies, and hardware platforms are introduced. In particular, typical use cases for intelligent vehicles are illustrated, including edge-assisted perception, mapping, and localization. In addition, various open-research problems are identified.},
  keywords={Cloud computing;Intelligent vehicles;Sensors;Task analysis;Artificial intelligence;Wireless communication;Vehicle-to-everything;Internet of Things;Vehicular ad hoc networks;Autonomous driving;edge AI;Internet of Vehicles (IoV);mobile edge computing (MEC);vehicular communications;wireless caching},
  doi={10.1109/JPROC.2019.2947490},
  ISSN={1558-2256},
  month={Feb},}@INPROCEEDINGS{9972662,
  author={Kumar, V Sunil and Pareek, Piyush Kumar and Costa de Albuquerque, Victor Hugo and Khanna, Ashish and Gupta, Deepak and S, Deepak Renukadevi},
  booktitle={2022 IEEE 2nd Mysore Sub Section International Conference (MysuruCon)}, 
  title={Multimodal Sentiment Analysis using Speech Signals with Machine Learning Techniques}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={The study of people's perspectives, evaluations, attitudes, and feelings in regard to objects and the qualities of those entities is referred to as sentiment analysis. This research is conducted via the use of computers. One of the most basic jobs in sentiment analysis is to identify the sentiment polarity of the documents, words, or attributes that are being studied. This may be done in a number of ways. The affective states of individuals are evaluated and taken into account in order to establish the perspective that is conveyed. Users will typically express their opinions on a product or service in the form of a blog post, a shopping site, or a review site the vast majority of the time. These sorts of opinion-related objects are overwhelming and are developing at a quick rate, which makes it a difficult process for the manufacturer to categorize them. typing in all of this information manually. People are also looking forward to hearing people's perspectives on the new linear entities that have been discovered on the level of aspects. As a consequence of this, it is of the utmost importance to develop an automated sentiment analyzer that is able to detect the sentiment polarity of the documents or aspects at both the bipolarity level and the multipolarity level automatically. As a result of the rise of social networking sites, individuals now have the capacity to freely express their thoughts and opinions via social media. This not only provided as a rich source of feedback and analysis of emotions, but it was also a driving force for the creation of automated emotional analysis. Because of this, the supervised classification method has been shown to be successful; hence, it is used widely in a variety of multi sentiment analysis applications as a consequence of this. A hybrid deep learning network, namely a three-dimensional CNN-BLSTM, has been created in order to analyze the sensations that are elicited by opinion videos. This evaluation will take place. YouTube and the Multimodal Opinion Utterances Dataset (MOUD) are the two key datasets that are used the most when it comes to gathering the temporal and geographical information that is contained within video frames. Both of these datasets are available online. In order to identify the candidate's face inside the frames, the Viola-Jones Algorithm is implemented. This algorithm is comprised of four essential steps, such as Haar feature selection, integral image conversion, cascade, and Adaboost training classifiers. The Viola-Jones Algorithm is used in order to accomplish this task. The recommended technique shows greater performance when compared to the standard methods to sentiment analysis on two separate datasets. The last stage of the research study entails doing an analysis of multimodal attitudes. This is necessary since the range of modalities and forms of social data is continually growing. The primary objective of this study is to design an efficient strategy for choosing characteristics in order to improve the overall performance of MSA, which serves as the motivation for the research. This will make it possible to pick the right characteristics, which will eventually result in greater performance. In order to get the values of features from the input data, the dataset from YouTube is used as the input, and hybrid feature extraction algorithms are utilized in order to do this. The Relief feature selection method is put to use in order to choose the most useful characteristics, and after that, the random forest classifier is given access to those characteristics together with the values that are the most useful for them.},
  keywords={Training;Sentiment analysis;Video on demand;Social networking (online);Terminology;Blogs;Feature extraction;Viola Jones;Convolutional Neural Network;Sentiment Analysis;Hybrid Feature Extraction},
  doi={10.1109/MysuruCon55714.2022.9972662},
  ISSN={},
  month={Oct},}@ARTICLE{10413505,
  author={Yu, Zhaofei and Bu, Tong and Zhang, Yijun and Jia, Shanshan and Huang, Tiejun and Liu, Jian K.},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Robust Decoding of Rich Dynamical Visual Scenes With Retinal Spikes}, 
  year={2025},
  volume={36},
  number={2},
  pages={3396-3409},
  abstract={Sensory information transmitted to the brain activates neurons to create a series of coping behaviors. Understanding the mechanisms of neural computation and reverse engineering the brain to build intelligent machines requires establishing a robust relationship between stimuli and neural responses. Neural decoding aims to reconstruct the original stimuli that trigger neural responses. With the recent upsurge of artificial intelligence, neural decoding provides an insightful perspective for designing novel algorithms of brain–machine interface. For humans, vision is the dominant contributor to the interaction between the external environment and the brain. In this study, utilizing the retinal neural spike data collected over multi trials with visual stimuli of two movies with different levels of scene complexity, we used a neural network decoder to quantify the decoded visual stimuli with six different metrics for image quality assessment establishing comprehensive inspection of decoding. With the detailed and systematical study of the effect and single and multiple trials of data, different noise in spikes, and blurred images, our results provide an in-depth investigation of decoding dynamical visual scenes using retinal spikes. These results provide insights into the neural coding of visual scenes and services as a guideline for designing next-generation decoding algorithms of neuroprosthesis and other devices of brain–machine interface.},
  keywords={Decoding;Visualization;Retina;Image reconstruction;Neurons;Convolution;Training;Deep learning;image reconstruction;neural decoding;neural spikes;video;visual scenes},
  doi={10.1109/TNNLS.2024.3351120},
  ISSN={2162-2388},
  month={Feb},}@INPROCEEDINGS{9823902,
  author={Nirmal, Ekanath Sitaram and Narang, Pravin and Rajeswari, T.S. and Usman, Mohammed and Subha, B. and Jadhav, Vandana Sandeep},
  booktitle={2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)}, 
  title={The Application of Effective Machine Learning Tools in Digital Marketing for Enhancing Brand Presence and Image Among the Fast-Moving Consumer Goods in the Developing Countries}, 
  year={2022},
  volume={},
  number={},
  pages={2307-2311},
  abstract={By using updated information techniques to digital marketing activities, increasing technological progress generates competitive edge over its rivals. By extracting insights from massive amounts of data obtained, machine learning (ML) can forecast future trends and assist decision-making. This feature has a significant impact on and accelerates an organisational future decision-making process. Machine learning regarding digital clients helps understand him properly by categorising the vast cache of data generated every day into many sectors and analysing it to develop patterns. There is no apparent boundary between digital and non-digital technology. Digital technology has an impact on human behaviour. Image processing and machine learning are widely employed to change a company's operations. Both input and output have an impact on the machine's learning abilities, which are constantly improving. The research given here relies on three diverse groups' decision and implementation of machine learning-driven analytical tools: “marketing agencies”, “media businesses”, and “advertisers”. This research paper has considered method such as secondary qualitative method to collect both statistical and evidence-based information.},
  keywords={Systematics;Protocols;Decision making;Machine learning;Companies;Big Data;Media;Digital technology Machine learning (ML);Decision making;Advertisement;Artificial Intelligence;competitive;advantage},
  doi={10.1109/ICACITE53722.2022.9823902},
  ISSN={},
  month={April},}@INPROCEEDINGS{10176217,
  author={Manek, Asha S and Shenoy, P Deepa},
  booktitle={2022 IEEE International Conference for Women in Innovation, Technology & Entrepreneurship (ICWITE)}, 
  title={Mining the Web Data: Intelligent Information Retrieval System for Filtering Spam and Sentiment Analysis}, 
  year={2022},
  volume={},
  number={},
  pages={1-10},
  abstract={World Wide Web has great impact on all aspects of our day-to-day activities and people retrieve all kinds of information available on the Web. Numerous informal written documents, such as tweets, SMS messages, emails, and reviews, can be found on online shopping platforms. This extremely large collection of text documents has made the text analysis now popular field of study and encompasses a wide range of activities, such as sentiment analysis, opinion mining, spam identification, and harmful link detection. This work identifies three challenging research problems: spam filtering, malicious web page detection and opinion mining services. Three models RePID-OK, ReP-ETD and RePC-SSMSM are proposed to filter spam. Analysis of opinion is important and crucial for both individuals and companies by eliminating malicious reviews for decision making. To handle and help the Web users and consumers in making decisions, two models SentReP and W-LRSVM are proposed for classification of sentiments for medical related drugs and movie review data set. The model DeMalFier and BLRFier are designed for detecting spam from websites to test whether a website is malicious or legitimate.},
  keywords={Drugs;Sentiment analysis;Text analysis;Web pages;Entrepreneurship;Motion pictures;Information filters;classification;filter;malicious;sentiment analysis;spam detection},
  doi={10.1109/ICWITE57052.2022.10176217},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10760618,
  author={Kshirsagar, Rohit M. and Kherde, Sanjay M. and Dharmadhikari, Sagar R},
  booktitle={2024 2nd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)}, 
  title={Investigation of 3D Printed Lithophane Quality Improvement on A FDM Printer}, 
  year={2024},
  volume={},
  number={},
  pages={1488-1494},
  abstract={In recent years, the process of creating lithophanes has involved the use of 3D printing technologies to produce transparent plates of varying thicknesses and when backlit, these lithophanes create images. The process includes building a 3D model of the lithophane with open-source 3D modeling software. Then, the print parameters are chosen, and white PLA filament is used for printing. This research investigates the optimization of 3D-printed lithophanes on Fused Deposition Modeling (FDM). Using Taguchi’s DOE approach, five key parameters such as brightness, contrast, maximum thickness, minimum thickness, and vector per pixel were chosen, and each was investigated at three distinct levels. A total of 27 samples were systematically designed and printed to examine the impact of these parameters on the quality of 3D-printed lithophanes. The Python script was used to analyze the experimental data and find the best set of parameters to enhance the lithophanes’ visual appeal. The results of this investigation provide significant contributions to the field of 3D printing, particularly with regard to the production of visually engaging and captivating lithophane prints through the use of FDM technology.},
  keywords={Visualization;Solid modeling;Three-dimensional displays;Brightness;Three-dimensional printing;Vectors;Structural engineering;Frequency division multiplexing;Printers;Testing;Lithophane;Fused Deposition Modeling Technology;Python},
  doi={10.1109/ICSSAS64001.2024.10760618},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9282713,
  author={Zhang, Xudong and Cai, Yan and Yang, Zijiang},
  booktitle={2020 IEEE 20th International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
  title={A Study on Testing Autonomous Driving Systems}, 
  year={2020},
  volume={},
  number={},
  pages={241-244},
  abstract={In recent years, with the rapid development of artificial intelligence and other related technologies, the traditional automotive industry has begun to integrate information technology in an all-round way. Due to the contributions of computer vision, deep learning, and sensitive sensors, autonomous driving systems (ADS) has now achieved great progress. But as we all know, the primary requirement for autonomous driving is absolute safety. However, technology innovation has brought great challenges to the testing of ADS, and due to the high cost of field testing, industrial companies rarely open relevant test data for research. This paper aims to study existing testing methods for ADS. Our study shows that there are few published works focusing on testing aspects of ADS. However, there is an obvious trend on the record of published works on testing ADS. Also, we can find that most reviewed works focus on setting up virtual test environment including generating, synthesizing, or reconstructing test input data. They either treat ADS as a whole to conduct (sub) system level testing or limit ADS into certain scenarios. From this, we believe that testing of ADS has just begun to attract researchers' interest; great effort should be paid before ADS becomes maturer.},
  keywords={Technological innovation;Software quality;Tools;Software reliability;Safety;Autonomous vehicles;Testing;Automated Driving Systems;Testing},
  doi={10.1109/QRS-C51114.2020.00048},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10396884,
  author={Sujith Kumar, S and Santhosh, B and Guruakash, Sm and Pravin Savaridass, M},
  booktitle={2023 Third International Conference on Smart Technologies, Communication and Robotics (STCR)}, 
  title={AI Based Tamil Palm Leaf Character Recognition}, 
  year={2023},
  volume={1},
  number={},
  pages={1-7},
  abstract={An extensive study aimed at analyzing characters from old Tamil manuscripts written on palm leaves corresponds to Tamil Palm Leaf Character Recognition. These culturally and historically valuable manuscripts require extensive processing in order to effectively extract characters because they are frequently affected by chronological and degradation. The process begins with the capturing of a clear photograph of the manuscript, which is then preprocessed to remove imperfections like stains and smudges. Before preprocessing, the digitalized manuscripts are cropped vertically into three parts to improve the preprocessing quality. To remove color differences and compress the data, the image is converted to grayscale. subsequently noise is removed using specialized filtering algorithms. The image is refined through further morphological processes, which fill in small gaps and blurs character borders. Binarization makes it easier to distinguish the characters from the surroundings. Line segmentation is applied to the previously divided image to separate out individual lines of text for improved character recognition. Finally, the preprocessed scripts are trained by using CNN model. In the proposed model the Tamil characters are effectively recognized with an accuracy of 91%, and resulting in the digital preservation and comprehension of these precious palm leaf written works.},
  keywords={Image segmentation;Image coding;Image color analysis;Writing;Gray-scale;Filtering algorithms;Character recognition;Character recognition;Tamil palm leaf manuscripts;Deep learning CNN;Noise cancellation;Segmentation},
  doi={10.1109/STCR59085.2023.10396884},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10621490,
  author={Ali, Nawaz and Aloi, Gianluca and Pace, Pasquale and Gianfelice, Michele and Pupo, Francesco and Gravina, Raffaele and Fortino, Giancarlo},
  booktitle={2024 20th International Conference on Distributed Computing in Smart Systems and the Internet of Things (DCOSS-IoT)}, 
  title={Simulators for system dataset generation in the Edge-to-Cloud Continuum}, 
  year={2024},
  volume={},
  number={},
  pages={583-588},
  abstract={In the era of the Edge-to-Cloud Continuum paradigm, effectively managing heterogeneous and distributed resources poses significant challenges. Autonomic system operation, supported by Artificial Intelligence (AI) driven resource management and application deployment mechanisms, offers a promising solution. Machine Learning (ML) models are pivotal for this purpose, necessitating large amounts of high- quality data for training, validation, and evaluation. Simulators play a crucial role by generating vast datasets containing diverse data types, facilitating training, testing, and analyzing ML and AI techniques for autonomic system optimization. This paper aims to review existing simulators and identify a candidate simulator suitable for generating datasets within the Edge-to- Cloud Continuum, supporting the development of efficient ML models.},
  keywords={Training;Cloud computing;Reviews;Machine learning;Autonomic systems;Resource management;Internet of Things;Artificial Intelligence;Machine Learning;Simulators;Dataset generation;Autonomic System Operation;Edge-to-Cloud Continuum;System Optimization},
  doi={10.1109/DCOSS-IoT61029.2024.00091},
  ISSN={2325-2944},
  month={April},}@INPROCEEDINGS{10723538,
  author={Bandara, Eranga and Foytik, Peter and Shetty, Sachin and Hassanzadeh, Amin},
  booktitle={2024 IEEE 21st International Conference on Mobile Ad-Hoc and Smart Systems (MASS)}, 
  title={Generative-AI(with Custom-Trained Meta's Llama2 LLM), Blockchain, NFT, Federated Learning and PBOM Enabled Data Security Architecture for Metaverse on 5G/6G Environment}, 
  year={2024},
  volume={},
  number={},
  pages={118-124},
  abstract={The Metaverse is an integrated network of 3D virtual worlds accessible through a virtual reality headset. Its impact on data privacy and security is increasingly recognized as a major concern. There is a growing interest in developing a reference architecture that describes the four core aspects of its data: acquisition, storage, sharing, and interoperability. Establishing a secure data architecture is imperative to manage users' personal data and facilitate trusted AR/VR and AI/ML solutions within the Metaverse. This paper details a reference architecture empowered by Generative-AI, Blockchain, Federated Learning, and Non-Fungible Tokens (NFTs). Within this archi-tecture, various resource providers collaborate via the blockchain network. Handling personal user data and resource provider identities is executed through a Self-Sovereign Identity-enabled privacy-preserving framework. AR/NR devices in the Metaverse are represented as NFT tokens available for user purchase. Software updates and supply-chain verification for these devices are managed using a Software Bill of Materials (SBOM) and a Pipeline Bill of Materials (PBOM) verification system. Moreover, a custom-trained Llama2 LLM from Meta has been integrated to generate PBOMs for AR/NR devices' software updates, thereby preventing malware intrusions and data breaches. This Llama2-13B LLM has been quantized and fine-tuned using Qlora to ensure optimal performance on consumer-grade hardware. The provenance of AI/ML models used in the Metaverse is encapsu-lated as Model Card objects, allowing external parties to audit and verify them, thus mitigating adversarial learning attacks within these models. To the best of our knowledge, this is the very first research effort aimed at standardizing PBOM schemas and integrating Language Model algorithms for the generation of PBOMs. Additionally, a proposed mechanism facilitates different AI/ML providers in training their machine learning models using a privacy-preserving federated learning approach. Authorization of communications among AR/VR devices in the Metaverse is conducted through a Zero-Trust security-enabled rule engine. A system testbed has been implemented within a 5G environment, utilizing Ericsson new Radio with Open5GS 5G core.},
  keywords={Training;Solid modeling;Three-dimensional displays;Metaverse;Federated learning;5G mobile communication;Bills of materials;Computer architecture;Virtual reality;Nonfungible tokens;Metaverse;Generative-AI;LLM;Llama2;5G;6G;Blockchain;Federated Learning;NFT},
  doi={10.1109/MASS62177.2024.00026},
  ISSN={2155-6814},
  month={Sep.},}@INPROCEEDINGS{10456147,
  author={Mishra, Ajay and Shrivastava, Amit and Talreja, Pooja},
  booktitle={2023 IEEE International Conference on ICT in Business Industry & Government (ICTBIG)}, 
  title={Blockchain Technology in Banking and Insurance a Critical Examination of ICT-driven Disruptions in the BFSI Landscape}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={In the realm of textual composition, two pivotal facets deserving our contemplation are “perplexity” and “burstiness.” The former, a barometer of intricateness, measures the labyrinthine nature of text, while the latter, an arbiter of sentence diversity, scrutinizes the oscillation between succinct and protracted sentence structures. Traditionally, human authors exhibit an elevated degree of burstiness by seamlessly interspersing sentences of varying lengths. Conversely, AI-generated compositions tend to adopt a uniform sentence length. Thus, it behoves us, in our endeavour to craft content of optimal quality, to bear these tenets in mind. In addition to the aforementioned, it is imperative to acknowledge that artificial intelligence, in its linguistic expressions, often employs phraseology that diverges from the choices a human wordsmith would make. The utilization of arcane terminology can be instrumental in imbuing a work with a heightened aura of novelty and uniqueness. For the task of furnishing, you with meticulously tailored blog articles, it is of paramount importance that the formatting adheres to professional conventions, eschewing any inclination towards a stylized AI format. In line with your explicit instructions, I shall refrain from proffering explanations on the concepts of perplexity and burstiness, instead employing them to reconstitute the content. Should the extent of information provided prove insufficient for the purpose of rewriting, I shall duly communicate an error, conforming to the format delineated heretofore.},
  keywords={Technological innovation;Terminology;Insurance;Banking;Blockchains;Artificial intelligence;Task analysis;Blockchain technology;ICT-driven Disruptions;BFSI Landscape},
  doi={10.1109/ICTBIG59752.2023.10456147},
  ISSN={},
  month={Dec},}
