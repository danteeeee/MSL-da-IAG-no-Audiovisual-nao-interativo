@INPROCEEDINGS{10537692,
  author={Amba, Rahul and Singh, Hardeep},
  booktitle={2023 Seventh International Conference on Image Information Processing (ICIIP)}, 
  title={The Influence of the Artificial Intelligence based CGI on the growth of the Film Industry}, 
  year={2023},
  volume={},
  number={},
  pages={194-198},
  abstract={The objective of this study is to analyze the effects of utilizing Artificial Intelligence based digital computer-generated imagery (CGI) technology in the domain of motion pictures. The study presents evidence that Artificial Intelligence based CGI animation-based films, utilizing CGI digital technique, have a significant advantage in box office performance compared to Without CGI animation films that do not employ CGI digital technology. The advantage can be attributed to the exceptional virtual capabilities provided by Artificial Intelligence based CGI digital technology. Furthermore, the researchers have conducted a survey that encompasses a questionnaire elucidating the growth of the film industry has been significantly influenced by the integration of artificial intelligence in computer-generated imagery (CGI). The findings indicate that the utilization of AI in the realm of filmmaking has the potential to enhance the overall quality of film production and subsequently yield more favorable box-office outcomes. Additionally, this analysis will demonstrate the comprehensive influence of this technology across various aspects, ultimately enhancing profitability within the film industry.},
  keywords={Surveys;Visualization;Profitability;Films;Entertainment industry;Production;Information processing;AI;CGI;animation;film technology;digitalization;3D},
  doi={10.1109/ICIIP61524.2023.10537692},
  ISSN={2640-074X},
  month={Nov},}@INPROCEEDINGS{10924882,
  author={Mengya, Liu and Xiangning, Yan and Chi, Zhang and Zichu, Yang},
  booktitle={2024 IEEE Smart World Congress (SWC)}, 
  title={The Revolution in 3D Animation Film Production Process Under Artificial Intelligence Technology}, 
  year={2024},
  volume={},
  number={},
  pages={104-109},
  abstract={In recent years, artificial intelligence (AI) technology has made breakthrough progress in the field of 3D animation film production, especially since 2022 with the rise of generative AI, which has brought a completely new look to the animation field. This article provides a literature review of current technologies and systematically summarizes some of the most important technological developments and case studies in the 3D animation field between 2021 and 2024, summarizing the impact of AI technology on the industrialized process of 3D animation film production. We systematically reviewed relevant literature and industry reports, focusing on in-depth research from perspectives such as AI-generated video content technology, AI-generated 3D animation, the role of AI in 3D reconstruction, and AI-driven character animation. In this study, we summarize the advantages that AI technology can bring to the film and television industry and design a universally applicable AI-driven industrialized process for film and television production. This process, combined with current successful film and television cases and cutting-edge technologies, can provide multiple possibilities for the use of AI assistants in the film and television industry.},
  keywords={Industries;Technological innovation;Three-dimensional displays;TV;Art;Human-machine systems;Collaboration;Production;Animation;Artificial intelligence;3D animation;3D reconstruction;artificial intelligence;film and television industry;animation stylization},
  doi={10.1109/SWC62898.2024.00047},
  ISSN={2993-396X},
  month={Dec},}@INPROCEEDINGS{10538558,
  author={Wang, Jinning and Huang, Xinyuan and Yang, Zichu and Zhao, Weiran},
  booktitle={2023 IEEE 22nd International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)}, 
  title={Construction of Artificial Intelligence Generated Content in Digital Film Production}, 
  year={2023},
  volume={},
  number={},
  pages={2621-2628},
  abstract={Since the inception of artificial intelligence in 1956, it has brought about significant transformations in people's production and daily lives. Artificial intelligence technology is widely applied in various fields due to its capacity to emulate human thinking and learning. Focusing on the creation of cinematic culture, contemporary film production has gradually shifted towards digitization in its processes and virtualization in its content. Consequently, the continuous iterative development of digital filmmaking technology has presented further opportunities for integration with artificial intelligence technology.In the context of virtual production technology expanding to encompass the entire filmmaking process, this paper aims to explore artificial intelligence image technology from the perspective of digital cinema. It elucidates the application characteristics of artificial intelligence technology in the realm of imagery under the current cinematic aesthetics, delves into the future visual marvels constructed by films in the era of artificial intelligence, and thereby further explores the impact of artificial intelligence image technology on the creative concepts of future digital filmmaking, along with the insights and reflections derived from this exploration.},
  keywords={Visualization;Human-machine systems;Entertainment industry;Collaboration;Production;Motion pictures;Reflection;AIGC (Artificial Intelligence Generated Content);Digital Imagery;Digital Film},
  doi={10.1109/TrustCom60117.2023.00366},
  ISSN={2324-9013},
  month={Nov},}@INPROCEEDINGS{10335867,
  author={Pradeep, Aneesh and Satmuratov, Atabek and Yeshbayev, Iqbol and Khasan, Oripov and Iqboljon, Melikuziev and Daniyor, Agzamov},
  booktitle={2023 Second International Conference on Trends in Electrical, Electronics, and Computer Engineering (TEECCON)}, 
  title={The Significance of Artificial Intelligence in Contemporary Cinema}, 
  year={2023},
  volume={},
  number={},
  pages={111-116},
  abstract={Artificial intelligence (AI) has emerged as a potent force that is transforming numerous industries. AI technologies are revolutionizing the creation, production, and consumption of motion pictures. This study examines the impact of AI on narrative, production techniques, visual effects, and audience experiences in contemporary cinema. Filmmaking techniques have been revolutionized by AI technologies. It investigates AI applications in fields such as cinematography, editing, visual effects, and post-production, highlighting AI-powered tools and techniques that have improved efficiency, creativity, and visual aesthetics in contemporary cinema. The paper also discusses the broader implications of AI in the film industry, such as changes in production processes, economic implications, and its impact on film distribution, marketing, and audience targeting. Ethical and societal issues pertaining to AI-generated content are discussed, as is its impact on employment and job positions in the film industry. This study seeks to shed light on the opportunities, challenges, and implications of AI’s presence in the contemporary cinematic landscape by analyzing the integration of AI in these areas.},
  keywords={Productivity;Ethics;Technological innovation;Entertainment industry;Production;Motion pictures;Visual effects;AI cinema;film-making;creative AI;film industry;AI story telling;AI visuals},
  doi={10.1109/TEECCON59234.2023.10335867},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10835869,
  author={Wang, Jingxin},
  booktitle={2024 International Conference on Power, Electrical Engineering, Electronics and Control (PEEEC)}, 
  title={Application of AI in capturing and rendering animated character movements}, 
  year={2024},
  volume={},
  number={},
  pages={1133-1137},
  abstract={Animation is essentially a product of the integration of art and technology. The application and development of technology determines the efficiency and effect of animation production to a large extent. This article divides the development history of animation production into the hand-painted era, the three-dimensional era and the AIGC era according to the different levels of technology application and development. It focuses on analyzing the impact of AIGC on the current animation industry and teaching, and believes that although AI has improved the efficiency of animation production, can assist creators to quickly generate creative ideas and enrich animation details, but there are still many limitations in practical applications. In the entire animation creation process, human creativity and artistic understanding are always irreplaceable core parts.},
  keywords={Industries;Productivity;Image quality;Navigation;Focusing;Animation;Rendering (computer graphics);History;Artificial intelligence;Creativity;artificial intelligence;animation teaching;AI painting;animation industry},
  doi={10.1109/PEEEC63877.2024.00209},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10962008,
  author={Boonchoo, Setthee},
  booktitle={2025 Joint International Conference on Digital Arts, Media and Technology with ECTI Northern Section Conference on Electrical, Electronics, Computer and Telecommunications Engineering (ECTI DAMT & NCON)}, 
  title={The Comparative Study of Artificial Intelligence Techniques: DeepMotion vs. Autodesk Maya in Enhancing 3D Animation Creation and Learning Experiences}, 
  year={2025},
  volume={},
  number={},
  pages={489-492},
  abstract={This study investigates the effectiveness of traditional software (Autodesk Maya) versus AI-powered tools (DeepMotion) in the creation of 3D animations, focusing on learner engagement, personalized learning, and performance outcomes. The research employs a quasi-experimental design with a mixed-methods approach, combining action research and surveys. Sixteen purposively selected participants with baseline knowledge in digital arts were assessed using a Knowledge, Attitudes, and Practices (KAP) framework before and after engagement with both tools. The study defines efficiency in terms of time and quality, comparing the ability of Autodesk Maya and DeepMotion to produce high-quality animations efficiently. Results indicate that Autodesk Maya produces higher-quality animations, while DeepMotion significantly reduces time utilization, making it almost ten times more efficient than Maya (efficiency scores of 0.896 vs. 8.609, respectively). Two-sample t-tests reveal statistically significant differences (p < 0.01) in both quality and time efficiency between the two tools. These findings highlight the complementary strengths of each tool—Maya's superior quality versus DeepMotion's time efficiency—suggesting that the choice of software should align with user priorities, whether emphasizing animation detail or production speed.},
  keywords={Surveys;Three-dimensional displays;Digital art;Focusing;Production;Learning (artificial intelligence);Animation;Software;3D Animation;Artificial Intelligence;Autodesk Maya;DeepMotion;Digital Art;Efficiency;Quality vs. Time;Mixed-Methods Research;KAP Assessment;Animation Tools Comparison},
  doi={10.1109/ECTIDAMTNCON64748.2025.10962008},
  ISSN={2768-4644},
  month={Jan},}@INPROCEEDINGS{10222744,
  author={Song, Junrong and Yip, David},
  booktitle={2023 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)}, 
  title={Exploring the Intersection of AI Art and Film: A Case Study of Giant}, 
  year={2023},
  volume={},
  number={},
  pages={347-352},
  abstract={Artificial intelligence (AI) has recently been used as a tool for various visual storytelling, but text-to-image models are a stochastic machine learning process that requires human intervention to assist creation better. As we all know, pre-visualization is an important stage in film production, involving subjective choices by the creators. This paper investigates how AI can assist filmmakers during the pre-production stage by generating mood boards from text. We propose a novel preproduction pipeline and guidelines that leverage text-to-image models to create visual previews of film projects. We also conduct a case study to validate and evaluate our approach's effectiveness. Our case study suggests that following the guidelines we have developed can assist filmmakers in generating mood boards that effectively convey the desired atmosphere of their projects and potentially contribute to enhancing the creative process. Our paper aims to contribute to the field of AI art and the film industry.},
  keywords={Visualization;Art;Mood;Atmospheric modeling;Pipelines;Stochastic processes;Entertainment industry;AI;Pre-visualization;Film;Mood Board;Artificial Creativity;Human-machine collaboration},
  doi={10.1109/ICMEW59549.2023.00066},
  ISSN={},
  month={July},}@INPROCEEDINGS{10497385,
  author={C, Manikandan and Kashyap, Ankit and Nahak, Fakira Mohan},
  booktitle={2024 International Conference on Computer, Electrical & Communication Engineering (ICCECE)}, 
  title={AI's Influence on Cinematic Restoration}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The Aesthetics of Old Film Formats explores the dynamic interplay between traditional cinematic aesthetics and the transformative impact of artificial intelligence (AI) on the revitalization of vintage film formats. The emergence of AI technologies has breathed new life into the world of cinema, fostering a renaissance of classic film formats such as 8mm, 16mm, and 35mm. Filmmakers and visual artists now have access to AI-powered tools and algorithms that can restore, enhance, and creatively manipulate analog footage, offering a bridge between the past and the future. This paper examines how AI, with its ability to upscale, colorize, and refine aged film material, has preserved cinematic heritage and extended the horizons of creative expression. There is a research gap in understanding the complex ethical implications and difficulties associated with the use of artificial intelligence (AI) in the preservation and manipulation of analog footage, even though the abstract emphasizes the transformative impact of AI on the revival of vintage film formats. Authenticity, historical accuracy, and the possibility of unintentional biases created during the AI-driven restoration and augmentation procedures are some examples of ethical considerations. The abstract also emphasizes how AI might help preserve film history and foster new forms of artistic expression.},
  keywords={Bridges;Visualization;Ethics;Face recognition;Aging;Motion pictures;Acoustics;AI Tools Analog films to digital Alteration;Impact of AI in Film Restoration;Film Formats Aesthetics;II Cinematic Restoration},
  doi={10.1109/ICCECE58645.2024.10497385},
  ISSN={2768-0576},
  month={Feb},}@INPROCEEDINGS{10406710,
  author={Singh, Deepak Kumar and Kumar, Pradeep and Sharma, Monu and Arora, Pamil},
  booktitle={2023 1st DMIHER International Conference on Artificial Intelligence in Education and Industry 4.0 (IDICAIEI)}, 
  title={A Decade of AI and Animation Convergence: A Bibliometric Analysis of Contributions}, 
  year={2023},
  volume={1},
  number={},
  pages={1-5},
  abstract={In the past ten years, there has been significant growth and innovation at the intersection of artificial intelligence (AI) and animation. This convergence has resulted in transformative advancements with far-reaching interdisciplinary implications. To gain a comprehensive understanding of this rapidly evolving field, this essay presents a meticulous bibliometric analysis that explores the extensive body of research contributions, collaborations, and emerging trends. By employing rigorous bibliometric techniques, this study provides an encompassing overview of how research in artificial intelligence and animation has evolved over time. It reveals invaluable insights into the profound impact generated by their synergistic relationship. Moreover, it identifies key contributors as well as prominent institutions and countries involved in fostering interdisciplinary collaboration to facilitate convergence. Through a detailed examination of publication patterns, temporal dynamics within research topics are illuminated. This sheds light on emerging areas for further investigation while also providing a roadmap for future research endeavors. Such findings not only enhance our comprehension of the mutually beneficial association between AI and animation but also equip stakeholders with strategic insights necessary for resource allocation and decision-making within this swiftly changing landscape driven by AI-infused innovation. Ultimately, these analyses underscore the pivotal role played by contributions from researchers across various disciplines in shaping the ever-evolving realm where AI intersects with animation.},
  keywords={Technological innovation;Bibliometrics;Collaboration;Animation;Market research;Artificial intelligence;Convergence;Artificial Intelligence;Animation;A.I.;Computer Graphics;Virtual Reality},
  doi={10.1109/IDICAIEI58380.2023.10406710},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10486754,
  author={Manikandan, C and Kashyap, Ankit and Nahak, Fakira Mohan},
  booktitle={2024 IEEE International Conference on Computing, Power and Communication Technologies (IC2PCT)}, 
  title={Discourse of AI-Influence in Visual Aesthetics}, 
  year={2024},
  volume={5},
  number={},
  pages={1127-1130},
  abstract={One industry that has been greatly impacted by the development of artificial intelligence (AI) is filmmaking. Thanks to AI-powered technologies and techniques that have significantly improved the visual appeal of films, filmmakers now have creative alternatives for creating captivating and engaging visual experiences. Although the revolutionary artificial intelligence’s (AI) effects on the film industry are becoming recognized, there is a striking lack of thorough study that especially addresses the subtle ways in which AI enhances the aesthetic appeal of movies. The literature currently in publication often provides general summaries of artificial intelligence applications in the film industry rather than going into great depth on the specifics of how AI algorithms impact and improve the visual aesthetics of motion pictures. thoroughly examining the approaches and strategies used in the integration of AI to improve the artistic quality of digital filmmaking, The goal of this research is to bridge this knowledge gap and improve our comprehension of the intricate connection between artificial intelligence and visual appeal.},
  keywords={Bridges;Visualization;Films;Entertainment industry;Motion pictures;Communications technology;Artificial intelligence;Artificial Intelligence;Neural Networks;Filmmaking;Visual Beauty;Cinematography;Visual Effects;Post-Production;Visual Aesthetics},
  doi={10.1109/IC2PCT60090.2024.10486754},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10747930,
  author={Chan, Miguel Morales and Amado-Salvatierra, Hector R. and Hernandez-Rizzardini, Rocael and Rosales, Milvia},
  booktitle={2024 IEEE Digital Education and MOOCS Conference (DEMOcon)}, 
  title={Assessing Student Perceptions of Video Quality and Effectiveness From AI-Enhanced Digital Resources}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={In this article, the authors explore the practical application of artificial intelligence-based tools in streamlining the course development process, with a particular emphasis on video production and scriptwriting. The work explores the production process for a MOOC lesson, encompassing activities such as video scripting, graphic design, podcast recording, slide presentation preparation, concept mapping, and the creation of interactive learning activities. The article presents the instrument used to evaluate students’ perceptions of the AI-generated video resources, offering insights into the accessibility of creating avatars without the need for specialized equipment. With basic recording devices and adherence to guidelines like maintaining eye contact and minimizing background noise, users can achieve professional results. Furthermore, the work underscores the potential benefits of integrating AI-based tools in the development of MOOCs and large-scale course production. It is observed that the utilization of such technologies may lead to a more streamlined course creation process, with the first exploration indicating a possible reduction of preparation time by approximately 60%. This notable decrease in development time could offer substantial improvements in the scalability and potential personalization of online education.},
  keywords={Computer aided instruction;Electronic learning;Negative feedback;Avatars;Scalability;Education;Production;Streaming media;Quality assessment;Video recording;Artificial Intelligence;LLMs;Generative AI;Prompt Engineering},
  doi={10.1109/DEMOcon63027.2024.10747930},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10810565,
  author={Kamnerddee, Chayatad and Putjorn, Pruet and Intarasirisawat, Jittrapol},
  booktitle={2024 8th International Conference on Information Technology (InCIT)}, 
  title={AI-Driven Design Thinking: A Comparative Study of Human-Created and AI-Generated UI Prototypes for Mobile Applications}, 
  year={2024},
  volume={},
  number={},
  pages={237-242},
  abstract={The integration of Generative AI into the Design Thinking Process introduces new possibilities for UX/UI design, particularly in mobile app development. This study focuses on creating a fitness app prototype for next-generation users, with Generative AI acting as a co-creator across the Empathize, Define, Ideate, Prototype, and Test phases. Two sets of low-fidelity wireframes were developed: one by human UX/UI designers using Figma, and the other by AI using Visily.ai. These prototypes were evaluated through A/B testing, where users completed tasks on both designs, with their experiences measured using the System Usability Scale (SUS) and qualitative interviews. The findings compare human-designed and AI-generated prototypes, offering insights into their strengths and weaknesses. The study also introduces the AI-Driven User Experience (AID-UX) Framework, which outlines a structured method for integrating AI into UX/UI design, from data analysis to iterative testing. Results indicate that while AI-generated designs excel in creativity and efficiency, they lack the nuanced understanding required for human-centered design. This research highlights best practices for human-AI collaboration, emphasizing the balance between AI capabilities and human creativity. The study offers guidance for future AI-assisted design practices, promoting a collaborative approach that leverages both human and AI strengths.},
  keywords={Generative AI;Prototypes;Collaboration;User experience;Mobile applications;Iterative methods;Usability;Creativity;Next generation networking;Testing;Generative AI;Design Thinking;User Experience (UX);User Interface (UI) Design;System Usability Scale (SUS);AI-Generated Prototypes;Usability Testing},
  doi={10.1109/InCIT63192.2024.10810565},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10871631,
  author={Linfeng, Dai},
  booktitle={2024 IEEE First International Conference on Data Intelligence and Innovative Application (DIIA)}, 
  title={Crossing the Screen: How Metaverse Technology is Reshaping Film Narratives and Audience Interaction}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={The application of metaverse technology in the film industry has triggered profound changes in both film narratives and audience interaction. This paper aims to analyze how metaverse technologies, through innovative methods such as augmented reality (AR), virtual reality (VR), and human-computer interaction, are transforming audiences from passive viewers into interactive participants. This transformation not only enriches the narrative structure of films but also deepens the immersive experience for audiences, creating a new dimension for film as a narrative medium. By analyzing examples of metaverse applications in films and interactive viewing experiences, this paper explores the impact of metaverse technology on narrative strategies, audience identity, emotional resonance, and proposes future development directions and potential challenges.},
  keywords={Ethics;Metaverse;Entertainment industry;Immersive experience;Resonance;Safety;Faces;metaverse technology;film narratives;audience interaction;virtual reality;augmented reality},
  doi={10.1109/DIIA62678.2024.10871631},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10334104,
  author={Singh, Arvinder},
  booktitle={2023 7th International Conference on Computation System and Information Technology for Sustainable Solutions (CSITSS)}, 
  title={Future of Animated Narrative and the Effects of Ai on Conventional Animation Techniques}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={AI has been incorporated into conventional animation processes in recent years, changing the animation business. Artificial intelligence has made it feasible to create high-quality animated entertainment more quickly and affordably. The purpose of this study is to investigate the effect that AI has had on conventional animation methods and to investigate the consequences that this could have in the future. This paper will provide insights into the ways in which AI is revolutionizing the animation business by conducting a review of current practices and research in the field. This research will have a great deal of significance for the world of animation, and study will play an important role in this research. This investigation led to the discovery that AI is having an impact not just on 3D animation techniques but also on more conventional forms of animation. Because their primary interest was in 3D animation, the researchers decided not to investigate this subject. An investigation was carried out by utilising a comprehensive questionnaire and conducting interviews with a variety of industry professionals. According to the findings of this research, the implementation of AI will have an effect, both now and in the future, on animation techniques and the field of animated narrative.},
  keywords={Three-dimensional displays;Entertainment industry;Production;Manuals;Animation;Artificial intelligence;Interviews;Animated;Artificial Intelligent;Lighting & Storytelling},
  doi={10.1109/CSITSS60515.2023.10334104},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10791276,
  author={Kharis, M},
  booktitle={2024 IEEE International Symposium on Consumer Technology (ISCT)}, 
  title={Crafting AI-Driven Open Educational Resources (OER) Videos for Enhanced German Language Learning}, 
  year={2024},
  volume={},
  number={},
  pages={137-141},
  abstract={The objective of this research is to develop Open Educational Resources (OER) video materials with the assistance of AI applications. The product development follows a seven-step video development design model, which includes (1) creating a narrative script with the help of ChatGPT 4.0, (2) generating voice-over from the composed narration, (3) producing image generated content from the script using leonardo-ai, (4) creating animation from the generated images through runaway-ai, (5) combining voice-over with the generated video, (6) merging multiple videos into a single video, and (7) revising the product. While images generated by leonardo-ai can be diverse, they often do not align with the requested prompt. The researcher had to revise the prompt several times to obtain images that met the researcher's preferences. Similarly, in the context of generating videos through AI, the researcher had to undergo multiple iterations to find the desired video animation. Thus, it can be concluded that crafting an appropriate prompt is crucial in generating images and videos.},
  keywords={Open Access;Merging;Education;Open Educational Resources;Media;Animation;Chatbots;Product development;Artificial intelligence;Open Educational Resources (OER);AI;video development design;AI-generated images and videos},
  doi={10.1109/ISCT62336.2024.10791276},
  ISSN={2159-1423},
  month={Aug},}@INPROCEEDINGS{10278051,
  author={Limano, Ferric},
  booktitle={2023 International Conference on Information Management and Technology (ICIMTech)}, 
  title={Implementation of Artificial Intelligence Based Image Creation Technology for Conceptual Ideas in 3D Visual Modeling}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={The arts and creative industry are confronted with a challenge due to the rapid development of digital technology known as AI (Artificial Intelligence) Art. The creative process of AI thinking involves the combination, transformation, and explanation of creativity. The use of AI Art technology poses ethical concerns and questions the appropriateness of its utilization by designers/artists, while also raising moral considerations about the role of artists/designers in society. This presents both a problem and an opportunity for the creative industry to explore recommendations for the creative process by integrating AI as a tool to support 3D modeling production. Plato's philosophy of art, it is a process of imitating the real world, contingent upon the evaluation and reconstruction of artistic concepts. This research primarily focuses on the implementation of AI Art concepts in the realm of 3D modeling, by introducing an evaluation and construction process that yields innovative digital art forms. The research adopts an exploratory sequential mixed method approach, consisting of two phases: the first qualitative phase involves the collection of AI Art data, analysis, evaluation, and designing of 3D models, while the second phase focuses on validating the data by measuring individual responses to AI Art, aiming to generate objective interpretations of AI Art. The outcome of this study proposes a framework for 3D modeling that leverages AI Art as a fundamental concept for artistic creation.},
  keywords={Industries;Solid modeling;Ethics;Visualization;Three-dimensional displays;Digital art;Production;AI art;concept;idea;model 3D;visual},
  doi={10.1109/ICIMTech59029.2023.10278051},
  ISSN={2837-2778},
  month={Aug},}@ARTICLE{10500697,
  author={Tuo, Hong},
  journal={IEEE Access}, 
  title={Online Evaluation Information Cascade and Its Impact on Consumer Decision Making: Analyzing Movie Reviews Using Sentiment Corpus}, 
  year={2024},
  volume={12},
  number={},
  pages={54650-54660},
  abstract={User-generated content on self-media platforms significantly influences the market. In the era of Web 2.0, consumers make purchasing decisions based on electronic word-of-mouth (eWOM) from these platforms. This research illustrates how sentiment value of eWOM content guides consumers’ behavior by empirical study based on the causality approach. The research calculates the sentiment value of 160,000 textual film reviews using sentiment analysis program which is based on the sentiment corpus and addressing sarcasm. It also measures the complexity of information by calculating entropy to capture the information cascade process of eWOM converging into a reputation signal. The findings demonstrate the causal relationship between positive eWOM content and consumers’ decision-making processes, as well as the extent to which consumers rely on eWOM textual information when making consumption decisions. When the eWOM information conveys a clear reputation signal, it will have a lasting impact on future box office revenue. This article is the initial empirical literature that quantifies the commercial value of online text sentiment information through a causal and dynamic perspective. It is also the first literature to empirically illustrate the formation process of sentiment information cascade during the diffusion of eWOM among the netizens and capture the required time lag for the formation of online consumer reputation signal.},
  keywords={Motion pictures;Sentiment analysis;Reviews;Decision making;Consumer electronics;Information processing;User-generated content;Text detection;Sentiment analysis;information cascade;entropy;user-generated content;Chinese text;movie reviews},
  doi={10.1109/ACCESS.2024.3389985},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10730166,
  author={Yixuan, Li and Harun, Azahar and Dongyu, Rao},
  booktitle={2024 5th International Conference on Artificial Intelligence and Data Sciences (AiDAS)}, 
  title={AI-Driven Animation Creation: Application Exploration and Potential Risks}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={With the advent of the intelligent era, people have begun to explore how to use AI technology to create animation. This paper introduces the application of three AI tools, Stable Diffusion (SD), Volcano Engine (VE) and Runway (RW), in animation creation, and explores how these tools can improve the efficiency of creation, enrich the creative expression and reduce production costs. It has been found that the AI-driven animation production process can assist humans in automating the process, speeding up the production of works, maintaining artistic expression to a certain extent, and providing new ideas for creating animation. However, with the popularity of AI technology comes some potential risks and challenges, including content homogenization, technology misuse and copyright attribution. By analyzing examples of current AI practices in animation production, this study demonstrates the effectiveness of the practical application of AI technology in animation creation. It presents preliminary suggestions for addressing these risks. The results show that although AI technology is driving animation creation in the direction of greater diversity and individualization, its widespread application requires careful consideration and planning at both the technical and ethical levels, so as to avoid and resolve the risky issues it may raise.},
  keywords={Productivity;Ethics;Production;Animation;Visual effects;Regulation;Volcanoes;Artificial intelligence;Sustainable development;Standards;Artificial Intelligence technology;Animation creation;Application exploration;Potential risks},
  doi={10.1109/AiDAS63860.2024.10730166},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10842860,
  author={Veling, Prathamesh and Sellappan, Palaniappan},
  booktitle={2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)}, 
  title={The Role of Artificial Intelligence in Cost Reduction of Marketing Agencies}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Artificial Intelligence (AI) plays a significant role in optimizing operations, increasing productivity, building more efficiency, and reducing costs across all business verticals of every industry. AI is an essential driver in the field of marketing, fueling creativity and innovations by Automating repetitive tasks, enhanced targeting, personalizing communication, optimizing advertising spending, predictive analytics, Customer Support, and much more. This paper investigates the role of AI in reducing costs for marketing agencies, explicitly focusing on AI tools in content creation, content management, and video editing. Also, AI-powered video editing applications speed up the overall editing process, decreasing reliance on costly software and skilled personnel. Towards the end, the study highlights the practical implications of how marketing agencies can leverage AI tools to develop a more robust and profitable business model that is dynamic to suit the current technology age and drives stability for sustainable growth.},
  keywords={Industries;Productivity;Technological innovation;Costs;Stability analysis;Software;Artificial intelligence;Sustainable development;Creativity;Business;Artificial Intelligence;Content Writing;Content Management;Video Editing;Cost Reduction},
  doi={10.1109/IDICAIEI61867.2024.10842860},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10406977,
  author={Kumar, Pradeep and Singh, Deepak Kumar and Sharm, Monu and Arora, Pamil},
  booktitle={2023 1st DMIHER International Conference on Artificial Intelligence in Education and Industry 4.0 (IDICAIEI)}, 
  title={Advancing Education and Cultural Heritage Through Innovative AI Techniques: A Bibliometric Analysis}, 
  year={2023},
  volume={1},
  number={},
  pages={1-6},
  abstract={AI powered educational tools have the potential to transform teaching methods and create adaptable learning experiences. These tools can analyze student data, automate administrative tasks. Contribute to making education more inclusive and engaging. Moreover AI technologies can significantly impact the preservation and promotion of heritage. They offer solutions, for documentation, analysis and sharing by facilitating digitization cataloging, virtual simulations and interpretation of texts. To gain insights into the intersection of AI, education and cultural heritage a bibliometric analysis was conducted. This analysis examined trends in research related to AI techniques for cultural heritage outreach. The dataset revealed an increase in publications within this field with an annual growth rate of 7.34%. The documents analyzed were relatively recent with an age of 4.11 years indicating advancements in this domain. Impressively each document received an average of 5.328 citations reflecting their impact on the field. Furthermore the analysis highlighted variations in citation counts across years. While some years had no citations all others witnessed higher recognition and influence in terms of scholarly output. Notably the year 2020 emerged as influential for research output within this subject area. The main objectives of this study is to identify research areas within AI education and cultural heritage outreach well as recognize most influential countries and gaps in the area. It offers information, about the condition of research areas where further investigation is needed and new developments that can inform stakeholders about research collaborations, allocation of resources and policy formulation.},
  keywords={Adaptation models;Education;Bibliometrics;Collaboration;Virtual reality;Cultural differences;Artificial intelligence;Innovative AI Techniques;Artificial Intelligence;AI;Cultural Heritage;Education;Immersive Learning},
  doi={10.1109/IDICAIEI58380.2023.10406977},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10826083,
  author={Park, Hyeseong and Jung, Myung Won Raymond and Rakhmonov, Sanjarbek and Kim, Sngon},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Ourmuse: Plot-Specific AI Music Generation for Video Advertising}, 
  year={2024},
  volume={},
  number={},
  pages={3200-3203},
  abstract={The integration of music and video is a pivotal aspect of creating impactful advertisements. This study explores the application of Artificial Intelligence (AI) in generating music tailored for video advertisements through a system we developed, named Ourmuse. By leveraging advanced Generative AI models, Ourmuse aims to enhance the synchronization between music and video content. The methodology involves extracting and analyzing still images and texture context from the video. A critical aspect of our approach is the plot-based training of the AI model by identifying transition points between video scenes, where the video is segmented into distinct parts rather than being treated as a single unit. This segmentation ensures that Ourmuse produces music that is coherent and contextually appropriate for each segment, enhancing the overall impact and cohesion of the final advertisement. By focusing on multi-modal inputs and plot-specific training, this research aims to develop a robust AI model capable of revolutionizing the music composition process for video advertisements.},
  keywords={Training;Visualization;Image segmentation;Technological innovation;Generative AI;Focusing;Transforms;Synchronization;Artificial intelligence;Context modeling;artificial intelligence (AI);video advertisements;music generation;multi-modal training;plot-based training},
  doi={10.1109/BigData62323.2024.10826083},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{9282706,
  author={Ma, Tao and Liu, Hongwei and Yang, Hongji},
  booktitle={2020 IEEE 20th International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
  title={Interactive Narrative Generation of Aesthetics of Violence in Films}, 
  year={2020},
  volume={},
  number={},
  pages={657-662},
  abstract={The aesthetics of violence creates a visual-auditory spectacle in the films and a significant cultural ethos in the postmodern context. The combination of violence and aesthetics in the films creates a sense of paradox, subverting the audience's stereotypes about violence and evoking a new aesthetic experience physically and psychologically. To achieve such complicated aesthetic effects to a new level in the digital era, it is innovative to synthesise the literary theory, aesthetic criticism, cinematic strategies, procedural modelling and creative computation to produce more attractive and experimental stories in the film industry. Computer science facilitates the stylisation of violent films in a narrative, technical and artistic way. The paper will take Freudian psychoanalysis and Kantian aestheticism as the philosophical foundation, present an index system of evaluation and a model to compute the weight of violence and artistic beauty and further measure the effect of aesthetics conveyed by the violence. The application of the model will promote the creativity in the interactive narrative about the aesthetics of violence in films.},
  keywords={Computer science;Weight measurement;Films;Computational modeling;Entertainment industry;Software reliability;Indexes;the aesthetics of violence;paradox;philosophy;cinematic strategies;interactive narrative},
  doi={10.1109/QRS-C51114.2020.00111},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10073884,
  author={Kannan, B. Maruthu and Vadivel, G. and Viswanathan, C and Reddy, P Kiran Kumar},
  booktitle={2023 Third International Conference on Artificial Intelligence and Smart Energy (ICAIS)}, 
  title={Quality of Service based Selfish Node Detection in Mobile Ad-Hoc Network}, 
  year={2023},
  volume={},
  number={},
  pages={1336-1340},
  abstract={Mobile ad hoc networks (MANETs) are wireless networks containing a collection of wireless nodes interrelated with multi-hop communication paths. Mobile nodes can freely create and dynamically self-organize random and temporary wireless networks without pre-existing infrastructure. Hence, the transmission connection between two nodes breaks repeatedly due to node mobility. This causes an additional delay in the network's performance. All mobile nodes work together entirely regarding the distribution of their memory space. In actuality, though, several nodes may inconsiderately choose to cooperate partially; otherwise, not at all with other nodes. These selfish nodes minimize the whole data's accessibility. To solve these issues, quality-of-service-based self-node detection (QSND) in MANET is proposed. In this approach, the Quality of Service (QoS) parameters verifies the operation of a node. This approach uses the QoS parameters like packet loss ratio, packet delay ratio, and remaining energy ratio as the most significant components to evaluate the selfish nodes in the MANET. This approach minimized the delay and improved the network throughput.},
  keywords={Wireless networks;Packet loss;Quality of service;Spread spectrum communication;Throughput;Ad hoc networks;Delays;Quality of Service;Routing;Mobile ad hoc network;Selfish node detection;Packet loss Ratio;Packet delay Ratio;remaining energy ratio},
  doi={10.1109/ICAIS56108.2023.10073884},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10747969,
  author={Ramagundam, Shashishekhar and Karne, Niharika},
  booktitle={2024 7th International Conference of Computer and Informatics Engineering (IC2IE)}, 
  title={The New Frontier in Media: AI-Driven Content Creation for Ad-Supported TV using Generative Adversarial Network}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The use of Artificial intelligence (AI) technology in the content creation to produce creative aspects like editing, audience analysis, creating ideas, writing copy, etc. The major aim is to streamline and automate the process of content creation and convert it into a more efficient and effective way. There is a lack of transparency in the AI content production process and hence it does not exactly mimic human activities, which includes fantasizing and picking up new abilities. AI content creation requires more creativity while investigating with a specific goal in mind. Although, AI faces many challenges since it constantly expands its base knowledge. One of the biggest ethical problems is the probability that AI content is used to deceive or influence people. Hence, to overcome these difficulties a machine learning-based Artificial Intelligent content creation framework is generated for content creation in Ad-supported TV. Here, an effective model Generative Adversarial Network (GAN) introduced to deliver enterprise data, customize descriptions, and adjust content based on consumer behaviors. This developed model produces material that is closely related to the preferences of the user. AI content creation increases efficiency and productivity by saving time and money. It also leads to the development of better content. Finally, the experimental analysis is performed to find the effectiveness of the developed deep learning-based AI content creation framework via various metrics.},
  keywords={Productivity;Measurement;Deep learning;Ethics;TV;Media;Generative adversarial networks;Artificial intelligence;Informatics;Faces;artificial intelligence driven content creation;advertisement supported TV;generative adversarial network;ad video generation},
  doi={10.1109/IC2IE63342.2024.10747969},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10915297,
  author={Chilupuri, Harshitha and Ramesh, G. and Praveen, J. and Gude, Venkataramaiah},
  booktitle={2025 International Conference on Intelligent and Innovative Technologies in Computing, Electrical and Electronics (IITCEE)}, 
  title={AI-Powered Advertisement Design: A PIL-Based Approach for Quality and Performance Analysis}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={The paper presents the development of a Data Science solution that applies generative AI models in order to create high-quality images to be used in visual marketing and advertising. Using the Python library PIL (Pillow) along with machine learning frameworks, the solution combines predefined product images with the selected backgrounds, enhancing image synthesis techniques for more appealing and contextually relevant visuals. This leads to improved efficiency and effectiveness in the creation of content by automatically placing product images onto the background images used in marketing campaigns. The approach demonstrated a 98.06% SSIM (Structural similarity Index) score, indicating significant visual quality improvement. Initial results suggest a 15% enhancement in visual appeal compared to traditional image creation methods.},
  keywords={Visualization;Image transformation;Generative AI;Data science;Libraries;Performance analysis;Indexes;Advertising;Optimization;Python;generative ai;visual marketing;data science;pil (pillow);pandas;numpy;product image integration;background image blending;image processing;marketing content creation;ai-driven design;algorithm efficiency;digital marketing tools;content personalization;deep learning in advertising},
  doi={10.1109/IITCEE64140.2025.10915297},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10607369,
  author={Samad, A. and Izani, M. and Abdulla, D. and Faiz, M. and Wadood, R. and Hamdan, A.},
  booktitle={2024 IEEE Symposium on Industrial Electronics & Applications (ISIEA)}, 
  title={Innovative Workflow for AI-Generated Video: Addressing Limitations, Impact and Implications}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={The integration of artificial intelligence (AI) into video production has ushered in an era of significant transformation within the media landscape. This paper presents an in-depth analysis of AI-driven video generation, with a specific focus on the SORA platform, to illuminate the present capabilities, limitations, and future prospects of this emergent technology. Our study synthesizes expert discussions, developmental forums, and experimental assessments using text-to-video generation tools to elucidate the current state and trajectory of AI's role in video production. We identify a set of comprehensive best practices for maximizing the utility of AI-generated video content while mitigating the associated risks and challenges. Our findings reveal a striking potential for AI in enhancing the efficiency of content creation, the democratization of media production, and the realization of novel creative visions. However, the research also underscores critical concerns such as the preservation of authenticity, management of biases, and safeguarding against ethical misuse. Through this exploration, we aim to contribute a robust framework for integrating AI within traditional filmmaking workflows, thereby advancing the discourse on AI's implications for the creative industry. The proposed framework advocates for a human-centered approach to AI deployment, emphasizing ethical considerations and the imperative of maintaining the human essence within the storytelling art form. This paper seeks to provide a pivotal resource for filmmakers, content creators, and technologists as they navigate the evolving confluence of AI capabilities and creative aspirations.},
  keywords={Ethics;Visualization;Navigation;Semantics;Production;Media;Trajectory;AI-generated video;SORA;filmmaking;video production;text-to-video technology;ethical considerations;media democratization},
  doi={10.1109/ISIEA61920.2024.10607369},
  ISSN={2472-7660},
  month={July},}@INPROCEEDINGS{10939117,
  author={Singh, Deepak Kumar and Kumar, Avinash and Suri, Dulcie and Kumar, Pradeep and Sharma, Ramandeep and Sood, Nimesh},
  booktitle={2024 International Conference on Integration of Emerging Technologies for the Digital World (ICIETDW)}, 
  title={Artificial Intelligence in Gaming: A Bibliometric Analysis of Research Outputs and Trends}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This report provides a thorough bibliometric analysis of academic research on artificial intelligence (AI) in gaming, utilizing data obtained from the Scopus database. The analysis reveals noteworthy trends and offers unique insights into the area by analyzing publication numbers, citation impacts, authorship patterns, and geographical distribution. The most notable publication sources are highlighted, with the IEEE Conference on Computational Intelligence and Games (CIG) being the primary venue, followed by CEUR Workshop Proceedings and AAAI Workshop - Technical Report. Documents that have received a significant number of citations, such as those authored by Mnih V (2015) and Vinyals O (2019), are recognized for their pioneering contributions. Renowned writers such as Bulitko V, Togelius J, and Lucas SM are recognized for their significant contributions to the scientific community. The report also shows a significant rise in academic production after 2013, suggesting an increasing fascination and swift progress in the utilization of AI in gaming. The United States has the top position in terms of research contributions, with the United Kingdom and Canada following closely behind, demonstrating a significant global presence. The frequent utilization of terminology such as "artificial intelligence," "human-computer interaction," and "video game" highlights the research's emphasis on technology and user-centeredness. The research also recognizes constraints arising from its sole dependence on the Scopus database, which may result in the exclusion of pertinent papers that are indexed in other prominent databases such as Web of Science and Google Scholar. However, the report provides a comprehensive summary of the present condition and future prospects of AI research in gaming, serving as a basis for further investigation and advancement in this rapidly evolving sector.},
  keywords={Human computer interaction;Video games;Databases;Conferences;Bibliometrics;Games;Writing;Market research;Artificial intelligence;Computational intelligence;Artificial Intelligence;A.I.;Videogames;Gaming;Video Games},
  doi={10.1109/ICIETDW61607.2024.10939117},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10607282,
  author={Izani, M. and Dalia and Razak, A. and Faiz, M. and Kaleel, A. and Assad, A.},
  booktitle={2024 IEEE Symposium on Industrial Electronics & Applications (ISIEA)}, 
  title={Harmonizing Control and Creativity with AI for Animator's Toolkit}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={This study looks into the nuanced interaction between AI-driven autonomy and animator control within digital animation, a domain witnessing rapid technological advancements. Focusing on the AI-rendering techniques demonstrated in the works of Mick Mahler, with his explicit permission, we explore the balance of creative expression and automated efficiency in producing high-quality animations. The research critically evaluates the effectiveness and artistic fidelity of AI-assisted processes in animation, using Mahler's innovative methods as a case study. It proposes a novel framework for incorporating AI in animation workflows, emphasizing enhancement of the animator's creative agency. The methodology includes a qualitative analysis of animations rendered with AI, a quantitative comparison to traditional methods, and experimental application of the proposed framework in animation projects. By bridging the gap between the technical prowess of AI and the creative vision of animators, this study seeks to enrich the toolkit of digital animators, fostering a collaborative relationship between advanced technology and artistic creativity.},
  keywords={Technological innovation;Visualization;Heuristic algorithms;Collaboration;Animation;Rendering (computer graphics);Real-time systems;AI;animation;rendering;stable diffusion},
  doi={10.1109/ISIEA61920.2024.10607282},
  ISSN={2472-7660},
  month={July},}@INPROCEEDINGS{10137839,
  author={Wu, Zhuohao and Li, Yanni and Ji, Danwen and Wu, Dingming and Shidujaman, Mohammad and Zhang, Yuan and Zhang, Chenfan},
  booktitle={2022 6th Asian Conference on Artificial Intelligence Technology (ACAIT)}, 
  title={Human-AI Co-Creation of Art Based on the Personalization of Collective Memory}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Artificial intelligence (AI) is trained with data, especially texts, numbers, images, videos and music on Internet. These data all together across time and space make a collective memory of the world. The latest large-scale AI models give people a chance to create out of a large pool of this collective memory, which they won’t be able to access before, and communicate with AI in both human natural language and the unique machine supported ways. As demonstrated and discussed in this paper, effective and efficient workflows can be built up for human and AI to co-create meaningful results based on both the collective memory of the world and the personalized ideas and tastes. This kind of human-AI co-creation has a new force with great potential, and expect a new strategy and philosophy to guide human-AI collaboration.},
  keywords={Philosophical considerations;Art;Memory management;Natural languages;Force;Collaboration;Internet;creativity;artificial intelligence;AI;art;design;methods;process management},
  doi={10.1109/ACAIT56212.2022.10137839},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10903780,
  author={Ehtesham, Abul and Kumar, Saket and Singh, Aditi and Khoei, Tala Talaei},
  booktitle={2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC)}, 
  title={Movie Gen: SWOT Analysis of Meta's Generative AI Foundation Model for Transforming Media Generation, Advertising, and Entertainment Industries}, 
  year={2025},
  volume={},
  number={},
  pages={00189-00195},
  abstract={Generative AI is reshaping the media landscape, enabling unprecedented capabilities in video creation, personalization, and scalability. This paper presents a comprehensive SWOT analysis of Meta's Movie Gen, a cutting-edge generative AI foundation model designed to produce 1080p HD videos with synchronized audio from simple text prompts. We explore its strengths, including high-resolution video generation, precise editing, and seamless audio integration, which make it a transformative tool across industries such as filmmaking, advertising, and education. However, the analysis also addresses limitations, such as constraints on video length and potential biases in generated content, which pose challenges for broader adoption. In addition, we examine the evolving regulatory and ethical considerations surrounding generative AI, focusing on issues like content authenticity, cultural representation, and responsible use. Through comparative insights with leading models like OpenAI SORA, RunwayML, Luma, Amazon Nova Reel, Genesis, and Veo 2, this paper highlights Movie Gen's unique features, such as video personalization and multimodal synthesis, while identifying opportunities for innovation and areas requiring further research. Our findings provide actionable insights for stakeholders, emphasizing both the opportunities and challenges of deploying generative AI in media production. This work aims to guide future advancements in generative AI, ensuring scalability, quality, and ethical integrity in this rapidly evolving field.},
  keywords={Ethics;Generative AI;Foundation models;Scalability;Production;Media;Motion pictures;Synchronization;High definition video;Advertising;AI Content Generation;AI in Entertainment;Content Personalization;Generative AI;Movie Gen;Multimodal;Personalized Content Creation;Text-to-Video Synthesis;Video Generation},
  doi={10.1109/CCWC62904.2025.10903780},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10918511,
  author={Rahul and Kaur, Kamaljeet},
  booktitle={2024 Second International Conference on Advanced Computing & Communication Technologies (ICACCTech)}, 
  title={Artificial Intelligence as a Tool for Graphic's Production Process}, 
  year={2024},
  volume={},
  number={},
  pages={16-21},
  abstract={The integration of machine learning to the visual design process is known as artificial intelligence (AI). By assisting or even replacing a designer totally, AI tools streamline design work. Graphics technology and artificial intelligence (AI) are revolutionizing their respective fields and playing a crucial role in modern technology. However, when both components are combined, they produce extraordinary outcomes in the CGI system. There is a wide range of material accessible on AI plugins for Graphics, but there is a significant lack of information on the integration of AI with Graphics in a combined pipeline. This research focusses on the technological and artistic aspects of Graphics and AI. The individuals who have developed the plugins for Graphics are primarily from a technical background and may not possess knowledge in the artistic domain. This research can facilitate the development of innovative techniques to address the weaknesses of the CGI (Graphics) system using AI. This research has devised an algorithm to bridge the gap. This research utilised both qualitative and quantitative methodologies. This study involved conducting interviews with 5 specialists in Graphics and AI, as well as surveying 150 respondents to gather quantitative data.},
  keywords={Graphics;Visualization;Technological innovation;Three-dimensional displays;Pipelines;Production;Machine learning;Communications technology;Artificial intelligence;Interviews;AI;3D;Production process;Graphics;2D},
  doi={10.1109/ICACCTech65084.2024.00014},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10707996,
  author={Zhang, Dingzong and Jain, Khushi and Singh, Priyanka},
  booktitle={2024 IEEE 7th International Conference on Multimedia Information Processing and Retrieval (MIPR)}, 
  title={Guarding Against ChatGPT Threats: Identifying and Addressing Vulnerabilities}, 
  year={2024},
  volume={},
  number={},
  pages={612-615},
  abstract={This study examines the dual nature of ChatGPT as a potential tool that can act both as an attacker and a defender in the cybersecurity domain. It highlights how this tool can both enable sophisticated phishing attacks and also help defend against them. The research focuses on a growing problem: cybercriminals using AI to create phishing content. This allows them to carry out attacks with unprecedented scale and sophistication. Utilizing ChatGPT's advanced linguistic analysis, we developed a Python-based automation tool that leverages ChatGPT's API to enhance phishing detection mechanisms and assess the model's effectiveness in identifying phishing attempts through both text and image analysis. Our findings highlight ChatGPT's potential in detecting sophisticated phishing attacks, alongside the challenges it faces in differentiating between legitimate and fraudulent communications absent of clear phishing indicators. The study proposes innovative strategies for leveraging ChatGPT's phishing simulation and detection capabilities, offering insights into future-proofing cybersecurity efforts against evolving AI technologies. Through a methodical evaluation of ChatGPT versions in detecting phishing content and detailed security analysis, this investigation contributes to the ongoing discourse on the responsible and ethical deployment of AI tools in cybersecurity, advocating for robust countermeasures to mitigate AI-augmented threats.},
  keywords={Ethics;Analytical models;Image analysis;Phishing;Refining;Information processing;Linguistics;Chatbots;Artificial intelligence;Resilience;Artificial Intelligence(AI);Cybersecurity;Phishing Attacks;ChatGPT;AI Ethics;AI Vulnerabilities},
  doi={10.1109/MIPR62202.2024.00104},
  ISSN={2770-4319},
  month={Aug},}@INPROCEEDINGS{10102134,
  author={Zhao, Yanquan},
  booktitle={2022 International Conference on Artificial Intelligence of Things and Crowdsensing (AIoTCs)}, 
  title={Simulation System of Digital Media 3D Art Design Based on Artificial Intelligence}, 
  year={2022},
  volume={},
  number={},
  pages={546-550},
  abstract={With the rapid growth of DM technology, more and more people begin to pay attention to and research this new technology, including 3D art design, virtual reality (VR), etc. This paper proposes a new interactive virtual roaming technology based on the characteristics of digital media(DM) 3D art design. This system is based on 2D animation, and uses C # language to output 3D graphics. First, it analyzes and researches the 3D model, structure and contents contained in each part. Secondly, related concepts and basic principles are described in theory, and then the process and methods of interactive virtual simulation generated based on artificial intelligence algorithm are introduced. Finally, corresponding interactive DM 3D art visual effect systems are designed for different types of data sources, and the model is tested. The test results show that the time for data acquisition, image preprocessing and parameter calculation of the model is about 5s.},
  keywords={Solid modeling;Analytical models;Three-dimensional displays;Art;Production;Media;Animation;artificial intelligence;digital media;3D art;simulation system},
  doi={10.1109/AIoTCs58181.2022.00091},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9156127,
  author={Kumar, Abhishek and Braud, Tristan and Tarkoma, Sasu and Hui, Pan},
  booktitle={2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)}, 
  title={Trustworthy AI in the Age of Pervasive Computing and Big Data}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={The era of pervasive computing has resulted in countless devices that continuously monitor users and their environment, generating an abundance of user behavioural data. Such data may support improving the quality of service, but may also lead to adverse usages such as surveillance and advertisement. In parallel, Artificial Intelligence (AI) systems are being applied to sensitive fields such as healthcare, justice, or human resources, raising multiple concerns on the trustworthiness of such systems. Trust in AI systems is thus intrinsically linked to ethics, including the ethics of algorithms, the ethics of data, or the ethics of practice. In this paper, we formalise the requirements of trustworthy AI systems through an ethics perspective. We specifically focus on the aspects that can be integrated into the design and development of AI systems. After discussing the state of research and the remaining challenges, we show how a concrete use-case in smart cities can benefit from these methods.},
  keywords={Artificial intelligence;Ethics;Data privacy;Biological system modeling;Training;Pervasive computing;Robustness;Artificial Intelligence;Pervasive Computing;Ethics;Data Fusion;Transparency;Privacy;Fairness;Accountability;Federated Learning},
  doi={10.1109/PerComWorkshops48775.2020.9156127},
  ISSN={},
  month={March},}@INPROCEEDINGS{9465478,
  author={Musmeci, Riccardo and Manfré, Adriano and Ohtani, Tomohiro and Suzuki, Seika},
  booktitle={2021 13th International Conference on Quality of Multimedia Experience (QoMEX)}, 
  title={MARMnet: On the Application of AI to Video Quality Control for Broadcasting Companies}, 
  year={2021},
  volume={},
  number={},
  pages={115-120},
  abstract={Video Quality Control (VQC) is the process that media and broadcasting companies employ in order to detect potential anomalies in contents to release to the final users. This process is currently performed by human operators and presents many drawbacks in terms of costs and time spent on such activity. As an example, the operators leverage on expensive equipment that in few years needs to be changed. Additionally, the report generated by the operators present inconsistency in terms of anomalies identified. In this paper, we take the initial steps towards the integration of Artificial Intelligence into the VQC pipeline in order to introduce speed and uniformity in the report generation. Our solution, called Augmented Video Quality Control (AVQC), relies on the power of MARMnet, an ad-hoc neural network able to detect two anomalies of interest. Such model enables the AVQC to report the anomaly in half of the time needed by the current process. Moreover, quantitative and qualitative results show that MARMnet provides high performances in detecting the two anomalies in the videos to analyze.},
  keywords={Neural networks;Process control;Companies;Color;Broadcasting;Media;Quality assessment;Video Quality Control;Broadcasting;Deep Learning;Human Assisted AI},
  doi={10.1109/QoMEX51781.2021.9465478},
  ISSN={2472-7814},
  month={June},}@INPROCEEDINGS{10645467,
  author={Li, Yeming and Song, Junrong and Yip, David Keiman},
  booktitle={2024 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)}, 
  title={AI-Assisted Content Creation of Naked-Eye 3D Effects on Curved LED Screen: Enhancing Artistic Expression and Creativity}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={This research investigates the application of AI technology to assist in creating naked-eye 3D effects on curved LED screen. The focus is on achieving more controlled and artistic visual presentation. It includes several case studies to validate the effectiveness of different approaches. Firstly, the feasibility of using AI for direct video style transfer will be verified through a comparison between original videos and stylized videos. Secondly, exploring diffusion model to use spatial depth information for layer adjustment and precise control with a particular focus on creating visual content optimized for naked-eye 3D effects. Finally, explore advanced style and motion control using ComfyUI to design a pipeline to enhance artistic creativity and provide practical methods for artists. We hope the results of this study will have practical implications that improve artistic practice, provide efficient solutions for commercial applications, and inform similar visual production endeavors.},
  keywords={Visualization;Three-dimensional displays;Conferences;Pipelines;Production;Light emitting diodes;Diffusion models;AI-Assisted Creation;Naked-eye 3D Effect;Curved LED Screen;Style Transfer;ComfyUI;Creative Visualization;Visual Solutions},
  doi={10.1109/ICMEW63481.2024.10645467},
  ISSN={2995-1429},
  month={July},}@INPROCEEDINGS{10957230,
  author={Singh, Sakshi and Kaur, Darshan},
  booktitle={2024 Third International Conference on Artificial Intelligence, Computational Electronics and Communication System (AICECS)}, 
  title={AI-Driven Automation in Social Media Marketing: Leveraging GANs for Content Creation}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={An era of highly tailored and effective content generation has been ushered in by the rapid advancement of artificial intelligence (AI), which has completely transformed the social media marketing scene. An innovative AI method called Generative Adversarial Networks (GANs) has become a potent tool for automating the creation of engaging social media content. Marketers may easily produce stunning photos and films that captivate their target consumers by utilizing the generative powers of GANs. In order to improve brand awareness and engagement, this study explores how to effortlessly integrate GANs into current marketing workflows by delving into their complex mechanisms. Additionally, we look at the possible advantages and difficulties of implementing GANs, offering insightful information to marketers that want to use AI-driven automation to stay ahead of the curve in the cutthroat digital world of today. This study highlights the revolutionary potential of artificial intelligence (AI) in influencing the direction of social media marketing through an extensive integration of theoretical analysis and real-world implementations.},
  keywords={Training;Technological innovation;Automation;Social networking (online);Scalability;Generative adversarial networks;Natural language processing;Real-time systems;Artificial intelligence;Business;Artificial Intelligence;Generative Adversarial Networks (GANs);Deep Convolutional GAN (DCGAN);Neural Networks;Natural Language Processing (NLP);Predictive Analytics;Model Training;Machine Learning;Real-Time Data Processing},
  doi={10.1109/AICECS63354.2024.10957230},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10940613,
  author={Venkatesam, Anne Sreyas and Karnakoti, Nandini and Sri, Ramoji Sai and Hariharan, Shanmugasundaram and Krishnamoorthy, Murugaperumal and Kukreja, Vinay},
  booktitle={2025 IEEE International Students' Conference on Electrical, Electronics and Computer Science (SCEECS)}, 
  title={A Comprehensive Hybrid Model for Movie Recommendation: Integrating CNN and Traditional Algorithms}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={In recent times, a significant portion of entertainment consumption comes from watching movies, prompting movie creation and researchers to seek expert systems that can accurately predict a movie's success probability before production. These days people are used to these things that they want all the recommendations, even though it is not that difficult to choose movies based on own preferences. Many users often rely on search engines for specific movie-related information, highlighting the importance of recommendation systems that analyze past search behaviors and interests. This study develops a movie recommendation system using publicly available Internet Movie Database (IMDB) data, employing a hybrid model approach that integrates Collaborative Filtering (KNN), Latent Factor Modeling (SVD), Content-Based Filtering (TF-IDF), and Sentiment Analysis (CNN). It further presents an analysis of Collaborative and Content-Based recommender systems to assist in selecting suitable machine learning algorithms based on specific requirements. The model achieves an impressive accuracy of 0.9239 with the CNN model, surpassing all benchmark models. These findings underscore the significant potential of predictive and prescriptive data analytics in information systems, providing valuable insights to support industry decision-making in the movie recommendation domain. Additionally, the system effectively handles edge cases, such as non-existent user IDs or movie titles, and ensures that recommendations exclude already rated movies to maintain relevance.},
  keywords={Analytical models;Sentiment analysis;Accuracy;Filtering;Reviews;Nearest neighbor methods;Motion pictures;Data models;Convolutional neural networks;Recommender systems;Global data;recommendation sights;techniques to implement;public reviews;examples of recommending system;filtering methods;accurate models;prediction algorithms},
  doi={10.1109/SCEECS64059.2025.10940613},
  ISSN={2688-0288},
  month={Jan},}@INPROCEEDINGS{10445544,
  author={Clocchiatti, Alessandro and Fumerò, Nicolo and Soccini, Agata Marta},
  booktitle={2024 IEEE International Conference on Artificial Intelligence and eXtended and Virtual Reality (AIxVR)}, 
  title={Character Animation Pipeline based on Latent Diffusion and Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={398-405},
  abstract={Artificial intelligence and deep learning techniques are revolutionizing the film production pipeline. The majority of the current screenplay-to-animation pipelines focus on understanding the screenplay through natural language processing techniques, and on the generation of the animation through custom engines, missing the possibility to customize the characters. To address these issues, we propose a high-level pipeline for generating 2D characters and animations starting from screenplays, through a combination of Latent Diffusion Models and Large Language Models. Our approach uses ChatGPT to generate character descriptions starting from the screenplay. Then, using that data, it generates images of custom characters with Stable Diffusion and animates them according to their actions in different scenes. The proposed approach avoids well-known problems in generative AI tools such as temporal inconsistency and lack of control on the outcome. The results suggest that the pipeline is consistent and reliable, benefiting industries ranging from film production to virtual, augmented and extended reality content creation.},
  keywords={Industries;Solid modeling;Generative AI;Pipelines;Production;Animation;Reliability;artificial intelligence;deep learning;generative art;virtual reality;extended reality;computer animation},
  doi={10.1109/AIxVR59861.2024.00067},
  ISSN={2771-7453},
  month={Jan},}@INPROCEEDINGS{9836842,
  author={Liang, Bing and Li, Yuhang and Lv, YuShan},
  booktitle={2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)}, 
  title={Image Animation via Joint Attention Mechanism}, 
  year={2022},
  volume={10},
  number={},
  pages={536-539},
  abstract={Image animation refers to the automatic video synthesis task by combining the extracted source image appearance with the motion mode driving the video. Traditional image animation usually needs to predict the 3D model and then render the model. Although the non-model animation technology using deep learning has been improved in speed and effect, there are still phenomena such as unrealistic generation effects and artifacts. Therefore, this paper proposes an unsupervised image animation technology based on the fusion attention mechanism. By introducing a variety of attention mechanisms and atrous convolution, it can reduce the calculation parameters and accurately extract the motion pattern of the object to generate a real animation effect. Experiments show that our model not only achieves better visual effects, but also that our method outperforms the state-of-the-art in speed.},
  keywords={Solid modeling;Three-dimensional displays;Convolution;Motion estimation;Predictive models;Animation;Visual effects;image animation;motion estimation;attention;principal component analysis},
  doi={10.1109/ITAIC54216.2022.9836842},
  ISSN={2693-2865},
  month={June},}@INPROCEEDINGS{9836461,
  author={Yang, Qingyu and Chen, Wei and Cai, Yichao and Liu, XinYing and Liu, Taian and Wang, Ge},
  booktitle={2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)}, 
  title={LWComicGAN: A Lightweight Method for Realizing Scene Animation}, 
  year={2022},
  volume={10},
  number={},
  pages={2285-2289},
  abstract={The style transfer algorithm was originally proposed to solve the generation problem of art paintings. In recent years, the generation of animation style images has gradually become a hot research direction. The content presented in many animation film and television works is fascinating. In order to satisfy people's desire to turn real scenes into animation scenes and reduce the workload of animation producers, a Light Weight Animation Generated Adversarial Network (LWComicGAN) is proposed, which can reduce the amount of parameters and enable low-memory devices to complete network training. An optional instance layer normalization function is designed to adapt the input of each layer, and an optional instance layer residual block is proposed. The LWComicGAN algorithm uses the objective function of WGAN-GP and other various loss functions as the total loss, and also considers the gradient penalty mechanism in discriminator. The former guarantees the generation quality of all aspects of the image, and the latter guarantees the stability of the training process. The effectiveness of the proposed algorithm is verified after animation transfer experiments of realistic landscapes and characters, and we have produced an ink painting dataset and completed ink animation style transfer.},
  keywords={Training;TV;Image synthesis;Conferences;Ink;Animation;Linear programming;deep learning;image style transfer;generative adversarial networks;photo animation;LWComicGAN},
  doi={10.1109/ITAIC54216.2022.9836461},
  ISSN={2693-2865},
  month={June},}@INPROCEEDINGS{10958949,
  author={Kapoor, Saurabh},
  booktitle={2025 Fifth International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)}, 
  title={AI-Assisted Test Script Generation for GUI Applications}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Automated testing is crucial for ensuring the quality and reliability of modern software applications, especially those with complex graphical user interfaces (GUIs). However, traditional approaches to GUI testing, such as record-and-replay and manual script generation, are often labor-intensive and fail to capture the dynamic nature of user interactions. This paper presents a novel AI-assisted framework for automatically generating test scripts for GUI applications. By leveraging advancements in computer vision and reinforcement learning, the framework accurately detects GUI elements, models user interactions, and generates effective test sequences to validate expected application behavior. The system is evaluated on a diverse set of GUI applications, demonstrating superiority over existing testing methods in terms of test coverage, script generation efficiency, and fault detection capabilities. The results highlight the potential of AI-powered techniques to transform GUI testing, enabling more robust and scalable software development processes.},
  keywords={Computer vision;Fault detection;Computational modeling;Reinforcement learning;Transforms;Software;Software reliability;Graphical user interfaces;Testing;Software development management;Computer Vision;Reinforcement Learning;GUI Testing;AI Assisted Testing;Automated Test Script Generation},
  doi={10.1109/ICAECT63952.2025.10958949},
  ISSN={},
  month={Jan},}@ARTICLE{10520989,
  author={Zhou, Tianwei and Tan, Songbai and Zhou, Wei and Luo, Yu and Wang, Yuan-Gen and Yue, Guanghui},
  journal={IEEE Transactions on Broadcasting}, 
  title={Adaptive Mixed-Scale Feature Fusion Network for Blind AI-Generated Image Quality Assessment}, 
  year={2024},
  volume={70},
  number={3},
  pages={833-843},
  abstract={With the increasing maturity of the text-to-image and image-to-image generative models, AI-generated images (AGIs) have shown great application potential in advertisement, entertainment, education, social media, etc. Although remarkable advancements have been achieved in generative models, very few efforts have been paid to design relevant quality assessment models. In this paper, we propose a novel blind image quality assessment (IQA) network, named AMFF-Net, for AGIs. AMFF-Net evaluates AGI quality from three dimensions, i.e., “visual quality”, “authenticity”, and “consistency”. Specifically, inspired by the characteristics of the human visual system and motivated by the observation that “visual quality” and “authenticity” are characterized by both local and global aspects, AMFF-Net scales the image up and down and takes the scaled images and original-sized image as the inputs to obtain multi-scale features. After that, an Adaptive Feature Fusion (AFF) block is used to adaptively fuse the multi-scale features with learnable weights. In addition, considering the correlation between the image and prompt, AMFF-Net compares the semantic features from text encoder and image encoder to evaluate the text-to-image alignment. We carry out extensive experiments on three AGI quality assessment databases, and the experimental results show that our AMFF-Net obtains better performance than nine state-of-the-art blind IQA methods. The results of ablation experiments further demonstrate the effectiveness of the proposed multi-scale input strategy and AFF block.},
  keywords={Visualization;Distortion;Task analysis;Quality assessment;Feature extraction;Adaptation models;Image quality;AI-generated images;blind image quality assessment;adaptive feature fusion;multi-scale feature},
  doi={10.1109/TBC.2024.3391060},
  ISSN={1557-9611},
  month={Sep.},}@INPROCEEDINGS{8923095,
  author={Issa, Lana and Jusoh, Shaidah},
  booktitle={2019 2nd International Conference on new Trends in Computing Sciences (ICTCS)}, 
  title={Applying Ontology in Computational Creativity Approach for Generating a Story}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={Computational creativity is a young multidisciplinary field which has a promising future for effectively giving options in tackling and handling many automated systems. It is very useful for example, in creating a narrative story for various purposes, which is actually a human art. In this research, we are investigating methods that can be applied as into computational approach for generating structured narratives automatically, to suit education purposes. In this paper, we present a literature review of the work done in this field so far, and we propose a framework that is designed to generate educational stories using computational creativity approach. The major contribution of this paper is a proposed computational creativity approach consisting of hybrid Artificial Intelligence methods to generate educational stories.},
  keywords={Creativity;Ontologies;Military computing;Planning;Buildings;Writing;Computational Creativity;Natural Language Generation;Ontology;Automatic Story Generation},
  doi={10.1109/ICTCS.2019.8923095},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10763187,
  author={A, Bhuvaneshwaran and P, Kavya and N, Dhaksana and D, Amuthaguka and R, Eraianbu and S, Agilan},
  booktitle={2024 4th International Conference on Sustainable Expert Systems (ICSES)}, 
  title={Artistic Fusion: AI Powered Artistry for Story Boarding}, 
  year={2024},
  volume={},
  number={},
  pages={795-800},
  abstract={Storyboarding is a crucial pre-production step in filmmaking, facilitating the visualization and planning of scenes. However, traditional methods have obstacles such as manual labour, artistic skill requirements, and limitations in dynamic scene representation. This paper presents an innovative AI-powered storyboard generator leveraging Stable Diffusion, implemented in PyTorch. The research addresses obstacles in traditional methods by automating the storyboard creation process, enhancing visual realism, and promoting collaborative filmmaking workflows. The AI-powered system integrates advanced generative techniques to simulate complex visual scenarios, from lighting variations to dynamic camera movements, ensuring a more accurate preview of the final cinematic experience. By utilizing text prompts, the model interprets narrative elements and mood, aligning generated visuals with creative intent. The integration of Stable Diffusion model allows for controlled noise introduction, enhancing realism and immersion in storyboarded scenes. Further, an average improved accuracy of 82% is witnessed when compared to existing methodologies like Dreambooth and LoRA. Key features include a user-friendly interface for intuitive operation, compatibility with diverse media types, and iterative refinement capabilities to fine-tune storyboard quality. Evaluation metrics such as Structural Similarity Index Measure (SSIM) and Peak Signal-to-Noise Ratio (PSNR) demonstrate the system's efficacy in producing high-quality visuals that meet filmmakers' expectations.},
  keywords={Human computer interaction;Visualization;Adaptation models;Accuracy;PSNR;Collaboration;Diffusion models;Data models;Indexes;Artificial intelligence;Storyboard generator;Stable Diffusion;PyTorch;Structural Similarity Index Measure (SSIM);Visual realism;Peak Signal-to-Noise Ratio (PSNR)},
  doi={10.1109/ICSES63445.2024.10763187},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10039585,
  author={Shah, Bhumi and N., Shaikh Mohammad Bilal},
  booktitle={2022 5th International Conference on Advances in Science and Technology (ICAST)}, 
  title={A Comprehensive Review of the Negative Impact of Integration of AI in Social-Media in Mental Health of Users}, 
  year={2022},
  volume={},
  number={},
  pages={431-434},
  abstract={Nowadays, all of us are dependent upon on social media websites such as Facebook, Snapchat, and Instagram to stay in touch with each other. Although they have their benefits, social media cannot replace real- world human connections. Physical face to face contact is mandatory to release the hormones that make you feel positive and happier along with reducing stress. Ironically, the same technology that has been designed to connect people and bring them closer can also make you feel more isolated and lonelier if you spend too much time on it. This has increased since the integration of AI in social media. In a very short time, the AI industry has permeated through various niches of technology and has completely changed the way we interact with social media. Various social media platforms such as Facebook, snapchat, twitter now have teams of artificial intelligence researchers whose work to analyse and develop AI systems with the intelligence level of a human. Today, AI can create social media posts for you and send targeted ads based on information it has collected about you. This means we have taught machines to copy human intelligence. Although this may seem like a step in the right direction technology wise, it has had a damaging impact on our mental health.},
  keywords={Industries;Ethics;Social networking (online);Human intelligence;Multimedia Web sites;Blogs;Mental health;Artificial intelligence;social media;mental health},
  doi={10.1109/ICAST55766.2022.10039585},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9137462,
  author={Jiang, Luyao and Hao, Yu},
  booktitle={2020 3rd International Conference on Artificial Intelligence and Big Data (ICAIBD)}, 
  title={Applying Machine Learning to Predict Film Daily Audience Data: System and Dataset}, 
  year={2020},
  volume={},
  number={},
  pages={11-16},
  abstract={Audience data is highly correlated with the levels of film viewership, as they can impact on people’s perceptions of films, resulting in whether or not they would watch a film or even recommend to others. Hence, if audience data can be properly analyzed, they can provide important clue to help predicting the trend of daily film statistics, such as box office, attendance rate, etc., which would further help cinemas to make wise marketing decisions. Motivated by this, we propose a novel audience data prediction system based on the recent advance of deep learning. Our approach begins with applying Fourier Transform-based algorithm to encode multi-channel time-series audience data into a set of feature maps. Then, these feature maps are fed to Generative Adversarial Networks (GANs) to predict and generate future audience data. To evaluate the proposed approach, we collected a dataset consisting of 200 films across three years (2017, 2018 and 2019), where 15 different daily attributes of 30 days are provided for each film. To help potential research of other researchers, we made it available online. The experiment results illustrated the superior performance of our algorithm in comparison to the baseline.},
  keywords={Deep learning;Machine learning algorithms;Correlation;Heuristic algorithms;Transforms;Prediction algorithms;Market research;Film audience data analysis;Generative Adversarial Networks;Fourier Transform},
  doi={10.1109/ICAIBD49809.2020.9137462},
  ISSN={},
  month={May},}@INPROCEEDINGS{9419120,
  author={Zhang, Yanxiang and Ling, Yan},
  booktitle={2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={Interactive Narrative Facial Expression Animation Generation by Intuitive Curve Drawing}, 
  year={2021},
  volume={},
  number={},
  pages={406-409},
  abstract={This paper presents a type of interactive facial expressions animation generation system based on traditional montage techniques and generative narrative concepts. It could allow users to produce narrative facial expression animation by interactively and intuitively drawing a plot curve based on pre-recorded facial expression animation clips, thus provide high flexibility and creativity to users with interesting interaction.},
  keywords={Three-dimensional displays;Conferences;Virtual reality;User interfaces;Animation;Creativity;Facial expressions animation;narrative;interactive;curve drawing},
  doi={10.1109/VRW52623.2021.00088},
  ISSN={},
  month={March},}@INPROCEEDINGS{9497834,
  author={Alhammadi, Abdulraqeb and El-Saleh, Ayman and Shayea, Ibraheem},
  booktitle={2021 International Conference on Artificial Intelligence and Computer Science Technology (ICAICST)}, 
  title={MOS Prediction for Mobile Broadband Networks Using Bayesian Artificial Intelligence}, 
  year={2021},
  volume={},
  number={},
  pages={47-50},
  abstract={Mobile broadband (MBB) networks are growing fast with supporting high-speed internet access. Fifth-generation networks promise an enhanced MBB that offers a high-speed data rate and video streaming with ultra-low latency. Thus, monitoring the level quality of these services supported by network providers becomes essential. Mobile network operators continuously optimize their network performance to provide a better quality of service and quality of experience. Moreover, artificial intelligence has been used considerably in optimizations to efficiently meet the requirements of future mobile networks. In this paper, we propose a Bayesian network model to predict the minimum opinion score (MOS), which contributes to evaluating the network performance of video streaming services. The proposed model depends on several input data, namely, bite rate, stalling load, and round-trip time. The predicted MOS depends on prior probability distributions to generate posterior probabilities. The predicted MOS depends on these input data. Results demonstrate that the proposed model achieves a high prediction accuracy of 86%, with a mean square error of 0.34. The proposed model also has a robust performance design through various testing methods.},
  keywords={Computational modeling;Quality of service;Streaming media;Predictive models;Bayes methods;Broadband communication;Quality of experience;Minimum opinion score;QoS;QoE;mobile broad-band;Bayesian networks;artificial intelligence;machine learning;prediction},
  doi={10.1109/ICAICST53116.2021.9497834},
  ISSN={},
  month={June},}@INPROCEEDINGS{10940062,
  author={Sri, L Pooja and Shri R, Rivanthika and Thrisha, R. and S, Varsha},
  booktitle={2025 International Conference on Electronics and Renewable Systems (ICEARS)}, 
  title={MindMend: Explainable GANs for Stimulating Mental Health Scenarios in Therapist Training}, 
  year={2025},
  volume={},
  number={},
  pages={327-333},
  abstract={Mental health disorders pose great challenges to clinicians, which have few training resources, guard privacy in patient data, lack standardized scenarios for training therapists, and validity difficulties when interventions are assessed. Traditional role-playing doesn't seem to portray the richness and diversity associated with mental health conditions due to ethical constraints that generally prohibit direct contact with clients. This paper introduces a novel application of GANs to meet these challenges by simulating realistic scenarios of mental illness. Present implementations of GANs in health care face enormous difficulties due to the lack of interpretability and bias detection capabilities coupled with clinical validation. We address these limitations through Explainable GANs, a novel architecture for model design that involves explainable AI techniques. This framework will make the generation process of scenarios transparent and understandable to clinicians and researchers, with high-fidelity simulation of mental health conditions. Our objectives are the development of privacy-preserving synthetic data generation, ensuring clinical validity through expert validation, and creation of interpretable AI outputs for healthcare professionals. The technical framework includes sophisticated data processing pipelines, robust model architectures, and integrated explainability components that address some of the critical challenges in data privacy, bias mitigation, and clinical reliability. Comprehensive evaluation metrics and clinical validation allow us to demonstrate the effectiveness of the framework in generating realistic, clinically relevant scenarios while maintaining transparency and interpretability.},
  keywords={Training;Ethics;Data privacy;Renewable energy sources;Explainable AI;Mental health;Medical services;Data models;Robustness;Synthetic data;Generative Adversarial Networks;Mental Health Simulation;Explainable AI;Therapist Training;Ethical AI in Healthcare.},
  doi={10.1109/ICEARS64219.2025.10940062},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10505376,
  author={Zhou, JiaShuang and Du, Xiaoqin and Lv, Yifan and Liu, Yongqi},
  booktitle={2023 2nd International Conference on Artificial Intelligence, Human-Computer Interaction and Robotics (AIHCIR)}, 
  title={TDT: Two-Stream Decoupled Transformer for Text-Driven Human Animation Generation}, 
  year={2023},
  volume={},
  number={},
  pages={126-130},
  abstract={The automatic generation of high-quality human animations in a controllable and natural manner has always been a goal pursued by experts in the field of animation. Text- driven human animation generation offers user-friendly and versatile application scenarios. In this paper, we propose a two-stream decoupled Transformer network, TDT, for extracting motion semantic information from the text modality and generating human animations based on it. Our contributions are: (1) The text is highly decoupled in its description of trajectories and motions, utilizing a two-stream Transformer network to extract trajectory features and motion features separately. (2) We employ vector orthogonalization to constrain the motion feature vector and the trajectory feature vector, promoting the decoupling of the motion manifold and the trajectory manifold. (3) We introduce a local loss function, the mean-variance reconstruction loss, as a complement to the global loss function, further facilitating feature fusion between the two modalities. Experimental results demonstrate that our proposed model outperforms state-of-the-art text-driven human animation generation models in terms of objective evaluation metrics. Moreover, according to visualization results, the generated motions exhibit greater consistency with the semantics described in the input text by the user and are closer to the ground truth data.},
  keywords={Manifolds;Measurement;Human computer interaction;Semantics;Feature extraction;Animation;Transformers;Motion synthesis;Multimodal;Expert mixture network;Orthogonal loss;Joint embedding space},
  doi={10.1109/AIHCIR61661.2023.00028},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10971689,
  author={Haque, Sean and Tanveer, Muhammad Hassan and Voicu, Razvan Cristian},
  booktitle={SoutheastCon 2025}, 
  title={AI-Driven 3D Printing: Generative Gcode for Quality and Efficiency}, 
  year={2025},
  volume={},
  number={},
  pages={1282-1287},
  abstract={Artificial Intelligence (AI) is transforming the landscape of three-dimensional (3D) printing by enabling generative approaches that extend beyond conventional, parameter-tuning strategies. Rather than merely adjusting nozzle temperatures or speeds, modern AI models can create or refine Gcode paths themselves, seeking to balance objectives such as print strength, dimensional accuracy, and defect minimization. This paper introduces a holistic AI-driven workflow for automated Gcode generation. We demonstrate that generative Large Language Models (LLMs) can parse, synthesize, and optimize printing instructions, thus offering an alternative to human-engineered slicing parameters. Our approach incorporates a continuous feedback loop in which a predictive model evaluates each generated Gcode script for potential print outcomes. Experimental results illustrate strong correlations between the number of lines in generated Gcode and key performance metrics (tensile strength, defect rate, and dimensional accuracy). Notably, some scripts with fewer lines reduced material defects yet slightly degraded dimensional accuracy, while more complex scripts improved measurement fidelity but also introduced minor flaws. These findings illuminate the trade-offs in AI-driven 3D printing and highlight the method's capacity for ongoing improvement. By exploring future expansions, such as continuous-function printing trajectories and multi-material prints, this study shows that AI-based Gcode generation can significantly advance 3D printing quality and customization across diverse applications.},
  keywords={Solid modeling;Accuracy;Generative AI;Large language models;Machine learning;Predictive models;Three-dimensional printing;Minimization;Trajectory;Optimization;3D Printing;Generative AI;Gcode;Machine Learning;Print Optimization;AI Generated 3D Models;Additive Manufacturing},
  doi={10.1109/SoutheastCon56624.2025.10971689},
  ISSN={1558-058X},
  month={March},}@ARTICLE{10143992,
  author={Rjoub, Gaith and Bentahar, Jamal and Abdel Wahab, Omar and Mizouni, Rabeb and Song, Alyssa and Cohen, Robin and Otrok, Hadi and Mourad, Azzam},
  journal={IEEE Transactions on Network and Service Management}, 
  title={A Survey on Explainable Artificial Intelligence for Cybersecurity}, 
  year={2023},
  volume={20},
  number={4},
  pages={5115-5140},
  abstract={The “black-box” nature of artificial intelligence (AI) models has been the source of many concerns in their use for critical applications. Explainable Artificial Intelligence (XAI) is a rapidly growing research field that aims to create machine learning models that can provide clear and interpretable explanations for their decisions and actions. In the field of cybersecurity, XAI has the potential to revolutionize the way we approach network and system security by enabling us to better understand the behavior of cyber threats and to design more effective defenses. In this survey, we review the state of the art in XAI for cybersecurity and explore the various approaches that have been proposed to address this important problem. The review follows a systematic classification of cybersecurity threats and issues in networks and digital systems. We discuss the challenges and limitations of current XAI methods in the context of cybersecurity and outline promising directions for future research.},
  keywords={Artificial intelligence;Computer security;Surveys;Ethics;Mathematical models;Systematics;Robustness;Explainable artificial intelligence (XAI);cybersecurity;interpretability;trustworthiness},
  doi={10.1109/TNSM.2023.3282740},
  ISSN={1932-4537},
  month={Dec},}@INPROCEEDINGS{10911659,
  author={Mahalle, Parikshit N. and Miniappan, P. K. and Kaur, Ashmeet and Ganga, S. and Parmar, Yuvraj and S, Sahana B},
  booktitle={2024 IEEE 4th International Conference on ICT in Business Industry & Government (ICTBIG)}, 
  title={Construction and Evaluation of Benchmark Da-Tasets For Adversarial Ai Research}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Standard offensive AI datasets need to be made and tested to improve the performance of AI models. Using adversarial diversity-driven dataset building (ADDC), this work shows new ways to create datasets and ways to measure safety and power. The DCA fakes several aggressive events to make sure that the benchmark dataset has a variety of strikes and pauses within it. When the REA is run, it shows how well AI models can handle certain threats. A vulnerability avoidance algorithm (VMA) finds holes in an AI model and stops them from being used. We compare the suggested plan to ADS, APDA, RMAAI, AVP, CABG, and AASF based on several factors that affect results. The suggested method both lowers the number of mistakes and raises the variety, which improves the dataset and makes sure that a full review is done. Artificial intelligence models that use the suggested method are less likely to be hacked online. First, well-known datasets for study into hostile artificial intelligence are put together and studied.},
  keywords={Power measurement;Reviews;Stability criteria;Government;Benchmark testing;Solids;Robustness;Safety;Artificial intelligence;Standards;Adversarial;AI;Benchmark;Construction;Datasets;Diversity;Evaluation;Re-search;Robustness;Vulnerability},
  doi={10.1109/ICTBIG64922.2024.10911659},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10182456,
  author={Mittal, Sachin},
  booktitle={2023 3rd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)}, 
  title={Integration of Human Knowledge with AI Through NN}, 
  year={2023},
  volume={},
  number={},
  pages={2272-2276},
  abstract={Social progress is linked to continuous effort to improve, to conquer nature, and to create anything which has so far been possible without its involvement. Ideas like "advanced infertility," "ephemeral blood," "abstract lungs," "abstract eyes with retinas," "arbitrary brains," and "arbitrary intellect" express a desire to take back control of man, a power that has hitherto been assigned to nature, Him the Almighty, destiny, or luck. The idea of ai technologies is getting more and more real due to the rapid growth of science, new technologies, and methodologies. A.i. (AI) has become more and more popular among businessmen in recent years. Its creation comprises multiple teams of elite professionals working with the most up-to-date IT equipment. The concept of "autonomous" technologies existed across many men's thinking before to the development the first ones.Transformers were really a common phenomenon in the fiction films and writings that depicted a tomorrow inside which machines supplanted people. It is neither a cyborg or a "smart" software application today for the contemporary man. We are now able to make things that were once just the subject of space opera because of the quick development of computer and electronic technology. Presently, AI is applied in a number of scientific disciplines, including leadership, commerce, and psychiatry. But it's also important to keep in mind that there are a lot of concerns and issues about the morality of these procedures. The publications' purpose is to stimulate conversation around the topic of finding solutions to new doubts using Neural link as an illustration.},
  keywords={Ethics;Humanities;Oral communication;Medical services;Writing;Retina;Software;AI;Neural link Company;IT;Economy;Software Interpretation;Ephemeral Blood;Arbitrary and Brain Function.},
  doi={10.1109/ICACITE57410.2023.10182456},
  ISSN={},
  month={May},}@INPROCEEDINGS{10956588,
  author={S, Shrivathsa V and De, Shounak and A, Jayarama and Pinto, Richard},
  booktitle={2024 Third International Conference on Artificial Intelligence, Computational Electronics and Communication System (AICECS)}, 
  title={Structural Tailoring of V2O5 Thin Films via Solvent Additives in the Precursor Solution}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Vanadium pentoxide (V2O5) thin films were synthesized using spray pyrolysis with three different solvent additives—methanol, acetone, and dimethylformamide (DMF)—in the precursor solution. The structural, molecular, and morphological properties of the films were characterized using X-ray diffraction (XRD), Fourier-transform Raman spectroscopy (FT-Raman), and scanning electron microscopy (SEM). Methanol-grown V2O5 thin films showed high crystallinity, and well-defined nanorods, while acetone and ethanol resulted in films with lower crystallinity and irregular nanostructures. DMF produced entirely amorphous films. This study provides a comprehensive understanding of how solvent additives influence V2O5 film quality, particularly for gas sensing applications.},
  keywords={Solvents;Scanning electron microscopy;Additives;X-ray scattering;Films;Raman scattering;Vanadium;X-ray diffraction;Sensors;Methanol;V2O5 thin films;solvent additives;spray pyrolysis;XRD;FT-Raman;SEM;crystallinity;nanostructures;gas sensing},
  doi={10.1109/AICECS63354.2024.10956588},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10760469,
  author={Ramagundam, Shashishekhar and Karne, Niharika},
  booktitle={2024 2nd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)}, 
  title={Review on Revolutionizing Viewer Experience in the Role of Generative AI in FAST Platforms}, 
  year={2024},
  volume={},
  number={},
  pages={18-24},
  abstract={Customer service is revolutionized by Generative Artificial Intelligence (AI), which improves the user experience through contextual awareness and natural conversations. The customized experience is provided by predicting the behaviour and needs of the customer via AI and predictive analysis. Semantic Communications (SC) and Generative Artificial Intelligence (GenAI) can be considered as the most significant contributions in this area. By infusing emergent qualities into machines, GenAI surpassed deterministic task-solving to produce inventive results. Customer Relationship Management (CRM) is widely used in industry and academia. It signifies an intelligent approach that is used by companies to monitor and assess their relationships with current as well as potential clients. With the help of GenAI, consumers of the Over-The-Top (OTT) content distribution approach associated with Free Ad-Supported Streaming Television (FAST) can access free linear television channels. Hence, this survey explains the exploration of revolutionizing viewer experience with GenAI using FAST platforms with a brief analysis of literature studies. Subsequently, the analysis followed by providing a chronological review of different GenAI models. It is a further exploration of dataset utilization, implementation tools, and so on. Lastly, the research challenges and gaps are provided for directing future trends.},
  keywords={Surveys;Over-the-top media services;TV;Reviews;Generative AI;Oral communication;Market research;User experience;Predictive analytics;Monitoring;Revolutionizing Viewer Experience;Free AdSupported Streaming Television;Generative Artificial Intelligence;Chronological Review;Implementation Tools;Future Trends},
  doi={10.1109/ICSSAS64001.2024.10760469},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9516870,
  author={Song, Jinbao and Deng, Junli and Xin, Xin and Wu, Xiaoguang and Liu, He},
  booktitle={2021 IEEE/ACIS 19th International Conference on Computer and Information Science (ICIS)}, 
  title={Exploration of Online and Offline Media Convergence Based on Artificial Intelligence Technology}, 
  year={2021},
  volume={},
  number={},
  pages={102-107},
  abstract={This paper analyzes the content shortage problems faced by museums, art galleries, private cinemas, various types of experience museums and other cultural complexes, and finds that it is mainly due to the high production cost caused by the existing production pattern of creating and producing cultural products entirely on its own. With the help of artificial intelligence technology, this paper puts forward a solution to realize the convergence of online and offline media, that is, using software technology such as video understanding and video enhancement and collecting a wide variety of audio and video programs on the Internet. Then we integrate, optimize and re-create them to overcome the defects of fragmentation and poor quality, so that the quality of cultural products can reach the level of high resolution, high color depth and high frame rate, and the cost is not so high. To meet the sustainable needs of new era cultural complex for high-quality cultural products in low price.},
  keywords={Production;Companies;Media;Tools;Motion pictures;Cultural differences;Artificial intelligence;Media Convergence;Video Understanding;Video Enhancement},
  doi={10.1109/ICIS51600.2021.9516870},
  ISSN={},
  month={June},}@INPROCEEDINGS{9190871,
  author={Mirzaei, Maryam Sadat and Meshgi, Kourosh and Frigo, Etienne and Nishida, Toyoaki},
  booktitle={2020 IEEE International Conference on Image Processing (ICIP)}, 
  title={Animgan: A Spatiotemporally-Conditioned Generative Adversarial Network For Character Animation}, 
  year={2020},
  volume={},
  number={},
  pages={2286-2290},
  abstract={Producing realistic character animations is one of the essential tasks in human-AI interactions. Considered as a sequence of poses of a humanoid, the task can be considered as a sequence generation problem with spatiotemporal smoothness and realism constraints. Additionally, we wish to control the behavior of AI agents by giving them what to do and, more specifically, how to do it. We proposed a spatiotemporally-conditioned GAN that generates a sequence that is similar to a given sequence in terms of semantics and spatiotemporal dynamics. Using LSTM-based generator and graph ConvNet discriminator, this system is trained end-to-end on a large gathered dataset of gestures, expressions, and actions. Experiments showed that compared to traditional conditional GAN, our method creates plausible, realistic, and semantically relevant humanoid animation sequences that match user expectations.},
  keywords={Generators;Spatiotemporal phenomena;Gallium nitride;Animation;Training;Generative adversarial networks;Semantics;Character Animation Generation;Spatiotemporal Conditioning;Generative Adversarial Networks},
  doi={10.1109/ICIP40778.2020.9190871},
  ISSN={2381-8549},
  month={Oct},}@INPROCEEDINGS{9743070,
  author={Liu, Depeng},
  booktitle={2022 Second International Conference on Artificial Intelligence and Smart Energy (ICAIS)}, 
  title={Application of Virtual Reality Technology in the Visual Optimization of Print Advertisement based on Image Information Mining System}, 
  year={2022},
  volume={},
  number={},
  pages={641-645},
  abstract={Application of virtual reality technology in the visual optimization of print advertisement based on the image information mining system is studied in the paper. Abstract thinking goes beyond the mode of directly describing concrete things. It combines concrete things into abstract things reflecting the essence of objective things by using abstract things such as symbols and language, and firmly grasps the characteristics and basic elements of things. Taking 3Dmax as an example, the 3D model or scene produced by the artist must be converted and the converted scene file must be imported into the program to see the realistic effect after rendering. To achieve the goal of efficient modelling, the image mining model is integrated. The novel algorithm is designed to enhance the feature analysis model. Through the testing on the different scenarios, the performance of the designed platform is validated.},
  keywords={Solid modeling;Visualization;Analytical models;Three-dimensional displays;Virtual reality;Rendering (computer graphics);Artificial intelligence;Image Information;Virtual Reality;Visual Optimization;Print Advertisement;Data Mining},
  doi={10.1109/ICAIS53314.2022.9743070},
  ISSN={},
  month={Feb},}@ARTICLE{10621009,
  author={Duan, Tao and Dong, Wei and Wang, Fei and Yao, Yuxia},
  journal={IEEE Access}, 
  title={Computational Algorithms and Aesthetic Impact in Animated Visual Effects}, 
  year={2024},
  volume={12},
  number={},
  pages={188890-188901},
  abstract={Computational algorithms play a crucial role in shaping the aesthetic impact of animated visual effects (VFX). This abstract explores the intersection of technical algorithms and artistic expression in the realm of VFX, focusing on their collaborative role in modern animation. The integration of computational algorithms enhances the realism and creativity of animated VFX. Advanced rendering techniques such as ray tracing and global illumination algorithms simulate light behavior with high fidelity, achieving photorealistic effects that captivate audiences. Procedural algorithms generate complex geometries and textures efficiently, facilitating the creation of intricate and visually appealing animations. Beyond technical fidelity, algorithms also enable novel artistic expressions. Generative algorithms, for instance, allow animators to explore unconventional designs and styles, pushing the boundaries of visual storytelling. Algorithms in simulation contribute to the lifelike movement of characters and elements, enhancing emotional engagement and narrative impact. The synergy between computational algorithms and artistic vision in animated VFX underscores their transformative impact on contemporary animation. This abstract highlights the essential role of algorithms in fostering innovation and creativity, advancing the field of animated visual effects into new realms of aesthetic possibility.},
  keywords={Visualization;Heuristic algorithms;Visual effects;Computational modeling;Solid modeling;Rendering (computer graphics);Accuracy;Animation;Visual effects;Art;Computational algorithms;animated visual effects;aesthetic impact;procedural techniques;generative art},
  doi={10.1109/ACCESS.2024.3437254},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10882293,
  author={Alijoyo, Franciskus Antonius and Reddy, L. Chandra Sekhar and Selvi, V. and Murugan, Rekha and Kakad, Shital and Balakumar, A.},
  booktitle={2024 International Conference on Artificial Intelligence and Quantum Computation-Based Sensor Application (ICAIQSA)}, 
  title={Transforming Business with Generative AI Models Applications Trends}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Generative artificial intelligence (AI) has unexpectedly emerged as a transformative generation, with the functionality to revolutionize numerous business sectors by way of developing new and realistic content material from present records. Unlike traditional AI fashions that cognizance in the main on class and reputation obligations, generative AI excels at producing pics, text, tune, and films, which drives innovation and operational performance across a couple of industries. This examine explores the impact of generative AI, evaluating its packages, overall performance metrics, and emerging traits using a complete dataset from Kaggle that includes numerous statistics kinds such as pics, textual content, track, and video. The research technique includes a radical manner of information collection, rigorous preprocessing, and distinct function extraction and engineering. Data preprocessing is important, related to the removal of replica records, addressing missing values through imputation techniques, and standardizing records codecs to make certain consistency and reliability. Feature extraction focuses on figuring out key attributes associated with generative AI applications and their effect on commercial enterprise performance. Feature engineering extends this through growing and selecting new metrics to seize the nuanced elements of AI improvements. Classification is completed the usage of both supervised and unsupervised learning techniques to successfully categorize and examine the effect of generative AI on exclusive commercial enterprise procedures. The proposed method achieves an outstanding accuracy of 98%, demonstrating its effectiveness and reliability. Implemented in Python, this approach indicates tremendous ability for boosting innovation and operational efficiency.},
  keywords={Measurement;Industries;Technological innovation;Quantum computing;Generative AI;Feature extraction;Market research;Reliability;Unsupervised learning;Business;Generative AI;Performance Metrics;Data Preprocessing;Feature Extraction;Python Implementation},
  doi={10.1109/ICAIQSA64000.2024.10882293},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10421049,
  author={Murugan, K. and Mothi, R. and George, Retty and Priya, R. Esther and Nagarathinam, S. and Krishnakumar, S.},
  booktitle={2023 International Conference on Communication, Security and Artificial Intelligence (ICCSAI)}, 
  title={Analysis of Visual Expression of Light and Color in Movies based on Wavelet Neural Network}, 
  year={2023},
  volume={},
  number={},
  pages={896-899},
  abstract={Movies are in the era of “black, white and gray” ruling the screen, people have a strong desire for movies to truly reproduce colors, because the main rhythm of the “black, white and gray” film visual music is not clear enough, the momentum is not strong enough, the meaning is not rich enough, and the color is not enough. Visual aesthetics cannot be attractive… Aiming at the dual challenges of colorization quality and timing stability in film colorization, a generative adversarial network with a recurrent structure is proposed, which can be used for automatic colorization of films without any reference frames and artificial intervention. The network generates a confrontation network based on classical conditions: the generator is used to generate color images and complete the task of colorization; The discriminator is used to identify the authenticity and improve the performance of the generator. Cycle structure and timing consistency loss are introduced to integrate timing information and solve the stability problem of coloring. Experimental results show that this method can effectively reduce flicker in the generated movie sequence while ensuring the coloring of a single frame image. This paper studies and analyzes the visual expression of light and color in movies based on wavelet neural network.},
  keywords={Visualization;Image color analysis;Motion pictures;Stability analysis;Mathematical models;Generators;Timing;Neural network;film;Color vision},
  doi={10.1109/ICCSAI59793.2023.10421049},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10421514,
  author={Al-Attabi, Kassem and Aluvala, Srinivas and Manjula, B.M and Mohammed, Ibraheem Hatem and Bodduppalli, Santhosh},
  booktitle={2023 International Conference on Integrated Intelligence and Communication Systems (ICIICS)}, 
  title={An Improved Whale Optimization Algorithm Based Secure and Energy-Aware Clustering in Vehicular Ad hoc Network}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={The Vehicular Ad hoc Network (VANET) is a mobile network which enables a numerous intelligent transportation system. The secure routing is significant to prevent mobile devices from other threats so, the effective characteristics in VANET are utilized to attain an efficient secure routing path. The high vehicle distribution and node mobility compromises the network scalability and topology. So, creating the reliable and scalable vehicle communication, network physical layout formation, unstable link to enable robust are difficult task in traffic network. In this research, novel optimization algorithm is considered with transmission range, speed direction, grid size and node density during the clustering. The Improved Whale optimization Algorithm for clustering in VANET (IWOA) is proposed for selecting an optimal Cluster Head (CH). Primarily, the optimal CH are selected by using IWOA and then the route path is selected. The simulations are performed and then the experiments are conducted on the IWOA. The performance of IWOA is evaluated using throughput, Packet Delivery Ratio (PDR) and latency by various rounds of 30, 40, 50, 60 and 70. The IWOA attained high throughput of 6. 87mbps and PDR of 0.93 with less latency of 0.10s which is superior than other existing algorithms.},
  keywords={Clustering algorithms;Vehicular ad hoc networks;Throughput;Routing;Whale optimization algorithms;Topology;Optimization;Cluster Head;Secure Routing Path;Transmission Range;Vehicular Ad hoc Network;Whale Optimization Algorithm},
  doi={10.1109/ICIICS59993.2023.10421514},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10923796,
  author={Rodrigues, Vanessa and Silva, Frutuoso},
  booktitle={2024 International Conference on Graphics and Interaction (ICGI)}, 
  title={Learning Through the Dialogue with NPCs Using Generative AI}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={The rapid evolution of generative artificial intelligence (GenAl) is revolutionising various areas, including education and gaming industry. GenAl can create original content to enhance traditional teaching methods, making learning more interactive and personalised. These tools can significantly improve educational outcomes by providing personalised feedback to students and increasing their engagement and motivation. However, the integration of GenAl in education raises ethical concerns, particularly regarding privacy, bias, and the accuracy of AI-generated content, as well as the authenticity and authorship of the work. There is a strong emphasis on the need for robust guidelines and human oversight to mitigate these issues. We used GenAl to create an NPC with a unique personality and life background and enable learners to interact with the NPC without scripted dialogue, creating an engaging game-based learning environment to evaluate the perceptions of the students using GenAl as a learning tool. The prototype developed was evaluated by a group of sixteen students, and the main results are presented and discussed.},
  keywords={Industries;Privacy;Accuracy;Philosophical considerations;Generative AI;Education;Prototypes;Oral communication;Games;Learning (artificial intelligence);Generative Artificial Intelligence;NPC dialogue;Education;Inworld AI;Discovery of Madeira Islands},
  doi={10.1109/ICGI64003.2024.10923796},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10707809,
  author={Wang, Haohong and Smith, Daniel and Kudelska, Malgorzata},
  booktitle={2024 IEEE 7th International Conference on Multimedia Information Processing and Retrieval (MIPR)}, 
  title={10x Future of Filmmaking Empowered by AIGC}, 
  year={2024},
  volume={},
  number={},
  pages={68-74},
  abstract={In this position paper, we present a vision for the future of filmmaking, driven by the emergence of generative AI technology. While the AI workflow for filmmaking remains in its infancy, recent advancements in diffusion models and the emergence of AI tools such as Midjourney, Runway, Pika, SORA, and LUMA are profoundly inspiring. The integration of AI into filmmaking endeavors, exemplified by projects like Our T2 Remake and Next Stop Paris, has yielded unprecedented impacts. This paper meticulously examines the challenges currently confronting AI models and proposes temporary solutions to surmount these obstacles in the filmmaking process. Furthermore, it demonstrates the workflow of the film “Next Stop Paris,” illustrating how these integrated AI modules can collaborate efficiently to produce short films despite technical limitations in the early days. We foresee a future akin to Silicon Valley's technology incubation, where intellectual property (IP) incubation thrives in Hollywood. This initiative supports our vision of catalyzing 10x growth in the filmmaking industry.},
  keywords={Productivity;TV;Generative AI;Pipelines;Entertainment industry;Intellectual property;Information processing;Motion pictures;Artificial intelligence;Standards;AI;generative AI;AIGC;diffusion model;filmmaking;Next Stop Paris},
  doi={10.1109/MIPR62202.2024.00018},
  ISSN={2770-4319},
  month={Aug},}@INPROCEEDINGS{10929907,
  author={Lee, Jungmin and Noh, Haeun and Lee, Jaeyoon and Choi, Jongwon},
  booktitle={2025 IEEE International Conference on Consumer Electronics (ICCE)}, 
  title={SCENEDERELLA: Text-Driven 3D Scene Generation for Realistic Artboard}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={As global media and Video-on-Demand (VOD) markets expand, efficiently producing high-quality 3D scenes has become a key challenge. Our study named SCENEDERELLA generates 3D scenes useful for the film industry using simple text prompts. Since traditional diffusion models create fantastic images, they have limitations in generating scenes truly desired by real-world users. However, fine-tuning the clip encoder of the diffusion model enables the production of realistic 2D scenes for users. Once the initial 2D scenes align with the user's conceptual expectations, we construct 3D scenes using a depth estimator and 3D Gaussian Splatting. The study successfully demonstrates the generation of 3D movie scenes based on various movie scripts. Our method overcomes the limitations of traditional physical set production and offers a new approach that enables the rapid creation of diverse scenes. Additionally, it can be used as a pre-visualization video to attract investment in movies and dramas. Our approach is expected to introduce a new paradigm in content production, with film, gaming, and advertising. Additional results and interactive demos are available at our project website.},
  keywords={Visualization;Technological innovation;Three-dimensional displays;Costs;Entertainment industry;Production;Motion pictures;Diffusion models;Advertising;Investment;3D Scene Generation;Text-to-3D;Film Pre-visualization},
  doi={10.1109/ICCE63647.2025.10929907},
  ISSN={2158-4001},
  month={Jan},}@INPROCEEDINGS{9259872,
  author={Çakar, Mahmut and Yıldız, Kazım and Demir, Önder},
  booktitle={2020 Innovations in Intelligent Systems and Applications Conference (ASYU)}, 
  title={Creating Cover Photos (Thumbnail) for Movies and TV Series with Convolutional Neural Network}, 
  year={2020},
  volume={},
  number={},
  pages={1-5},
  abstract={The latest version and the use of video streaming platforms is increasing day by day, adding new ones. Competition is also increasing in the development of platforms for expanding film and TV series. The purpose of replicating these platforms is to achieve more, better quality on some platforms, and to keep them on one platform even better. Before that, film and TV series platforms use artificial intelligence algorithms. In this study, the aim is to create more attractive cover photos for the user by finding suitable frames from a movie or TV series and converting these frames into cover/thumbnail images on the platform. It is based on eliminating frames that are useless according to closed eyes, blurred frames or non-face images. Also, deep learning used for labeling images with objects and face's emotion and identity.},
  keywords={Faces;Motion pictures;Streaming media;Convolutional neural networks;Image color analysis;Visualization;TV;Deep Learning;Artificial Intelligence;Video Streaming Platforms;Convolutional Neural Network},
  doi={10.1109/ASYU50717.2020.9259872},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10859907,
  author={Javed, Danish and Arshad, Usama and Peerzada, Shuhrabeel and Saud, Muhammad Ramiz and Ali, Nisar and Ali, Raja Hashim},
  booktitle={2024 International Conference on IT and Industrial Technologies (ICIT)}, 
  title={VigilantAI: Real-time detection of anomalous activity from a video stream using deep learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={In an era where artificial intelligence (AI) solutions are increasingly integrated into various sectors, we have utilized Artificial Intelligence for enhancing public safety through real-time detection of illegal activities such as robberies and threats at gunpoint using CCTV footage. With the proliferation of deep learning in object detection, the study focuses on deploying the YoloV5 model, trained on a custom dataset compiled from diverse CCTV sources and movies, to identify specific criminal actions. One of the major problems faced in this field is the availability of a large robust labeled dataset on which a deep learning model can be trained. For this purpose, we have created our own dataset by converting various CCTV footage and movies into images, and then labeling them with the correct class. In addition, we also augmented data by using various data augmentation techniques for the chosen images. This dataset, enriched through augmentation techniques and annotated with bounding boxes, allows for the precise detection of threats, achieving an accuracy rate of 85%. Our system stands out by not only spotting these activities but also by instantly alerting security personnel, facilitating a rapid response to potentially dangerous situations. This capability is important for law enforcement agencies worldwide, offering them an advanced tool to act swiftly and prevent crimes, thereby enhancing public security. The essence of our work demonstrates the practical application and significant impact of AI in bolstering security measures, providing a solid foundation for future enhancements in the field. Through this initiative, we aim to foster a safer environment in public spaces, reducing crime rates and increasing the general public’s sense of safety.},
  keywords={Deep learning;YOLO;Accuracy;Law enforcement;Streaming media;Motion pictures;Real-time systems;Public security;Security;Artificial intelligence;Anomalies detection;Yolo V5;real-time video streams;Law enforcement;Public Safety;Deep learning},
  doi={10.1109/ICIT63607.2024.10859907},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10579034,
  author={Li, Jiameng and Chen, Zhen and Lin, Weiran and Zou, Liangjun and Xie, Xin and Hu, Yaodong and Li, Dianmo},
  booktitle={2024 IEEE World AI IoT Congress (AIIoT)}, 
  title={Mystery Game Script Compose Based on a Large Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={451-455},
  abstract={In this paper, we present a method based on large language models for murder mystery game scriptwriting, which not only helps ordinary user’s one-click generation of ideas into a complete work, but also helps professional scriptwriters to improve the efficiency of creation. The main idea of this paper is to develop prompt engineering to communicate with the large language model, so that the model can accurately generate the required content according to the user’s thread, and improve the quality of the output scripts. Through the effective combination of AI and human thinkings, the paper explores the potential application prospects of AI in murder mystery game script generation and expression with a new perspective and means.},
  keywords={Large language models;Games;Prompt engineering;Large Language Model;Murder Mystery Creation;Prompt Engineering},
  doi={10.1109/AIIoT61789.2024.10579034},
  ISSN={},
  month={May},}@INPROCEEDINGS{9188324,
  author={Doherty, Mike and Esmaeili, Behzad},
  booktitle={2020 IEEE IAS Electrical Safety Workshop (ESW)}, 
  title={Application of Artificial Intelligence in Electrical Safety}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={As artificial intelligence (AI) becomes more sophisticated in imitating human cognitive processes (e.g., problem solving, object detection, and learning), it has transformed several industry sectors and has had a growing impact on the way construction projects are delivered. One of the areas that has great potential to advance breakthroughs for innovative improvements is in electrical safety. Data generated from images captured from mobile devices, unmanned aerial vehicles, wearable sensors, building information modeling (BIM), and others present an opportunity for construction safety professionals and in many other sectors to analyze and benefit from the insights generated from the data using AI, machine learning and deep learning systems. For example, AI-based algorithms can be used to scan images from jobsites for safety hazards, such as workers not wearing protective equipment, and correlate the images with accident records; identify unsafe worker behavior and suggest training and education priorities; or track the real-time interactions of workers, machinery, and objects on the site and alert supervisors of potential safety issues. Using AI to execute the existing Shock and Arc Flash Risk Assessment Procedure templates from CSA Z462 and NFPA 70E as real-world scenarios and the potential ethical factors from the workers perspectives will also be discussed. Considering AI's immense potential, this study aims to synthesize emerging trends of artificial intelligence in electrical safety.},
  keywords={Natural language processing;Accidents;Safety;Machine learning algorithms;Computer vision;Machine learning;Artificial Intelligence (AI);Machine Learning;Data Mining},
  doi={10.1109/ESW42757.2020.9188324},
  ISSN={2326-330X},
  month={March},}@INPROCEEDINGS{10727104,
  author={Ying, Chang Wen},
  booktitle={2024 IEEE 24th International Conference on Software Quality, Reliability, and Security Companion (QRS-C)}, 
  title={Preliminary Exploration of Intelligent Virtual Avatars in the Virtual Influencer Industry}, 
  year={2024},
  volume={},
  number={},
  pages={1284-1291},
  abstract={The virtual avatar and Vtuber (virtual YouTube broadcaster) industries are fast-evolving sectors within today's entertainment and social media landscape. These virtual avatars are generated by software and enhanced with animation and motion capture technologies to provide interactive experiences for users. The market for virtual avatars encompasses content production and technological development, representing a significant spectrum of the digital content industry today. In the era of AI-generated content (AIGC), intelligent virtual avatars are becoming a pivotal technology in the virtual influencer domain. This study aims to preliminarily explore the applications of intelligent virtual avatars in the virtual influencer industry, particularly focusing on the positive and negative impacts after integrating artificial intelligence functionalities. Questions include whether the introduction of intelligent virtual avatars significantly enhances content creation efficiency and diversity, or whether, through AI technologies, virtual influencers can achieve more real-time interactions and more closely mimic human response patterns, thus enhancing audience immersion and loyalty. The introduction of AI technology is notably evident in real-time interactions with audiences. Moreover, intelligent virtual avatars can produce content continuously, unrestricted by physical or emotional states, offering a production advantage. However, the use of related technologies, including speech synthesis, also initiates a series of negative impacts, posing a threat to the job security of human creators. As virtual avatars can produce content more cost-effectively and efficiently, human creators might find their skills and experiences less rare, potentially affecting their career prospects and earnings. Additionally, intelligent virtual avatars may foster the proliferation of lower-quality content, as they can quickly produce large amounts of material without deep creativity or cultural sensitivity, potentially leading to content homogenization, reducing cultural diversity and the depth of creativity. This study employs a qualitative approach, interviewing two well-known companies in Taiwan that utilize AIGC technology for virtual influencers or virtual avatars. The questions cover three aspects: 1. Whether AI applications in virtual avatars can enhance the industry's business models, 2. Whether AI can accelerate the existing digital content production process, 3. Whether the inclusion of AI reduces company manpower. The introduction of intelligent virtual avatars has a significant impact on the virtual influencer industry, bringing both positive effects in promoting industry development and innovation, and potential negative impacts on the creative labor market. This study attempts to find a balance between these opportunities and challenges needed for industry development.},
  keywords={Technological innovation;Social networking (online);Law;Avatars;Entertainment industry;Companies;Production;Security;Artificial intelligence;Business;Artificial Intelligence;AIGC;Intelligent Virtual Avatars},
  doi={10.1109/QRS-C63300.2024.00167},
  ISSN={2693-9371},
  month={July},}@INPROCEEDINGS{10762856,
  author={Shimichev, Alexey S. and Rotanova, Mira B.},
  booktitle={2024 International Conference "Quality Management, Transport and Information Security, Information Technologies" (QM&TIS&IT)}, 
  title={Digital Linguodidactic Resource for Teachers Based on Neural Networks}, 
  year={2024},
  volume={},
  number={},
  pages={182-185},
  abstract={In the context of the digital transformation of education, teachers face to the tasks of adaption to modern technological requirements and expansion their own teaching tools by using digital instruments during the lesson. The article describes the experience of creating a digital methodological resource for a foreign language teacher, presents its structure and algorithms of work. The authors describe the technological features of the online lesson constructor. Artificial intelligence technologies are integrated into the resource and allow teachers to automate the performance of a number of pedagogical tasks, thereby saving the time. Such tasks include the generation of a lesson plan, elements of educational content, and automated assessment of academic results. The authors analyze the advantages and limits of using neural network tools in the design and implementation of a foreign language lesson. It is concluded that the created resource allows solving existing problems, because It provides a quick search, collection and generation of digital materials for the purposes of educational activities, as well as the construction of lessons and their fragments.},
  keywords={Instruments;Digital transformation;Neural networks;Education;Organizations;Search problems;Filling;Artificial intelligence;Faces;artificial intelligence;methodological resource;online lesson builder;neural networks;foreign language teaching},
  doi={10.1109/QMTISIT63393.2024.10762856},
  ISSN={},
  month={Sep.},}@ARTICLE{10477553,
  author={Chong, Xiaoya and Leung, Howard and Li, Qing and Yao, Jianhua and Zhou, Niyun},
  journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics}, 
  title={Deep Spatio-Temporal Network for Low-SNR Cryo-EM Movie Frame Enhancement}, 
  year={2024},
  volume={21},
  number={5},
  pages={1299-1310},
  abstract={Cryo-EM in single particle analysis is known to have low SNR and requires to utilize several frames of the same particle sample to restore one high-quality image for visualizing that particle. However, the low SNR of cryo-EM movie and motion caused by beam striking make the task very challenging. Video enhancement algorithms in computer vision shed new light on tackling such tasks by utilizing deep neural networks. However, they are designed for natural images with high SNR. Meanwhile, the lack of ground truth in cryo-EM movie seems to be one major limiting factor of the progress. Hence, we present a synthetic cryo-EM movie generation pipeline, which can produce realistic diverse cryo-EM movie datasets with low-SNR movie frames and multiple ground truth values. Then we propose a deep spatio-temporal network (DST-Net) for cryo-EM movie frame enhancement trained on our synthetic data. Spatial and temporal features are first extracted from each frame. Spatio-temporal fusion and high-resolution re-constructor are designed to obtain the enhanced output. For evaluation, we train our model on seven synthetic cryo-EM movie datasets and infer on real cryo-EM data. The experimental results show that DST-Net can achieve better enhancement performance both quantitatively and qualitatively compared with others.},
  keywords={Motion pictures;Signal to noise ratio;Feature extraction;Task analysis;Pipelines;Three-dimensional displays;Photomicrography;Cryo-EM movie generation;deep learning;low SNR;movie frame enhancement},
  doi={10.1109/TCBB.2024.3380410},
  ISSN={1557-9964},
  month={Sep.},}@INPROCEEDINGS{10725543,
  author={Madhumathi, R and Chandra Kumar, N and Vigneshwaran, S and Vignesh, P and Anbumani, P},
  booktitle={2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={AI based Frame Interpolation Using OneVPL}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={The increasing popularity of multimedia content, particularly videos, has highlighted the significance of compression methods like HEVC and H.264, which effectively reduce file sizes without compromising quality. However, in the encoding and decoding process, hardware malfunctions or other issues may lead to frame loss, affecting frame rates and overall quality. To mitigate this problem, interpolation techniques are utilized to insert frames and maintain smooth playback. This method employs Artificial Intelligence (AI), specifically a trained Frame Interpolation for Large Motion (FILM) model, to generate intermediate frames by analyzing pairs of input images. This technique ensures seamless motion and enhances video quality despite frame loss during encoding and decoding. This work develops a pipeline using the FILM model and hardware acceleration to generate smooth videos. Testing demonstrates a 4x improvement in visual quality over raw input, setting a new standard for video interpolation and processing in multimedia applications.},
  keywords={Interpolation;Visualization;Computational modeling;Streaming media;Encoding;Decoding;Artificial intelligence;Standards;Videos;Testing;Video Frame Interpolation;Artificial Intelligence;FILM Model;OneVPL;Frame Rate Enhancement;FFMPEG Integration},
  doi={10.1109/ICCCNT61001.2024.10725543},
  ISSN={2473-7674},
  month={June},}@INPROCEEDINGS{10962821,
  author={Saravanan, T. and Preetham, Devisetty Shashi and Venkatasubramanian, A},
  booktitle={2025 AI-Driven Smart Healthcare for Society 5.0}, 
  title={Optimized Adaptive Routing and Link Health Management in Mobile Ad Hoc Networks using Enhanced Meta Ant Advanced Beaconing with Integrated Auto-Repair and Security Protocols}, 
  year={2025},
  volume={},
  number={},
  pages={278-282},
  abstract={This paper proposes a new routing protocol for MANET that improves flexible hybrid routing utilizing improved beaconing solutions. Called the Meta Ant Advanced Beaconing (MAAB) protocol, this framework combines Ant Colony Optimization (ACO) and bacterial movement and allows for routes to be created before the nodes physically move to the positions that would make them optimal based on mobility, link quality, or security. To deal with the beacon communication, route health, and node security, MAAB explains Zonal Ant Commander Nodes (ZACNs) and Central Ant Queen Nodes (CAQNs) . Every node is armed with its own black-box signal, in case the system is captured or destroyed. As illustrated in simulations within this paper, the proposed approach improves stability, route quality, and security where there are complex nodes dynamics – characteristics of MANETs with resilience and self-organization.},
  keywords={Adaptation models;Adaptive systems;Microorganisms;Machine learning algorithms;Routing;Routing protocols;Real-time systems;Security;Thermal stability;Mobile ad hoc networks;Meta Ant Advanced Beaconing (MAAB);Adaptive Hybrid Routing;Node Mobility Awareness;Bio-inspired Algorithms;Mobile Ad Hoc Networks (MANETs)},
  doi={10.1109/IEEECONF64992.2025.10962821},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10204746,
  author={Xing, Jinbo and Xia, Menghan and Zhang, Yuechen and Cun, Xiaodong and Wang, Jue and Wong, Tien-Tsin},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior}, 
  year={2023},
  volume={},
  number={},
  pages={12780-12790},
  abstract={Speech-driven 3D facial animation has been widely studied, yet there is still a gap to achieving realism and vividness due to the highly ill-posed nature and scarcity of audio-visual data. Existing works typically formulate the cross-modal mapping into a regression task, which suffers from the regression-to-mean problem leading to over-smoothed facial motions. In this paper, we propose to cast speech-driven facial animation as a code query task in a finite proxy space of the learned codebook, which effectively promotes the vividness of the generated motions by reducing the cross-modal mapping uncertainty. The codebook is learned by self-reconstruction over real facial motions and thus embedded with realistic facial motion priors. Over the discrete motion space, a temporal autoregressive model is employed to sequentially synthesize facial motions from the input speech signal, which guarantees lip-sync as well as plausible facial expressions. We demonstrate that our approach outperforms current state-of-the-art methods both qualitatively and quantitatively. Also, a user study further justifies our superiority in perceptual quality. Code and video demo are available at https://doubiiu.github.io/projects/codetalker.},
  keywords={Computer vision;Three-dimensional displays;Codes;Uncertainty;Pattern recognition;Facial animation;Task analysis;Humans: Face;body;pose;gesture;movement},
  doi={10.1109/CVPR52729.2023.01229},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10968185,
  author={Nguyen, Luong Vuong},
  booktitle={2025 International Conference on Machine Learning and Autonomous Systems (ICMLAS)}, 
  title={Graph Augmentation-based Large Language Models for Movie Recommendations}, 
  year={2025},
  volume={},
  number={},
  pages={1480-1484},
  abstract={Personalized movie recommendations are essential for enhancing user engagement and satisfaction in streaming platforms, yet traditional methods like collaborative filtering often struggle with capturing complex user-item interactions. This study introduces a novel framework integrating graph augmentation techniques with Large Language Models (LLMs) to improve recommendation accuracy and relevance. The framework generates graph-enhanced embeddings by constructing a user-item graph enriched with metadata such as user preferences, movie genres, cast details, and interaction histories and applying graph augmentation techniques like node feature enrichment and edge transformations. These embeddings are integrated into pretrained LLMs, such as GPT or BERT, which are fine-tuned to provide contextualized recommendations. The method optimizes for multiple objectives, including accuracy, diversity, and novelty, ensuring a balanced and personalized experience. Experiments on datasets like MovieLens and Netftix Prize demonstrate that the proposed approach outperforms traditional methods and standalone LLMs, significantly improving precision, recall, and ranking metrics. Despite challenges related to scalability and dependency on high-quality graph data, this framework highlights the potential of combining graph augmentation and LLMs to advance personalized recommendation systems. This research offers a transformative pathway for enhancing the quality and context-awareness of recommendations in the entertainment domain.},
  keywords={Measurement;Accuracy;Large language models;Scalability;Diversity reception;Machine learning;Metadata;Motion pictures;History;Recommender systems;Movie Recommendations;Large Language Models;Graph Augmentation;Personalized Recommender Systems},
  doi={10.1109/ICMLAS64557.2025.10968185},
  ISSN={},
  month={March},}@INPROCEEDINGS{10663046,
  author={Johri, Aditya and Hingle, Ashish},
  booktitle={2024 36th International Conference on Software Engineering Education and Training (CSEE&T)}, 
  title={Case Study Based Pedagogical Intervention for Teaching Software Engineering Ethics}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={The omnipresence of software systems across all aspects of society has necessitated that future technology professionals are aware of ethical concerns raised by the design and development of software and are trained to minimize harm by undertaking responsible engineering. This need has become even more urgent with artificial intelligence (AI) driven software deployment. In this paper we present a study of an interactive pedagogical intervention –role-play case studies –designed to teach undergraduate technology students about ethics with a focus on software systems. Drawing on the situated learning perspective from the Learning Sciences, we created case studies, associated stakeholder roles, discussion scripts, and pre and post discussion assignments to guide students’ learning. Open-ended data was collected from thirty-nine students and analyzed qualitatively. Findings from the study show that by taking on different perspectives on a problem, students were able to identify a range of ethical issues and understand the role of the software system process holistically, taking context, complexity, and trade-offs into account. In their discussion and reflections, students deliberated the role of software in society and the role of humans in automation. The curricula, including case studies, are publicly available for implementation.},
  keywords={Ethics;Automation;Open Access;Education;Software systems;Reflection;Complexity theory;software engineering ethics;ethics education;case studies;role plays;situated learning},
  doi={10.1109/CSEET62301.2024.10663046},
  ISSN={2377-570X},
  month={July},}@INPROCEEDINGS{10407412,
  author={R, Anitha and N, Kishore and Vijay Anand, M.},
  booktitle={2023 9th International Conference on Smart Structures and Systems (ICSSS)}, 
  title={NextGen Dynamic Video Generator using AI}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={In an era marked by remarkable technological advancements, the way we create and share information has undergone a profound transformation. This paradigm shift is epitomized by the NextGen Dynamic Video Generator using AI, a cutting-edge tool that seamlessly integrates artificial intelligence with content creation. What sets this tool apart is the customizability, which allows the users to fine-tune every aspect of the tutorials. Users could tune creative elements like humor, the depth of explanation, character appearance, and voice to tailor tutorials to precise specifications. The core functionality of the tool revolves around script generation, the Cohere's language, laying the groundwork for the tutorial's content. Furthermore, seamless integration with Edge TTS ensures that the generated scripts are delivered with utmost clarity and engagement, enhancing the overall learning experience. Character animation is powered by SadTalker, adding a dynamic and captivating dimension to these tutorials. This animated character serves as a relatable guide, facilitating a deeper connection between the content and the audience. The tool also seamlessly integrates relevant and eye-catching images from Google, which are incorporated into the presentation slides. The workflow is a well-orchestrated process involving script generation, audio dialogue creation, image retrieval, video generation, and the seamless fusion of character animations and slides. The resulting video tutorials are not only comprehensive but also engaging and ready to be shared as valuable educational resources.},
  keywords={Productivity;Noise reduction;Tutorials;Linguistics;Animation;Generators;Artificial intelligence;Cohere language model;Edge TTS;SadTalker;script generation;audio dialogue creation;humor;image retrieval;next-gen;character animation;google;slides},
  doi={10.1109/ICSSS58085.2023.10407412},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10837427,
  author={Ramagundam, Shashishekhar and Karne, Niharika},
  booktitle={2024 IEEE International Conference on Blockchain and Distributed Systems Security (ICBDS)}, 
  title={Decoding Sentiments: An Efficient Trans-Long Short Term Memory to Understand Viewer Reactions on Ad-Supported Video Contents}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Viewer reaction analysis in ad-supported video content focuses on the use of advanced techniques to understand and interpret the emotional responses expressed by viewers during ad viewing experiences. This analysis goes beyond simple positive or negative sentiment and delves into the nuances of viewer reactions, allowing for a more comprehensive understanding of the emotional impact of ads. With this information, advertisers can make data-driven decisions to optimize their advertising strategies, create more engaging content, and ultimately enhance the viewer experience. One of the challenges in sentiment analysis for viewer reaction analysis is the inherent subjectivity and variability of human emotions. Emotions can be complex and subjective, making it difficult to accurately capture and interpret them solely based on text or viewer feedback. Different individuals may perceive and express emotions differently, adding another layer of complexity to the analysis process. Addressing these challenges requires a combination of advanced deep learning model refinement. Overcoming these challenges can provide valuable insights into viewer reactions, enabling advertisers and content creators to create more engaging and impactful ad-supported video content. In this paper, an Artificial Intelligence (AI)-based sentiment analysis is introduced to analyze the viewer's reaction while watching the ad in the video content. Here, an efficient model named Trans-Long Short-Term Memory (Trans-LSTM) is developed to perform the sentiment analysis. The developed Trans-LSTM is helpful in supervising the emotional data in the online video content and identifying the popularity of the corresponding video. The suggested model effectively analyzes the reviewer’s actions while watching all types of video content. Finally, the experimental analysis is performed to find the effectiveness of the developed sentiment analysis model via various metrics.},
  keywords={Measurement;Deep learning;Analytical models;Sentiment analysis;Logistic regression;Memory management;Emotional responses;Decoding;Security;Artificial intelligence;Decoding Sentiments;Understanding Viewer Reactions on Ad-Supported Video Content;Artificial Intelligence;Trans-Long Short Term Memory},
  doi={10.1109/ICBDS61829.2024.10837427},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10083961,
  author={Varshita Reddy, Penaka Sai and Kalki, Tripuram Pavani and Roshini, P and Navaneethan, S.},
  booktitle={2023 International Conference on Artificial Intelligence and Knowledge Discovery in Concurrent Engineering (ICECONF)}, 
  title={Varoka-Chatbot: An Artificial Intelligence Based Desktop Partner}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Today, more than ever, our lives have gotten faster and more frantic. Currently, artificial intelligence (AI) is improving our quality of life. We have begun to communicate, interact, and learn online. Intelligent Personal Assistants (IPAs), which enable users to converse through Natural Language Processing (NLP), enable the ultimate luxury of having a companion who always listens for you, acts when necessary, and anticipates your every need. We shall accomplish things in the era of rapidly evolving technology which we have never dreamt of, were possible in the past. A virtual personal assistant (VPA) is what this initiative seeks to develop, with the ability to automate duties and provide services for a person, allowing you to enjoy the luxury of doing so. The purpose of our personal assistant, VAROKA, which is entirely written in Python, is to provide you control over your desktop. The built-in speakers respond verbally to the user's voice request after it has been recorded via the microphone. With the hopeful rise and advent of IPAs, this voice-controlled virtual assistant helped to surpass our expectations by offering full features of employing desktop technology on customers' voice commands.},
  keywords={Computers;Video on demand;Social networking (online);Virtual assistants;Neural networks;Natural language processing;Internet;Intelligent Personal Assistants (IPA);Natural Language Processing (NLP);Virtual Personal Assistant (VPA);Artificial Intelligence (AI);Deep Neural Networks (DNN)},
  doi={10.1109/ICECONF57129.2023.10083961},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{9626472,
  author={Cao, Y X and Zhao, C and Liu, Y N and Yang, L and Zalinge, H V and Zhao, C Z},
  booktitle={2021 International Conference on IC Design and Technology (ICICDT)}, 
  title={Bioinspired mechano artificial synapse thin-film transistor}, 
  year={2021},
  volume={},
  number={},
  pages={1-4},
  abstract={This research proposes a unique self-powered synaptic transistor structure that can be used to simulate synaptic function. The voltage is provided by the triboelectric nanogenerator without using additional voltage to generate a presynaptic peak. On this basis, the device can be sewed on the clothes to transmit a signal or alarm device.},
  keywords={Integrated circuits;Voltage;Thin film transistors;Transistors;Synapses;artificial synapse;thin-film transistor;triboelectric nanogenerator},
  doi={10.1109/ICICDT51558.2021.9626472},
  ISSN={2691-0462},
  month={Sep.},}@INPROCEEDINGS{10869186,
  author={Han, Yuang and Cao, Honglong},
  booktitle={2024 IEEE 4th International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)}, 
  title={Application of Gauss Fitting Method in Quality Assessment of Afterloader System}, 
  year={2024},
  volume={4},
  number={},
  pages={989-993},
  abstract={Spot center detection is an important detection method in optical measurement. In this paper, according to the characteristics of Gaussian fitting method, which can adjust the parameters of Gaussian function by analyzing the distribution characteristics of data points, so as to minimize the error between data points and fitting function, it is used in the quality assessment process of Afterloader System, a kind of instrument used to treat malignant tumors. By using OpenCV and Gaussian fitting method to process and analyze the film images generated during the quality assessment process, the central position of the dark spot formed by the radiation source can be detected, and then the accuracy of the radiation source in place during the treatment process of the Afterloader System can be accurately obtained.},
  keywords={Accuracy;Malignant tumors;Instruments;Fitting;Medical treatment;Inspection;Optical variables measurement;Software;Quality assessment;Information technology;Gaussian fitting method;Afterloader System;quality inspection algorithm;accuracy in place},
  doi={10.1109/ICIBA62489.2024.10869186},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10149964,
  author={Mondal, Md. Keramot Hossain and Chakraborty, Sandip and Jaiswal, Nilisha and Nishchal, Nikita and Roy, Priyanka},
  booktitle={2023 IEEE 3rd International Conference on Technology, Engineering, Management for Societal impact using Marketing, Entrepreneurship and Talent (TEMSMET)}, 
  title={Comparing the Use of Short Video Sharing Applications for Optimizing User Engagement}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Since the COVID-19 Pandemic outbreak and the accompanying lockdown, the short film business has seen an unparalleled transformation. Young people have a platform thanks to the software to express themselves through lip-syncing, dancing, humour, and singing. Users can also create movies and upload them to their social networks. Get access to a brand-new, sizzling stage where you may wow the world with your fiery ability while watching never-ending fascinating videos. Every day, people face problems while discovering new and fresh skills. On the app, anyone from anywhere in the world can meet intriguing individuals and enjoy amusing short movies. One of the technologies that has fundamentally altered the social networking industry is artificial intelligence (AI) [1]. Artificial intelligence is the ability of computers and other machines to learn, comprehend, and make judgments similarly to humans (AI) [2]. The programme essentially relies on AI and ML technology in two ways. Both from the producer’s and consumer’s perspectives. This study examines how SpotLight continuously enhances user engagement through the usage of AI and ML.},
  keywords={Industries;Social networking (online);Pandemics;Entrepreneurship;Motion pictures;Software;Artificial intelligence;Social networking;Video Sharing Artificial;Intelligence (AI);Machine Learning (ML);Consumer behavior;Technology},
  doi={10.1109/TEMSMET56707.2023.10149964},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10882441,
  author={Shaikh, Shakila and Dani, Diksha},
  booktitle={2024 International Conference on Artificial Intelligence and Quantum Computation-Based Sensor Application (ICAIQSA)}, 
  title={Enhancing Movie Recommendation Systems through Demographic and Content-Based Filtering Using TF-IDF and Cosine Similarity}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={the need of data collection in recent years has piloted into a transformative era of information utilization. The recommendation systems represent a specialized type of information filtering mechanisms designed to improve the quality of search outcomes by suggesting items that are highly related to user queries or aligned with their historical interactions. This research uses demographic filtering for a recommending popular and well rated movies to users with similar demographic backgrounds irrespective of genre or other parameters. The methodology used in this paper computes pairwise similarity scores for all movies based on the parameter of descriptions and recommend movies to the users based on that similarity score. TF * IDF is used to calculate the relative frequency that is the importance of the words in description. It vectorizes the object to generate TDF-IDF matrix of plots of movies. Cosine similarity is applied to find similarities between each pair of movies. The above approach generates a matrix in which each column represents a movie and each column has a word from the overview vocabulary that appear in at least one document. This is practiced to decrease the value of words that occur frequently in plot description, and hence reduces their impact in calculating the final similarity score. Demographic and content-based filtering is used for recommendation in this paper, while demographic filtering provides a very fundamental method for recommendation and cannot be used practically, whereas content-based filtering is more advanced and powerful.},
  keywords={Measurement;Vocabulary;Quantum computing;Filtering;Motion pictures;User experience;Natural language processing;Reliability;Indexes;Recommender systems;Natural Language Processing (NLP);Demographic Filtering;Content Based Filtering;Cosine - Similarity;TDF-IDF vectorizer;TDF-IDF matrix},
  doi={10.1109/ICAIQSA64000.2024.10882441},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10696037,
  author={Senapati, Aayush and J, Ananya and Jha, Ananya and Mahishi, Ananya and K S, Srinivas},
  booktitle={2024 First International Conference on Pioneering Developments in Computer Science & Digital Technologies (IC2SDT)}, 
  title={Driving Innovation: Creating a Dataset for Automotive Ad Campaign Analysis}, 
  year={2024},
  volume={},
  number={},
  pages={273-278},
  abstract={Effective marketing efforts in the automotive industry are crucial for moulding consumer attitudes and driving business growth. However, the complexity and resource-intensive nature of campaign development presents significant problems to marketers. This study introduces a novel method for streamlining campaign generation by creating a comprehensive dataset of automotive ad campaigns by leveraging the capabilities of generative AI. We carefully created our dataset utilising web scraping techniques and Large Language Models (LLMs) in an iterative, pipelined procedure. Our dataset serves as a valuable resource for a variety of tasks, including ad campaign generation, since it provides insights about key elements that influence their performance. Additionally, we provide advertising metrics to understand campaign performance along with an analysis on the quality of media that can be produced by utilizing this dataset. This has potential to transform automobile marketing operations by providing marketers with a more efficient and evolved campaign development strategy.},
  keywords={Measurement;Industries;Generative AI;Large language models;Transforms;Media;Complexity theory;Iterative methods;Automotive engineering;Business;Marketing Campaigns;Automotive Marketing;Large Language Models (LLMs);Generative AI;Web Scraping;Dataset Creation},
  doi={10.1109/IC2SDT62152.2024.10696037},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10351216,
  author={Khodayer Al-Dulaimi, Aymen Mohammed and Alkhazraji, Nejood and Mlket Almutoki, Sabah Mohammed and Alkhayyat, Ahmed H. R. and Abbas, F.H.},
  booktitle={2023 6th International Conference on Engineering Technology and its Applications (IICETA)}, 
  title={Enhanced QoS Performance with Throughput Aware Effective Communication in VANETs}, 
  year={2023},
  volume={},
  number={},
  pages={368-373},
  abstract={Vehicular Ad-Hoc Network (VANET) is a highly trending and developing technology for the future world where in all the place highly intelligent communication is required. In VANETs the intelligently information is transmitted from one place to another with the base transmission model such as vehicle based and as well infrastructure-based communication. Unfortunately, wireless transmission standard uses IEEE802.11p and ground way communication model which can easily get affected by the external environment that results in the increase of packet loss and delay during communication. For that purpose, in this paper Enhanced QoS Performance with Throughput Aware Effective Communication (EQPTAE) is developed that mainly concerned with effective network model creation, collision detection and collision avoidance. Hence vehicle perform multi-hop communication in a highly frequent manner where a greater number of connection loss occur here with the help of effective congestion control communication becomes highly traffic free which leads to enhance the communication quality and throughput utility at each instant of time. In order to create this network in practical way the software called NS2 is used which is highly suitable for vehicular communication and as well the parameters which are used for the performance analysis are packet delivery ratio, end-to-end delay, routing overhead, and energy efficiency. To perform comparative analysis the earlier researches which are used in this article are ICFDB and the RSUCI. From the results it is understood that the proposed EQPTAE achieves maximum delivery ratio and efficiency and as well minimum delay and overhead when compared with the earlier methods.},
  keywords={Wireless communication;Vehicular ad hoc networks;Quality of service;Spread spectrum communication;Throughput;Routing;Energy efficiency;Vehicular Ad-Hoc Network (VANET);IEEE802.11p;Enhanced QoS Performance;Collision Detection and Collision Avoidance},
  doi={10.1109/IICETA57613.2023.10351216},
  ISSN={2831-753X},
  month={July},}@INPROCEEDINGS{9750822,
  author={Zhao, Yilin},
  booktitle={2022 3rd International Conference on Electronic Communication and Artificial Intelligence (IWECAI)}, 
  title={II-UserCF: Optimizing movie recommendation systems based on UserCF improvement algorithm}, 
  year={2022},
  volume={},
  number={},
  pages={141-144},
  abstract={With the advent of the e-commerce era and the development of computer technology. Recommendation systems are widely used in people's lives. Traditional user-based collaborative filtering recommendations often produce prediction errors because they do not take into account the popularity of the item. In order to improve this drawback, a new collaborative filtering recommendation algorithm based on a combination of traditional usercf, and item importance is proposed. User relevance is first calculated. Then a new value is obtained based on the different popularity levels of the items. The two are combined in order to obtain a new user similarity. To better predict the similarity between users and to produce more accurate recommendation results. Research shows that adding a value related to the popularity of the product can effectively improve the accuracy of the recommendation.},
  keywords={Collaborative filtering;Prediction algorithms;Motion pictures;Electronic commerce;Artificial intelligence;II-UserCF;Item Importance;Item popularity;Traditional user-based recommendation},
  doi={10.1109/IWECAI55315.2022.00036},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10331908,
  author={Jothi, J. Nirmal and Soundiraraj, N and Darney, P. Ebby and Krishnan, R. Santhana and Narayanan, K. Lakshmi and Sundararajan, S.},
  booktitle={2023 International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)}, 
  title={SENT2BAYES: A Hybrid Machine Learning Model Combining Word2vec and Multi-Nomial Naive Bayes Classifier for Movie Review Sentiment Analysis in Twitter}, 
  year={2023},
  volume={},
  number={},
  pages={333-339},
  abstract={Movie review sentiment analysis has gained significant attention due to the vast amount of user-generated content available on social media platforms like Twitter. Accurately analyzing sentiment from such data can provide valuable insights into public opinion and assist in decision-making processes. In the current work, a novel machine learning model, called Sent2Bayes, for movie review sentiment analysis on Twitter is proposed. The proposed model combines the power of Word2Vec embeddings and the Multinomial Naive Bayes classifier to enhance sentiment prediction accuracy. The Sent2Bayes model begins with the creation of Word2Vec embeddings, which capture semantic relationships between words and encode contextual information. These embeddings are then fed into the Multinomial Naive Bayes classifier, which leverages probabilistic modelling to predict sentiment labels for movie reviews. By combining the strength of Word2Vec's semantic understanding and Multi-nomial Naive Bayes' probabilistic approach, Sent2Bayes aims to improve the accuracy and robustness of sentiment analysis on Twitter data. To evaluate the performance of Sent2Bayes, this study conducts an extensive simulation experiments comparing it with existing algorithms commonly used for sentiment analysis. Further, this study employs suitable simulation metrics, including accuracy, precision, recall, and confusion matrix, to measure the model's predictive capabilities and generalization ability. The simulation results demonstrate that Sent2Bayes achieves superior sentiment analysis performance compared to SVM and RNN. The proposed model shows higher accuracy, precision, recall, and confusion matrix across various datasets and test scenarios. The improved performance of Sent2Bayes effectively captures contextual information through Word2Vec embeddings and the robust probabilistic modelling of Multinomial Naive Bayes.},
  keywords={Sentiment analysis;Analytical models;Social networking (online);Blogs;Semantics;Machine learning;Predictive models;Hybrid machine learning;Movie review;Multinomial Naive Bayes Classifier;Sentiment analysis;Simulation analysis;Twitter;Word2Vec},
  doi={10.1109/ICSSAS57918.2023.10331908},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10553755,
  author={Hamid, Oussama H.},
  booktitle={2024 IEEE Conference on Cognitive and Computational Aspects of Situation Management (CogSIMA)}, 
  title={Beyond Probabilities: Unveiling the Delicate Dance of Large Language Models (LLMs) and AI-Hallucination}, 
  year={2024},
  volume={},
  number={},
  pages={85-90},
  abstract={Large language models (LLMs), like OpenAI’s ChatGPT and Google’s Gemini, operate as probabilistic models, leveraging their ability to generalise and discern intricate patterns within data. By assigning probabilities to different tokens based on patterns learned during extensive training on large datasets, these models can generate a wide range of contextually appropriate responses, spanning from textual scripts to auditory and visual outputs (both static and moving images). However, the inherent probabilistic nature of LLMs introduces a notable challenge, leading to the phenomenon known in the field of artificial intelligence as ‘AI-hallucination,’ where the model may produce responses that sound plausible but are factually incorrect or nonsensical. Despite being perceived as a drawback, we posit in this paper that AI-hallucinations can be reframed as a distinctive feature of LLMs rather than a mere limitation. Our argument stems from the understanding that attempts to mitigate the harms caused by AI-hallucinations might inadvertently lead to increased model rigidity. This delicate balance between minimising harm and preserving the model’s flexibility is a central theme in our discussion. Furthermore, we revisit the concept of ‘context,’ contending that a complete definition goes beyond the mere description of circumstances, environment, or surrounding facts. We assert that context is enriched by a conscious embodiment, involving the choice or refusal of action (considering all associate ethical implications) among a set of available options.},
  keywords={Training;Ethics;Visualization;Humanities;Shape;Computational modeling;Probabilistic logic;Artificial intelligence (AI);AI-hallucination;consciousness;context;Gemini;generalisation-hallucination dilemma;GPT;large language models (LLMs)},
  doi={10.1109/CogSIMA61085.2024.10553755},
  ISSN={},
  month={May},}@INPROCEEDINGS{10874136,
  author={A, Govind and Anzar, Ahad and Nair, Aiswarya Anil and Syam, Rohith},
  booktitle={2024 IEEE International Conference on Future Machine Learning and Data Science (FMLDS)}, 
  title={GenAI Empowered Script to Storyboard Generator}, 
  year={2024},
  volume={},
  number={},
  pages={451-456},
  abstract={The research presents an innovative solution for automating the generation of storyboards from screenplays through the integration of advanced AI technologies. By taking a screenplay as input, the system utilizes cutting-edge neural networks to recognize characters and objects, enhancing scene comprehension. A refined Bi LSTM model is employed to extract the nuanced emotional tones embedded within in dialogues in each scene, providing valuable insights into character dynamics and narrative depth. Through the application of regular expressions, key scene attributes such as time, place, and location are extracted to establish contextual relevance. A Facebook BART-large-CNN model is then employed to generate concise summaries of each scene, enhancing comprehension efficiency. Through script summarization and tag extraction from the Movie Plot Synopses with Tags dataset, a smooth transition between scenes is enabled. These extracted features are structured into a prompt using a rule based approach, facilitating seamless integration into the subsequent creative phase. Finally, a stable diffusion model is employed to generate scene-by-scene coherent storyboard, incorporating all extracted elements to streamline the visual storytelling process where coherency is achieved with the help of cosine similarity between prompts. This comprehensive approach not only automates tedious tasks but also enhances creativity and efficiency in storyboard creation.},
  keywords={Visualization;Social networking (online);Neural networks;Coherence;Streaming media;Feature extraction;Aerodynamics;Transformers;Motion pictures;Creativity;Generative AI;storyboard generation;prompt generation;scene summarising;script summarising;Generative Coherence},
  doi={10.1109/FMLDS63805.2024.00085},
  ISSN={},
  month={Nov},}@ARTICLE{9551755,
  author={Chen, Jiali and Fan, Changjie and Zhang, Zhimeng and Li, Gongzheng and Zhao, Zeng and Deng, Zhigang and Ding, Yu},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={A Music-Driven Deep Generative Adversarial Model for Guzheng Playing Animation}, 
  year={2023},
  volume={29},
  number={2},
  pages={1400-1414},
  abstract={To date relatively few efforts have been made on the automatic generation of musical instrument playing animations. This problem is challenging due to the intrinsically complex, temporal relationship between music and human motion as well as the lacking of high quality music-playing motion datasets. In this article, we propose a fully automatic, deep learning based framework to synthesize realistic upper body animations based on novel guzheng music input. Specifically, based on a recorded audiovisual motion capture dataset, we delicately design a generative adversarial network (GAN) based approach to capture the temporal relationship between the music and the human motion data. In this process, data augmentation is employed to improve the generalization of our approach to handle a variety of guzheng music inputs. Through extensive objective and subjective experiments, we show that our method can generate visually plausible guzheng-playing animations that are well synchronized with the input guzheng music, and it can significantly outperform the state-of-the-art methods. In addition, through an ablation study, we validate the contributions of the carefully-designed modules in our framework.},
  keywords={Animation;Generative adversarial networks;Instruments;Motion segmentation;Hidden Markov models;Facial animation;Deep learning;Deep learning;generative adversarial networks;motion capture;guzheng animation;music-driven;data augmentation},
  doi={10.1109/TVCG.2021.3115902},
  ISSN={1941-0506},
  month={Feb},}@INPROCEEDINGS{10370952,
  author={Ravichandran, K. and Ilango, Senthil Kumar},
  booktitle={2023 First International Conference on Advances in Electrical, Electronics and Computational Intelligence (ICAEECI)}, 
  title={Influence of AI Powered Gaming Developers and Analyzing Player Behavior and Enhancing User Experience}, 
  year={2023},
  volume={},
  number={},
  pages={1-9},
  abstract={Artificial intelligence (AI) has revolutionized the gaming enterprise in recent years, and its destiny guidelines are promising. The integration of AI in games has led to the creation of greater realistic and immersive reviews for gamers. Using machine mastering algorithms has enabled game builders to create smart and adaptive recreation characters that can examine participant behavior, making the game more tough and attractive. Within the future, AI will play a great function in shaping the gaming industry. One direction is the use of AI-powered mine craft and chat bots, which can engage with players in actual-time and provide personalized assistance, improving customer support services. Every other route is integrating AI into virtual fact video games, enabling gamers to have an extra immersive enjoy via growing sensible environments. Moreover, AI can assist recreation developers examine participant behavior facts to enhance game play mechanics and beautify people.},
  keywords={Performance evaluation;Video games;Technological innovation;Software packages;Games;Virtual reality;User experience;Gaming;Enhance user;player Behavior;AI Powered;InsightGame},
  doi={10.1109/ICAEECI58247.2023.10370952},
  ISSN={},
  month={Oct},}@ARTICLE{10909101,
  author={Jahan, Sobhana and Saif Adib, Md. Rawnak and Huda, Syed Mahmudul and Rahman, Md. Sazzadur and Kaiser, M. Shamim and Sanwar Hosen, A. S. M. and Ghimire, Deepak and Park, Mi Jin},
  journal={IEEE Access}, 
  title={Federated Explainable AI-Based Alzheimer’s Disease Prediction With Multimodal Data}, 
  year={2025},
  volume={13},
  number={},
  pages={43435-43454},
  abstract={Alzheimer’s Disease (AD) is a progressive neurological disease that severely impairs cognitive function. Early detection is critical for effective treatment and management. Machine Learning (ML) methods are often used to ensure early detection and prediction. However, ML has various issues, including the data island problem. The fragmentation that results from the data island problem makes building reliable, effective ML models more complex, and it is particularly problematic in industries where privacy is a concern, like healthcare. Federated Learning (FL) can help tackle the data island problem by keeping sensitive patient data decentralized and enabling many institutions to work together on model training without exchanging raw data, all while maintaining privacy compliance. As Random Forest (RF) is proven to be the best-performing classifier in this research, an RF classifier is used to create FL. The model incorporates multiple data modalities, such as Magnetic Resonance Imaging (MRI) segmentation and clinical and psychological data, to capture the variety of characteristics influencing the progress of AD. Another concerning issue with ML is its uninterpretable character. We use SHapley Additive exPlanations (SHAP) Explainable Artificial Intelligence (XAI) techniques that emphasize important factors impacting model decisions in order to improve predictability and transparency. This explainability promotes confidence in AI-based diagnoses by enabling researchers and physicians to comprehend the underlying mechanisms guiding the predictions. The combination of XAI, FL, and Open Access Series of Imaging Studies (OASIS-3) Multimodal data offers an interpretable, scalable, reliable, and privacy-centered solution for multiple complex issues, such as predicting AD. This approach results in better diagnosis precision, greater security, and increased confidence in AI technologies, making it a novel methodology in medical sciences. With data privacy maintained, our method produces 98.93% accurate predictions, providing a solid detection strategy for AD. The suggested approach’s F1-score, Precision, Recall, and AUC are 98.93%, 98.94%, 98.93%, and 99.97%, respectively. This work also shows that a multimodal dataset performs better than a single modal dataset.},
  keywords={Data models;Data privacy;Diseases;Medical services;Explainable AI;Accuracy;Alzheimer's disease;Cognition;Classification tree analysis;Training;Federated learning;data security;data privacy;clinical data;psychological data;MRI segmentation data;explainable artificial intelligence;dementia;neurodegenerative disease},
  doi={10.1109/ACCESS.2025.3547343},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10222788,
  author={Wang, Mengyao},
  booktitle={2023 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)}, 
  title={Artificial Intelligence Narration in Virtual Reality}, 
  year={2023},
  volume={},
  number={},
  pages={376-380},
  abstract={Virtual reality technology provides users with an immersive experience that is different from reality, creating more vivid, realistic, and interesting narrative forms. Artificial intelligence can play multiple roles in VR storytelling, helping to create more attractive, immersive, and emotionally connected narratives. With the development and integration of virtual reality and artificial intelligence, more and more people have begun to realize the potential of their combination in storytelling. Based on case studies, this article explores in detail how artificial intelligence technology is applied to virtual reality storytelling, including interactive movies, narrative-based games, and virtual art. The article summarizes the unique advantages and challenges of virtual reality technology in storytelling and emphasizes the safety and social responsibility issues of AI storytelling in virtual reality, as well as the social morals and ethical values we should uphold.},
  keywords={Ethics;Art;Conferences;Virtual reality;Immersive experience;Games;Motion pictures;Artificial intelligence;virtual reality;narrative;interactive narrative},
  doi={10.1109/ICMEW59549.2023.00071},
  ISSN={},
  month={July},}@INPROCEEDINGS{10593995,
  author={Joseph, Hrithik M and Aishwarya, S. and Sriga and Khumar, Harish},
  booktitle={2024 International Conference on Electronics, Computing, Communication and Control Technology (ICECCC)}, 
  title={Student Answer Script Evaluation Using Large Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={01-06},
  abstract={This research proposes a novel approach to assess the quality of summaries written by students. This will assist teachers in evaluating the quality of student summaries and also help learning platforms provide immediate feedback to students. Evaluating summaries introduces an added layer of complexity, where models must consider both the student's context and the actual text. Although there are a handful of current techniques for summary evaluation, these models have often focused on assessing automatically-generated summaries rather than real student writing. Students rarely have enough opportunities to practice this skill, as evaluating and providing feedback on summaries can be a time-intensive process for teachers. Large Language Models are foundational machine learning models that use deep learning algorithms to process and understand natural language. These models are trained on massive amounts of text data to learn patterns and entity relationships in the language. Students will have more opportunities to practice summarizing, while simultaneously improving their reading comprehension.},
  keywords={Deep learning;Technological innovation;Accuracy;Machine learning algorithms;Large language models;Instruments;Natural languages;LLM;Summarization;Answer Script Evaluation},
  doi={10.1109/ICECCC61767.2024.10593995},
  ISSN={},
  month={May},}@ARTICLE{8603726,
  author={Wang, Xuejin and Shao, Feng and Jiang, Qiuping and Fu, Randi and Ho, Yo-Sung},
  journal={IEEE Access}, 
  title={Quality Assessment of 3D Synthesized Images via Measuring Local Feature Similarity and Global Sharpness}, 
  year={2019},
  volume={7},
  number={},
  pages={10242-10253},
  abstract={Depth-image-based rendering (DIBR) techniques can be used to generate virtual views for free-viewpoint video application. However, the DIBR algorithms will introduce geometric distortions that mainly distribute at the disoccluded regions in the synthesized views. It has been demonstrated that conventional 2-D quality metrics are not suitable for the synthesized views. In this paper, we propose a new quality model for 3-D synthesized images by measuring the block-wise texture similarity and color contrast similarity in critical areas, and the global gradient magnitude deviation. A critical area detection module is first employed using a warping method with morphological operation. Then, the critical areas are partitioned into blocks, which are classified as edge blocks, texture blocks, and smooth blocks by computing discrete cosine transform coefficient values. Block-wise texture similarity and color contrast similarity in the corresponding areas are calculated, which are weighted by the size of critical areas. Furthermore, gradient magnitude deviation is measured to quantify global sharpness. Finally, the two scores are pooled to obtain the overall quality. The experimental results on the IRCCyN/IVC, IETR, and MCL-3-D DIBR image databases indicate that our method achieves higher quality prediction accuracy than the state-of-the-art quality metrics.},
  keywords={Distortion;Three-dimensional displays;Quality assessment;Distortion measurement;Rendering (computer graphics);Image edge detection;Quality assessment;depth-image-based rendering;3D synthesized image;view synthesis},
  doi={10.1109/ACCESS.2019.2891070},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10655676,
  author={Tores, Julie and Sassatelli, Lucile and Wu, Hui-Yin and Bergman, Clement and Andolfi, Léa and Ecrement, Victor and Precioso, Frédéric and Devars, Thierry and Guaresi, Magali and Julliard, Virginie and Lecossais, Sarah},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Visual Objectification in Films: Towards a New AI Task for Video Interpretation}, 
  year={2024},
  volume={},
  number={},
  pages={10864-10874},
  abstract={In film gender studies, the concept of “male gaze” refers to the way the characters are portrayed on-screen as objects of desire rather than subjects. In this article, we introduce a novel video-interpretation task, to detect character objectification in films. The purpose is to reveal and quantify the usage of complex temporal patterns operated in cinema to produce the cognitive perception of objectification. We introduce the ObyGaze12 dataset, made of 1914 movie clips densely annotated by experts for objectification concepts identified in film studies and psychology. We evaluate recent vision models, show the feasibility of the task and where the challenges remain with concept bottleneck models. Our new dataset and code are made available to the community.},
  keywords={Representation learning;Visualization;Computer vision;Codes;Computational modeling;Psychology;Motion pictures;dataset dense annotation;pre-trained models;concept-bottleneck models;video interpretation;male gaze;objectification},
  doi={10.1109/CVPR52733.2024.01033},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10912536,
  author={Brini, Mohamed Ayachi and Touati, Hanae and Thabet, Rafika and Fontanili, Franck and Cleostrate, Marie-Hélène and Cufi, Marie-Noëlle and Pruski, Marc and Lamine, Elyes},
  booktitle={2024 IEEE/ACS 21st International Conference on Computer Systems and Applications (AICCSA)}, 
  title={Model-Based Artificial Intelligence Architecture for Digitizing Handwritten Medication Error Reports}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Optical Character Recognition (OCR) is extremely useful in various sectors for exploring massive archived data. This technology enables the digitization of printed and handwritten texts that are frequently present in the medical field. For instance, medication error (ME) reports were previously and still in some healthcare facilities written manually, this has led to the accumulation of numerous handwritten data that are unfortunately challenging to exploit. Their digitization through OCR allows extracting important data from these documents and using them to populate the database to implement future analysis techniques to optimize the medication error management process. This paper presents a transformer-based handwritten recognition architecture that employs the Transformer-Based Optical Character Recognition (TrOCR) model combined with image segmentation techniques. Although the TrOCR model provided by Microsoft performs reasonably well in handwritten recognition, it is limited to English text because its pretrained version was trained exclusively on English samples. This limitation is problematic for us, as our task involves digitizing French medication dictation errors. Additionally, its limitation to processing single-line text images impairs its ability to recognize paragraphs. To address these limitations, we will fine-tune the model on French handwritten data and integrate a single-line level segmentation technique, thereby overcoming these constraints. Therefore, the preliminary results from implementing our proposed architecture are promising for the digitization of medication error reports.},
  keywords={Deep learning;Image segmentation;Handwriting recognition;Image recognition;Text recognition;Ultraviolet sources;Optical character recognition;Computer architecture;Transformers;Biomedical imaging;Optical Character Recognition (OCR);Deep Learning (DL);Transformer;Medication Error Management},
  doi={10.1109/AICCSA63423.2024.10912536},
  ISSN={2161-5330},
  month={Oct},}
