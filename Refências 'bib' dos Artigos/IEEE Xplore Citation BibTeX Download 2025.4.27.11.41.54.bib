@INPROCEEDINGS{10849934,
  author={Yang, Ruolin and Li, Da and Zhang, Honggang and Song, Yi-Zhe},
  booktitle={2024 IEEE International Conference on Visual Communications and Image Processing (VCIP)}, 
  title={SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Sketching is a uniquely human tool for expressing ideas and creativity. The animation of sketches infuses life into these static drawings, opening a new dimension for designers. Animating sketches is a time-consuming process that demands professional skills and extensive experience, often proving daunting for amateurs. In this paper, we propose a novel sketch animation model SketchAnimator, which enables adding creative motion to a given sketch, like "a jumping car". Namely, given an input sketch and a reference video, we divide the sketch animation into three stages: Appearance Learning, Motion Learning and Video Prior Distillation. In stages 1 and 2, we utilize LoRA to integrate sketch appearance information and motion dynamics from the reference video into the pre-trained T2V model. In the third stage, we utilize Score Distillation Sampling (SDS) to update the parameters of the Bézier curves in each sketch frame according to the acquired motion information. Consequently, our model produces a sketch video that not only retains the original appearance of the sketch but also mirrors the dynamic movements of the reference video. We compare our method with alternative approaches and demonstrate that it generates the desired sketch video under the challenge of one-shot motion customization.},
  keywords={Visual communication;Image processing;Dynamics;Animation;Diffusion models;Mirrors;Automobiles;Text to video;Creativity;Sketch animation;diffusion process;generative model;video generation;motion extraction},
  doi={10.1109/VCIP63160.2024.10849934},
  ISSN={2642-9357},
  month={Dec},}@INPROCEEDINGS{10612339,
  author={Sprague, Christopher Iliffe and de la Asunción-Nadal, Víctor and García-Fernández, Alberto},
  booktitle={2024 9th International Conference on Smart and Sustainable Technologies (SpliTech)}, 
  title={Implementing AI in Advanced Recycling of Perovskite Solar Cells}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={The electrification of society is an essential component of the effort to achieve a fossil-free world, where solar cells will play a central role in the future energy system. The advancement of photovoltaics should be aligned with principles of the circular economy. In the last years, lead halide perovskites have risen as a leading candidate for third-generation solar cells, experiencing rapid advancement. However, the manufacturing of commercial products inevitably generates significant waste and end-of-life devices, leading to potentially severe environmental repercussions. To tackle this challenge, proactive research and development of recycling and recovery technologies for perovskite solar cells are very necessary. Here we introduce a proof of concept, which, to the best of our knowledge, is the first AI-guided method designed to predict the optimal recycling treatment for perovskite solar cells and the first construction of a perovskite recycling dataset. Using sentiment analysis language processing for the first time, we achieved up to 70% accuracy in predicting the correct action for a given device structure. This innovative approach opens up new possibilities for applying sentiment analysis as a tool for e-waste recycling.},
  keywords={Waste reduction;Sentiment analysis;Technological innovation;Protocols;Accuracy;Photovoltaic cells;Perovskites;Perovskite solar cells;recycling;artificial intelligence},
  doi={10.23919/SpliTech61897.2024.10612339},
  ISSN={},
  month={June},}@ARTICLE{10540119,
  author={Tolba, Rahma M. and Elarif, Taha and Taha, Zaki and Hammady, Ramy},
  journal={IEEE Access}, 
  title={Interactive Augmented Reality System for Learning Phonetics Using Artificial Intelligence}, 
  year={2024},
  volume={12},
  number={},
  pages={78219-78231},
  abstract={The increasing adoption of language learning apps that utilize Augmented Reality (AR) and Artificial Intelligence (AI) for speech recognition has sparked interest in the potential benefits for phonetics education. However, currently available AR apps only focus on teaching letter names and vocabulary, lacking the potential for a more immersive learning experience. To address this limitation, this paper introduces an interactive AR system that integrates AI speech recognition with AR to provide an engaging and interactive learning experience. To showcase the capabilities of the proposed system, we have created a prototype for the Arabic Phonetic Atlas textbook. This prototype enhances reading the /s/ sound page in the Atlas by incorporating a 3D animated model of the speech organs onto the existing 2D image. The dynamic animation of the 3D model reflects the sound description provided in the Atlas. The system also offers real-time user pronunciation feedback through a customized AI phoneme recognition system. A comprehensive user study was conducted to evaluate the usability and learning impact of the proposed system, involving 83 adult participants aged between 18-40. The assessment approach involved the use of both direct and indirect observations, as well as various surveys to gather both numerical and qualitative information. The findings indicate not only a greater level of understanding compared to conventional methods but also an improved capability to master specific phonemes quickly and effortlessly. In addition, they are showing great potential for the proposed system to be incorporated into the conventional classroom setting as an instructional aid.},
  keywords={Phonetics;Artificial intelligence;Three-dimensional displays;Solid modeling;Recording;Mars;Animation;Augmented reality;Natural language processing;Interactive augmented reality;learning phonetics;language learning;Arabic phonetics},
  doi={10.1109/ACCESS.2024.3406494},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8875976,
  author={Koc, Emircan and Ozsoy, Adnan},
  booktitle={2019 International Artificial Intelligence and Data Processing Symposium (IDAP)}, 
  title={Approximate Data Driven Parallel Shape Matching for Soft Body Physics Simulations}, 
  year={2019},
  volume={},
  number={},
  pages={1-5},
  abstract={We propose an algorithm that is fully parallel and has linear time complexity for soft body simulation that addresses three principal issues; Visual Quality, Performance and Ease of use. It works using the power of precomputed collision result look-up data and basic approach of shape matching. Since data driven shape matching approach only uses user generated precomputed collision results, deformation results cannot be unexpected. This creates visual quality and improves ease of use. Also, usage of these look-up data opens ways to improve Performance. In our tests, we achieved direct linear speed up depending on the processor's core count.},
  keywords={Computational modeling;Shape;Visualization;Industries;Mathematical model;Deformable models;Strain;Soft Body;Physics;Shape Matching;Parallel Programming},
  doi={10.1109/IDAP.2019.8875976},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{8959886,
  author={Soo, Matteo and Yang, Ya-Chi and Soo, Von-Wun},
  booktitle={2019 International Conference on Technologies and Applications of Artiﬁcial Intelligence (TAAI)}, 
  title={Automatic Conversion of a Chinese Fairy Story into a Script – A Preliminary Report and Proposal}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={Script writing for a movie or theater play is a difficult task that requires substantial literature expertise and creativity of human drama writers. We are interested in developing AI techniques and tools that can facilitate the literature creative process. To convert a Chinese fairy story into a script, we have to extract characters, locations, time, objects, as well as dialogues in order to decide the proper scenes in which actors and conductors can follow to conduct the actions and dialogues in the story. We reported preliminary results on this study using natural language processing techniques and commonsense knowledge. The work also poses challenging problems for various future research directions.},
  keywords={Story generation;script wring;information extraction;common sense reasoning;scene division;dialogue of speakers},
  doi={10.1109/TAAI48200.2019.8959886},
  ISSN={2376-6824},
  month={Nov},}@INPROCEEDINGS{10616499,
  author={Hariharasudhan, S and Subasree, S. and Bhuvaneswari, M. and Victor, Mekvin and Kumar, Rintu and Kumar, K. Sambath},
  booktitle={2024 4th International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)}, 
  title={The Role of IOT in Creating SC’S through Ultra Fast Updation of the Status for Accurate Action Plan}, 
  year={2024},
  volume={},
  number={},
  pages={188-193},
  abstract={The idea of a smart city includes the merging of technologies and advances aimed at improving urban efficiency, scientific progress, the preservation of the environment, and social inclusion. Coined in the year 2000, the term became widely used in politics, business, management, and urban planning groups to drive tech-based changes in urban areas. It reacts to the difficulties posed by postindustrial communities handling problems such as pollution to the environment, demographic changes, population growth, health care monetary crises, and resource shortages. Beyond technical answers, the smart city idea includes non-technical innovations for healthy urban life. Particularly encouraging is the application that uses Internet of The circumstances (IoT)based sensors in healthcare, applying machine learning for effective data management. This paper discusses the application of AI-powered Ai and Wireless Sensor Networks, more commonly known as the field of health care, acting as a basic study to understand the impact of IoT in smart cities, especially in healthcare, for the sake of future research.},
  keywords={Technological innovation;Accuracy;Smart cities;Sociology;Medical services;Machine learning;Blockchains;IoT;smart cities;healthcare;AI-powered systems;wireless sensor networks;machine learning;blockchain;smart technology;urban efficiency;data management},
  doi={10.1109/ICACITE60783.2024.10616499},
  ISSN={},
  month={May},}@INPROCEEDINGS{9421020,
  author={Orihara, Ryohei},
  booktitle={2021 5th IEEE Electron Devices Technology & Manufacturing Conference (EDTM)}, 
  title={Applications of AI Technologies in Flash Memory Business}, 
  year={2021},
  volume={},
  number={},
  pages={1-3},
  abstract={In semiconductor manufacturing, major issues include automation of the operation, expansion of the production scale, and cost reduction and competitiveness enhancement by means of quality control. On Kioxia's production sites, a vast amount of data is collected from automated production lines. AI technologies are being applied to the data in order to implement information systems capable of quality monitoring and problem solving, which helps us keep high product quality. In this paper, three case studies of AI technology applications are reported. The first two cases are from the production stage, one being an application to yield analysis, and another being an application to defect inspection. The last case is from the design stage, an application to the topography simulation. On the other hand, the creation of artwork is a rapidly emerging field for the AI application. TEZUKA2020, a project in which AI and human collaboratively tried to produce a new piece of manga by the legendary Japanese artist Osamu Tezuka, was recently carried out as an ad campaign to promote Kioxia's corporate name. The project is briefly reported in this paper.},
  keywords={Public relations;Production;Machine learning;Quality control;Surfaces;Product design;Manufacturing;Machine learning;semiconductor manufacturing;pattern recognition;defect classification;Osamu Tezuka},
  doi={10.1109/EDTM50988.2021.9421020},
  ISSN={},
  month={April},}@INPROCEEDINGS{9255497,
  author={Hasibuan, Zainal A.},
  booktitle={2020 International Workshop on Big Data and Information Security (IWBIS)}, 
  title={Towards Using Universal Big Data in Artificial Intelligence Research and Development to Gain Meaningful Insights and Automation Systems}, 
  year={2020},
  volume={},
  number={},
  pages={9-18},
  abstract={The increasing number of human activities using information and communication technology (ICT) generates a tremendous amount of data. It brings the opportunity to use universal big data for further computation that may deliver new insights and automation system. The technology that enables this work is artificial intelligence (AI). As known, the utilization of AI technology is becoming more and more pervasive in our daily life. Furthermore, discussion related to AI has played an essential role in an organization's decision-making process. Thus, this paper discusses the utilization of AI technology that penetrates end-to-end various aspects of human activities, such as in education, health, business, social life and so forth. The end-to-end process begins with data collection covering various types of data (text, picture, audio, video, animation) and various methods (survey, observation, interview, experiment). Then, it continues with the pre-process data cleansing and process to determine data features, and seeks the relationship within them, using machine learning process with appropriate algorithms. The results may then be used for applications portfolio in order to improve organization strategies and programs. The organization's performance can be visualized from time to time for continuous quality improvement. The end-to-end cycles of processes continue, from data collection, data pre-processing and processing, performance computation monitoring (identification, classification, prediction, and prescription), improving strategy and program, and implementing in the real-life activities until generating more behavioral data, finding pattern and design the systems. As a whole, this end-to-end cycle leads us to an AI automation system, with the power to generate meaningful insights suited to solve current problems, predicting trending issues, and understanding phenomena.},
  keywords={Feature extraction;Artificial intelligence;Classification algorithms;Correlation;Automation;Data visualization;Big Data;ICT;artificial intelligence;AI;automation system;big data;finding pattern;machine learning;deep learning;end-to-end cycle processes},
  doi={10.1109/IWBIS50925.2020.9255497},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10947926,
  author={Joshi, Rahul and Kumari, Suman and Pandey, Krishna},
  booktitle={2024 International Conference on Artificial Intelligence and Emerging Technology (Global AI Summit)}, 
  title={From Gaming Online to Metaverse: VR's Growing Impact on Everyday Life}, 
  year={2024},
  volume={},
  number={},
  pages={318-323},
  abstract={The metaverse facilitates users in accessing a diverse array of experiences that were previously inaccessible to them. Nevertheless, it is essential to note that this emerging technical paradigm has yet to achieve widespread commercialization. The production of experiences in the metaverse is currently in its nascent phase. Advancements and enhancements in hardware and software technologies are expected to enhance the quality and aesthetic appeal of metaverse environments greatly. Utilizing specialized gear tailored to specific events, like hiking, surfing, and skiing, can enhance the vividness of experiences and render them more lifelike. However, the narrative structure of a game is constructed within the context of human cognition and digital programming. Consequently, immersive experiences only create a collection of “virtual monuments” that are derived from or subservient to the actual physical world. Hence, narrativity inside video games establishes them as a performative domain that effectively mirrors the intricate dynamics of contemporary culture. This paper elucidates the role of the gaming industry in the emergence of the metaverse. Additionally, this paper examines prominent gaming platforms and illustrates their distinguishing characteristics. Furthermore, it expounds on the role of gaming in influencing the development of the metaverse, as it facilitates the creation and engagement of novel gaming encounters within this virtual realm.},
  keywords={Video games;Metaverse;Games;Virtual reality;Transforms;Production;Software;Mirrors;Artificial intelligence;Programming profession;AI;animation;digital;Internet;metaverse;online games;virtual reality},
  doi={10.1109/GlobalAISummit62156.2024.10947926},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10927354,
  author={Peter, Emmanuel and Otuoniyo, Ufuoma-Oghene and Opabode, Mujeeb and Kolade, Oluwatosin and Alimi, Rereloluwa and Ajiboye, Michael and Lawal, Olamide},
  booktitle={2024 IEEE 5th International Conference on Electro-Computing Technologies for Humanity (NIGERCON)}, 
  title={Enhancing Sensorimotor Rehabilitation through FPGA-Based Reaction Time Game with AI-Driven Recovery Tracking}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={This paper presents an innovative approach to assist individuals with Sensorimotor impairments in their limbs by utilising an FPGA system-based reaction time game integrated with Artificial Intelligence-driven recovery tracking. The core objective is to help patients regain limb functionality by engaging them in a competitive and immersive gaming experience that promotes reflex improvement. The system leverages the FPGA board to create an interactive environment where patients respond to illuminated LEDs by activating corresponding switches. AI algorithms are employed to monitor and analyse the recovery rate of each finger which then helps to provide detailed insights into the patient's progress. The AI tracks which fingers show improvement over time and evaluates their responsiveness when specific LEDs light up. This data-driven approach offers personalised feedback and aids the rehabilitation process by highlighting areas needing further attention. The paper details the design, implementation, and potential impact of this system on Sensorimotor rehabilitation, thereby demonstrating a novel intersection of gaming, AI, and healthcare technology.},
  keywords={Limbs;Technological innovation;Humanities;Medical treatment;Games;Light emitting diodes;Real-time systems;Artificial intelligence;Field programmable gate arrays;Monitoring;Sensorimotor impairment rehabilitation;Interactive therapy;AI in Rehabilitation;Rehabilitation technology},
  doi={10.1109/NIGERCON62786.2024.10927354},
  ISSN={2377-2697},
  month={Nov},}@ARTICLE{10746594,
  author={Cheng, Runze and Sun, Yao and Niyato, Dusit and Zhang, Lan and Zhang, Lei and Imran, Muhammad Ali},
  journal={IEEE Transactions on Mobile Computing}, 
  title={A Wireless AI-Generated Content (AIGC) Provisioning Framework Empowered by Semantic Communication}, 
  year={2025},
  volume={24},
  number={3},
  pages={2137-2150},
  abstract={With the significant advances in AI-generated content (AIGC) and the proliferation of mobile devices, providing high-quality AIGC services via wireless networks is becoming the future direction. However, the primary challenges of AIGC services provisioning in wireless networks lie in unstable channels, limited bandwidth resources, and unevenly distributed computational resources. To this end, this paper proposes a semantic communication (SemCom)-empowered AIGC (SemAIGC) generation and transmission framework, where only semantic information of the content rather than all the binary bits should be generated and transmitted by using SemCom. Specifically, SemAIGC integrates diffusion models within the semantic encoder and decoder to design a workload-adjustable transceiver thereby allowing adjustment of computational resource utilization in edge and local. In addition, a resource-aware workload trade-off (ROOT) scheme is devised to intelligently make workload adaptation decisions for the transceiver, thus efficiently generating, transmitting, and fine-tuning content as per dynamic wireless channel conditions and service requirements. Simulations verify the superiority of our proposed SemAIGC framework in terms of latency and content quality compared to conventional approaches.},
  keywords={Semantics;Computational modeling;Decoding;Servers;Transmitters;Transceivers;Receivers;Wireless networks;Dynamic scheduling;Image edge detection;AI-generated content;semantic communication;diffusion model;intelligent workload adaptation},
  doi={10.1109/TMC.2024.3493375},
  ISSN={1558-0660},
  month={March},}@INPROCEEDINGS{10687926,
  author={Anandhi, R J and Anjimoon, Shaik and Tiwari, Sandeep Kumar and Singh, Navdeep and Parmar, Ashish and Faisal, Alabboodi Ahmed Sahib},
  booktitle={2024 OPJU International Technology Conference (OTCON) on Smart Computing for Innovation and Advancement in Industry 4.0}, 
  title={Harnessing the Capabilities of OpenAI’s CLIP and RNN for Visual Sequence Understanding in Film Editing}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Using OpenAI’s Contrastive Language-Image Pretraining (CLIP) and Recurrent Neural Network (RNN) technology, this study develops a novel approach to video editing. Our proposed workflow, which consists of three primary techniques, alters the use of visual order information in film editing. Story speed, scene selection, and CLIP’s multimodal comprehension are all enhanced when RNN’s time analysis is applied in tandem with it. To locate pertinent visual content using semantic matching and textual descriptions, Scene Retrieval and Semantic Matching (SRSM) first use CLIP. To find the optimal pace for film sequences, the second technique, TAPO, employs recurrent neural networks (RNNs) to analyze how time evolves. The third application that creates music to match the atmosphere of movie sequences is Adaptive Soundtrack Generation (ASG). These techniques, when used, make the experience of going to the movies more enjoyable for everyone. Our approach is clearly superior to the conventional methods of film editing, as demonstrated by our comprehensive study. Emotional impact, narrative coherence, audience happiness, immersion, and anticipation value were all highly rated, indicating that viewers were highly engaged. Its entertaining qualities have also garnered rave reviews. These figures suggest that our innovative approach to film editing has the potential to shake up the industry with its innovative features.},
  keywords={Visualization;Technological innovation;Recurrent neural networks;Reviews;Semantics;Music;Transforms;Adaptive Soundtrack Generation;CLIP;Film Editing;Multimodal Understanding;Narrative Cohesion;OpenAI;Pacing Optimization;RNN;Scene Selection;Viewer Engagement},
  doi={10.1109/OTCON60325.2024.10687926},
  ISSN={},
  month={June},}@INPROCEEDINGS{9032527,
  author={Khanna, Aryan and Manoj, Bhadage Ankit and T., Yashashwini and Parihar, Gulam Mohib and Maurya, Richa and B., Rajitha},
  booktitle={2019 Third International conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)}, 
  title={DeepStreak: Automating Car Racing Games for Self Driving using Artificial Intelligence}, 
  year={2019},
  volume={},
  number={},
  pages={420-425},
  abstract={Commercial Video games are becoming popular with the advent of Artificial Intelligence. Most of the online or offline video games are passive i.e. they strictly follow the scripted instructions. And then there are trending competitions in AI-based games that need to generate auto-scripts and auto-interactions to adapt to the dynamic scenario, improve performance and also resolve the challenges which are faced in real-time. Hence there is a need to develop new techniques with intelligence which can have a significant impact on the gaming industry. Thus, this paper discusses the list of challenges and research opportunities available for developing new AI techniques that can be used by computer game developers and proposes techniques to completely automate the video games using three different approaches including key points being data augmentation and segmentation techniques followed by feeding data to CNNs.},
  keywords={Image segmentation;Games;Semantics;Data models;Automobiles;Artificial intelligence;Convolutional neural networks;Artificial Intelligence;Clustering;CNN;Data Augmentation;Semantic Augmentation;Deep Learning},
  doi={10.1109/I-SMAC47947.2019.9032527},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9936738,
  author={Sychandran, C S and Shreelekshmi, R},
  booktitle={2022 IEEE International Conference on Artificial Intelligence in Engineering and Technology (IICAIET)}, 
  title={A hybrid Xception-Ensemble model for the detection of Computer Generated images}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Digital images play a vital role in digital communication due to their applications in various domains like games, movies, and medical and legal spheres. Entities fabricate content through computer-generated images, which causes severe adverse consequences. We propose a novel hybrid Xception-Ensemble approach for distinguishing computer-generated images using the depthwise separable convolution of the Xception architecture. We use depthwise separable convolution and the parameters transferred from the pre-trained ImageNet weights to distinguish the features in computer-generated images with ensemble average learning for efficient classification. The accuracy of the proposed system is better than that of state of the art systems on DSTok, Columbia PRCG and Rahmouni datasets.},
  keywords={Knowledge engineering;Convolution;Law;Digital images;Transfer learning;Neural networks;Feature extraction;Digital image forensics;Deep learning;Computer generated images;Transfer learning;Hybrid Xception-Ensemble},
  doi={10.1109/IICAIET55139.2022.9936738},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10361104,
  author={Hu, Guangkuo and Zhao, Yanyan and Samia, Elghazali},
  booktitle={2023 2nd International Conference on Data Analytics, Computing and Artificial Intelligence (ICDACAI)}, 
  title={Influence of Perceived Authenticity of Virtual Spokesperson on Consumer Brand Attitude}, 
  year={2023},
  volume={},
  number={},
  pages={51-56},
  abstract={With the rapid development of digital technologies such as virtual reality (VR), artificial intelligence (AI) and computer-generated image (CGI), the types of virtual spokespersons are more and more abundant and widely applied. Virtual spokespersons build a new communication channel between brands and consumers, so it is urgent for enterprises to explore the influence of perceived authenticity of virtual spokespersons on consumers' brand attitudes. Based on the perspective of psychological distance, through the empirical analysis of the results, it is found that the perceived authenticity of virtual spokesperson has a significant positive impact on psychological distance and brand attitude, and psychological distance has a significant positive impact on brand attitude. Psychological distance plays a partial mediating role between virtual spokesperson's perceived authenticity and brand attachment.},
  keywords={Data analysis;Psychology;Virtual reality;Communication channels;Modems;Artificial intelligence;Virtual spokesperson;computer-generated image technology;perceived authenticity;psychological distance;brand attitude},
  doi={10.1109/ICDACAI59742.2023.00015},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10866539,
  author={Chini, Mrinmoy Kumar and Sagar, Sipra and Bhanu, D. and Sharma, Vishal and Vigenesh, M. and Kumar, Santosh},
  booktitle={2024 1st International Conference on Sustainable Computing and Integrated Communication in Changing Landscape of AI (ICSCAI)}, 
  title={Big Data in Agricultural and Environmental Assessment: Opportunities and Difficulties}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Big Data in Agriculture and Ecological Analysis: Possibilities and Challenges The same capabilities that gave rise to the Internet and are revamping medicines are now being used as agriculture stands on the verge of a digitalization. In general, the cheap cost of gathering growth, together with data from temperature readings and data gathered by aircraft and satellites, is what is driving this digital agrarian economy. These technologies promise to generate more food with less resources, less land, less labor, and a lower environmental impact. The inability to gather and analyses data in a manner that produces usable farmers, as well as the need to teach farmers how to utilize such tools, are now obstacles to fulfilling this promise. The promise and obstacles to fulfilling to change agriculture are reviewed in this essay.},
  keywords={Natural resources;Temperature distribution;Costs;Satellites;Production;Pricing;Big Data;Agriculture;Manufacturing;Monitoring;Big Data;Agriculture;Environmental assessment;Data analysis;Data gathering;Farmers;Precision agriculture;Sensor technology;Sustainability;Data ownership;Policy concerns;Farm policy;Agri-environmental policy;Traceability;Data management;Data sharing},
  doi={10.1109/ICSCAI61790.2024.10866539},
  ISSN={},
  month={July},}@INPROCEEDINGS{10396815,
  author={Mi, Xin and Cao, Yang and Li, Qiao},
  booktitle={2023 4th International Conference on Intelligent Design（ICID）}, 
  title={The Application Path of AIGC in Assisting Game Character Design - A Case Study of the Black Bear Monster from Journey to the West}, 
  year={2023},
  volume={},
  number={},
  pages={275-282},
  abstract={With the continuous development of artificial intelligence technology, AIGC (AI Generated Content) technology has been widely applied. However, there is still a lack of an effective AI-assisted design pathway in the field of game character design. This paper takes a perspective from game character design and is based on the cultural connotations and aesthetic features of the Black Bear Monster from "Journey to the West". It constructs an application pathway that introduces AIGC to promote innovative thinking activities and enhance the creative abilities of designers, thereby achieving AIGC-assisted game character design. The aim of this paper is to optimize the game character production process, provide insights for the creation of monster characters in "Journey to the West", and promote Chinese monster culture and the spirit of the journey.},
  keywords={Solid modeling;Technological innovation;Design methodology;Games;Production;Cultural differences;Artificial intelligence;AIGC;Game Character Design;Journey to the West;3D digital modeling technology;Black Bear},
  doi={10.1109/ICID60307.2023.10396815},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9371846,
  author={Shah, Bibi Mariat and Murtaza, Mohsin and Raza, Mansoor},
  booktitle={2020 5th International Conference on Innovative Technologies in Intelligent Systems and Industrial Applications (CITISIA)}, 
  title={Comparison of 4G and 5G Cellular Network Architecture and Proposing of 6G, a new era of AI}, 
  year={2020},
  volume={},
  number={},
  pages={1-10},
  abstract={In this 21st Century, the deployment of wireless technology has created a spur amongst technologists about the future progression of wireless communication. It has not only transformed the way of communication but also paved a way for many multi-functional devices and technology. The essence of modern culture is the digital economy, which is also the base for a wireless system of connections. There was a time when wireless networks evolved, from 0G towards 4G along with their architectures and new features. The correlation is contrasting between 4G and 5G with their architectures, coverage, speed quality of service, bandwidth, and latency rates. But later the 5G system architecture conceptualized a full-bodied network, which introduced an architecture that worked intensively while the petulant analysis and consultations are stimulated to bring an advanced evolvement in the wireless community, hence 5G was slowly and gradually deployed all over the world. Meanwhile, the idea of 6G is being looked into and the researchers and engineers are working on its architecture and development, which will make it significantly different from the preceding generations some of which have made a mark and some that have not. This paper presents a complete understanding of variations between 4G and 5G wireless network architectures, the comparisons, and the evolvement through all these years. The analytical and factual results help in advancing an evolved 6G architecture and the change, enhancement, and transformation it will bring in the future of Wireless Cellular Networks.},
  keywords={6G mobile communication;Cellular networks;5G mobile communication;Wireless networks;Systems architecture;Quality of service;Intelligent systems;4G;5G;6G;Architecture;Wireless;conceptualized;analytical;generations;enhancements;transformation},
  doi={10.1109/CITISIA50690.2020.9371846},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10722016,
  author={Mehnen, Lars and Pohn, Birgit},
  booktitle={2024 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)}, 
  title={Supporting Academic Teaching with Integrating AI in Learning Management Systems: Introducing a Toolchain for Students and Lecturers}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Artificial Intelligence (AI) is transforming educational technology by enhancing both teaching and learning processes. This paper examines the “LearnStreamAI” project at Technikum Wien, which integrates an AI-driven chatbot within the Moodle LMS to support real-time student interactions. Utilizing advanced AI technologies like OpenAI’s ChatGPT-4, Google Gemini, and Anthropic Claude3, the chatbot adapts to individual students’ knowledge levels, provides tailored feedback, and enhances engagement through interactive elements in quizzes. Multilingual subtitles using OpenAI’s Whisper technology further improve accessibility for diverse student bodies. In parallel, a toolchain has been developed that automates the creation of academic materials using a PowerShell script and the OpenAI API, based on an easily maintainable Excel input file. This includes generating PowerPoint slides, Moodlecompatible questions, and detailed topic descriptions, all exported into Moodle XML format. The approach ensures accessibility and ease of use for lecturers across various disciplines. The results indicate significant time savings and improved consistency in material preparation. Feedback from pilot studies shows that the AI-generated content is clear, relevant, and well-aligned with academic goals. The system also aids lecturers in quickly acquiring new knowledge by explaining, translating, and summarizing literature. This paper discusses the design, implementation, benefits, and potential improvements of this AI-driven tool, highlighting its role in modern academic teaching and the associated challenges and ethical considerations.},
  keywords={Learning management systems;Ethics;XML;Learning (artificial intelligence);Chatbots;Software;Real-time systems;Telecommunications;Internet;Materials preparation;Artificial Intelligence;Teaching;Learning;ChatGPT},
  doi={10.23919/SoftCOM62040.2024.10722016},
  ISSN={1847-358X},
  month={Sep.},}@INPROCEEDINGS{10730211,
  author={Pangestu, Muhamad Wisnu and Setyanto, Arief},
  booktitle={2024 8th International Conference on Information Technology, Information Systems and Electrical Engineering (ICITISEE)}, 
  title={Reconstructing Missing Frame in Video Transmission with Frame Interpolation}, 
  year={2024},
  volume={},
  number={},
  pages={435-440},
  abstract={This study addresses the issue of missing frame in video transmission, a critical factor affecting Quality of Service (QoS) in video communications. Using spatial interpolation techniques, the research generates replacement frames by analyzing pixels and patterns from surrounding frames, evaluating their accuracy in enhancing visual quality, playback smoothness, and user satisfaction. The FILM Model simulates frame loss scenarios, testing an algorithm designed to recover missing frames and assessing its effectiveness through the Structural Similarity Index (SSIM). Six different videos with four types of transitions were examined, spanning various video types. T-Test analysis across diverse video scenarios indicates that the model performs exceptionally well in Low Lighting and HDR conditions, maintaining high SSIM values and visual quality even at greater frame distances. However, challenges arise with AI-generated and camera phone footage, where SSIM values significantly decline during complex transitions like Cross Dissolve. These differences are statistically significant, highlighting the model's need for improvement in handling diverse video types. The findings underscore the importance of refining the model to better manage dynamic scenarios and complex transitions, enhancing its applicability in broader video production contexts and potentially improving real-time video communication systems.},
  keywords={Visualization;Solid modeling;Interpolation;Stability criteria;Refining;Quality of service;Streaming media;Solids;Real-time systems;Testing;Framedrop;Frame Interpolations;Video Restoration;Generated Frame;FILM Model;SSIM},
  doi={10.1109/ICITISEE63424.2024.10730211},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10770891,
  author={Aryal, Sadiksha and Magani, Niyomdi and Oluoch, Jared},
  booktitle={2024 Conference on AI, Science, Engineering, and Technology (AIxSET)}, 
  title={An Ensemble Machine Learning Technique for Detecting Distributed Denial of Service Attacks in Vehicular Ad Hoc Networks}, 
  year={2024},
  volume={},
  number={},
  pages={170-178},
  abstract={Vehicular Ad-Hoc Networks (VANETs) have the potential to improve road safety and enhance traffic management. Due to their mobility and short-lived network connections, they are vulnerable to a variety of security risks, the most disruptive of which is distributed denial of service (DDoS) attack. This attack can completely take down a network thus depriving road users of crucial services, compromising traffic efficiency, and negatively impacting vehicle safety. This paper presents a mechanism for detecting DDoS attacks on VANETs using machine learning algorithms. The novelty of this work is an ensemble algorithm created by leveraging the strengths of six different ML algorithms to detect DDoS attacks in a VANET environment. The proposed ensemble algorithm performs better or at least comparable to existing solutions on key performance indicators.},
  keywords={Support vector machines;Machine learning algorithms;Firewalls (computing);Vehicle safety;Vehicular ad hoc networks;Virtual environments;Denial-of-service attack;Road safety;Security;Computer crime;Vehicular Ad-Hoc Networks;DDoS;Machine Learning;Random Forest;Network Security},
  doi={10.1109/AIxSET62544.2024.00032},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10298467,
  author={Xu, Bowen and Nguyen, Thanh-Dat and Le-Cong, Thanh and Hoang, Thong and Liu, Jiakun and Kim, Kisub and Gong, Chen and Niu, Changan and Wang, Chenyu and Le, Bach and Lo, David},
  booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Are We Ready to Embrace Generative AI for Software Q&A?}, 
  year={2023},
  volume={},
  number={},
  pages={1713-1717},
  abstract={Stack Overflow, the world's largest software Q&A (SQA) website, is facing a significant traffic drop due to the emergence of generative AI techniques. ChatGPT is banned by Stack Overflow after only 6 days from its release. The main reason provided by the official Stack Overflow is that the answers generated by ChatGPT are of low quality. To verify this, we conduct a comparative evaluation of human-written and ChatGPT-generated answers. Our methodology employs both automatic comparison and a manual study. Our results suggest that human-written and ChatGPT-generated answers are semantically similar, however, human-written answers outperform ChatGPT-generated ones consistently across multiple aspects, specifically by 10% on the overall score. We release the data, analysis scripts, and detailed results at https://github.com/maxxbw54/GAI4SQA.},
  keywords={Data analysis;Manuals;Chatbots;Software;Artificial intelligence;Software engineering;Generative AI;large language model;Software Q&A;Stack Overflow;ChatGPT},
  doi={10.1109/ASE56229.2023.00023},
  ISSN={2643-1572},
  month={Sep.},}@INPROCEEDINGS{10911974,
  author={Arshad, Mariam and Ghareeb, Shatha and Mustafina, Jamila},
  booktitle={2024 17th International Conference on Development in eSystem Engineering (DeSE)}, 
  title={Advancements in AI: A Study of English-to-Urdu Dubbing Methods and Applications}, 
  year={2024},
  volume={},
  number={},
  pages={249-254},
  abstract={The global film industry faces significant challenges in making content accessible across diverse linguistic and cultural contexts, particularly through dubbing, which involves translating and synchronizing audio for different languages. Traditional dubbing methods, while effective, are labour-intensive, costly, and often struggle with preserving cultural nuances and emotional depth. With advancements in artificial intelligence (AI), there is a promising opportunity to address these challenges and revolutionize the dubbing process. This research explores the application of AI-driven technologies to improve the efficiency and accuracy of dubbing English-language films into Urdu. The research investigates the current state of AI-driven dubbing technologies, identifies their limitations, and proposes methods to enhance their effectiveness. Key objectives include analysing AI technologies for translation and speech synthesis, developing a streamlined process for English-to-Urdu dubbing, and evaluating the cultural and emotional fidelity of AI-generated content. A comprehensive methodology involving data extraction, speech-to-text conversion, translation, text-to-speech synthesis, and audio-video integration is employed to create an automated dubbing system. The system’s performance is assessed based on accuracy, naturalness, and synchronization, revealing both successes and areas for improvement. The findings demonstrate that AI-driven dubbing can significantly enhance the efficiency and scalability of the dubbing process, although challenges related to accent variability, cultural sensitivity, and voice naturalness remain. The study concludes with recommendations for future research, including the need for improved transcription accuracy, advanced translation models, and real-time dubbing capabilities. This research contributes valuable insights into the potential of AI to transform film localization and other sectors by making content more accessible and culturally relevant to diverse audiences.},
  keywords={Translation;Accuracy;Sensitivity;Scalability;Transforms;Text to speech;Cultural differences;Synchronization;Artificial intelligence;Speech to text;AI for real-world applicaions;dubbing;cultural context},
  doi={10.1109/DeSE63988.2024.10911974},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10795949,
  author={Himi, Shinthi Tasnim and Tanzila Monalisa, Natasha and Sultana, Shirin and Afrin, Anika and Hasib, Khan Md},
  booktitle={2024 IEEE International Conference on Computing, Applications and Systems (COMPAS)}, 
  title={Automated Exam Script Checking using Zero-Shot LLM and Adaptive Generative AI}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The manual grading of exam scripts is a labor-intensive process, often plagued by subjectivity and inconsistency. This study presents an innovative approach to automating the evaluation of exam scripts using the Large Language Model (LLM) integrated with Zero-Shot Learning (ZSL) and Generative AI. Our system leverages the capabilities of GPT-4 to generate and evaluate answers, applying similarity measures to ensure accuracy and fairness in grading. The model’s adaptability through ZSL allows it to assess new questions without additional training. The system also incorporates iterative refinement techniques to enhance the quality of standard answers over time. Performance evaluations demonstrate a low error rate, with an average relative error of 1.29% for annotated questions and 1.67% for non-annotated questions compared to human graders. The performance underscores the automated system’s transformative potential in revolutionizing educational assessments, offering a consistent, fair, and highly efficient alternative to traditional grading methods.},
  keywords={Training;Performance evaluation;Adaptation models;Technological innovation;Generative AI;Large language models;Zero shot learning;Semantics;Manuals;Standards;Automatic Evaluation;LLM;Zero Shot;Generative AI},
  doi={10.1109/COMPAS60761.2024.10795949},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10563353,
  author={Rathi, Preeti and Budhani, Sandeep Kumar and Murari Upadhyay, Govind and Vats, Prashant and Kaur, Ranjeeta and Saini, Ashok Kumar},
  booktitle={2024 4th International Conference on Innovative Practices in Technology and Management (ICIPTM)}, 
  title={Unmasking Deepfakes: Understanding the Technology, Risks, and Countermeasures}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This research explores the transformative role of deepfake technology as a cutting-edge tool in the realm of Visual Effects (VFX) filmmaking. Deepfake, an artificial intelligence-driven technique, has evolved beyond its controversial origins to become an invaluable asset in creating hyper-realistic digital content. This paper delves into the intricacies of deepfake applications, focusing on its utilization as a powerful tool for seamlessly integrating computer-generated imagery (CGI) into live-action sequences. The study investigates the advancements in deepfake algorithms, emphasizing their ability to convincingly replicate human expressions, gestures, and voices. Through a comprehensive analysis of case studies and industry practices, we examine how deepfake technology has transcended traditional limitations, providing filmmakers with a versatile and efficient means of enhancing visual storytelling. Furthermore, the ethical considerations and challenges associated with the use of deepfake in filmmaking are discussed. As technology blurs the lines between reality and fiction, concerns regarding misinformation and the potential for misuse are addressed.},
  keywords={Industries;Deepfakes;Ethics;Machine learning algorithms;Statistical analysis;Focusing;Production;Deepfake;Synthetic media;Artificial intelligence;Machine learning;Neural networks;Generative adversarial networks (GANs);Audio-visual manipulation},
  doi={10.1109/ICIPTM59628.2024.10563353},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10499423,
  author={Zhong, Qijun},
  booktitle={2023 3rd International Signal Processing, Communications and Engineering Management Conference (ISPCEM)}, 
  title={Enhancing Interactive Storytelling: A Computational Approach to Autonomously Generating Role-Playing Game Scripts with Natural Language Processing}, 
  year={2023},
  volume={},
  number={},
  pages={714-719},
  abstract={In the realm of role-playing games (RPGs), the crafting of rich and detailed narratives is crucial, with legendary games like Dungeons & Dragons (D&D) setting the bar for creativity and player-driven storytelling. This research explores the innovative intersection of artificial intelligence and RPGs by developing a method for the automated generation of RPG scripts, harnessing the capabilities of advanced natural language processing (NLP). Our study presents an elaborate NLP framework, utilizing a comprehensive collection of D&D source materials, to methodically extract key character features (such as attributes, races, and classes) using machine learning techniques. This framework is further enhanced by integrating sentiment analysis and a sophisticated bi-directional recurrent neural network equipped with a multi-head attention mechanism. This combination is instrumental in accurately modeling narrative elements like event sequences, causal relationships, and thematic links. The culmination of this process is the automated generation of diverse and engaging RPG scripts, each characterized by distinct character dynamics, immersive settings, and compelling story arcs. The effectiveness and impact of these generated scripts were evaluated through a user study that gathered both qualitative and quantitative data. The findings demonstrate our system's proficiency in creating narratives that are not only contextually diverse but also resonate deeply with RPG conventions, indicating a significant advancement in interactive storytelling. This research not only reinforces the role of AI in augmenting game design but also underscores the broader potential of AI in creative content generation, marking a new era in the synergy between computational technology and artistic narrative creation.},
  keywords={Sentiment analysis;Analytical models;Recurrent neural networks;Computational modeling;Diversity reception;Games;Bidirectional control;Role-Playing Games;Natural Language Processing;Multi-head Attention;Custom-Tailored Sentiment analysis},
  doi={10.1109/ISPCEM60569.2023.00134},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10486537,
  author={Ratani, Snehi and Phatta, Disha and Phalke, Dhananjay and Varun, Nihaal and Aher, Anagha},
  booktitle={2024 IEEE International Conference on Computing, Power and Communication Technologies (IC2PCT)}, 
  title={Web Sculptor - Generative AI Based Comprehensive Web Development Framework}, 
  year={2024},
  volume={5},
  number={},
  pages={1729-1732},
  abstract={Modern web interfaces must seamlessly incorporate complex 3D animations to provide engaging online experiences. This is the era of creativity and technology fusion. With an approachable drag-and-drop interface, WebSculptor transforms web development by addressing issues with traditional UI builders. It makes it easier to create visually appealing interfaces by utilizing HTML, CSS, and JavaScript, and it uses MariaDB for effective database management. Web development becomes easier and more productive with the use of cutting-edge technologies such as HTML-to-JSX transformation and an AI-driven template suggestion system, which improve the code creation process. By personalizing template recommendations, the platform’s adaptive AI engine improves the caliber of web development.},
  keywords={Codes;Three-dimensional displays;Generative AI;Transforms;Production;Games;User interfaces;UI/UX;SentenceTransformer;Cosine similarity;Styling components},
  doi={10.1109/IC2PCT60090.2024.10486537},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{9836737,
  author={Fan, Jiaxin and Li, Yuhang and Liang, Bing and Ding, Youdong},
  booktitle={2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)}, 
  title={Self-Supervised Low-Light Image Enhancement based on Retinex Model}, 
  year={2022},
  volume={10},
  number={},
  pages={2138-2141},
  abstract={Self-supervised learning does not require any paired low-/normal-images in training, which greatly improves its application range. However, the existing self-supervised learning methods only set a single target brightness value, which reduces the brightness of the highlights in the image, resulting in image artifacts. In this paper, we propose a self-supervised learning method based on Retinex theory for low-light image enhancement. We convert the image to HSV color space to reduce the loss of color information, and the proposed value estimation network is used to generate the final reflectance(R), which is treated as the enhanced brightness component, combined with the hue(H) and saturation(S) component, and then converted back to the RGB color space to obtain the final enhanced image. In addition, we introduce an attention module to reduce artifacts caused by setting a single target brightness value. Experiments demonstrate that our method outperforms multiple existing state-of-the-art methods.},
  keywords={Training;Image color analysis;Conferences;Brightness;Estimation;Self-supervised learning;Information technology;Retinex;HSV;low-light;image enhancement;attention mechanism},
  doi={10.1109/ITAIC54216.2022.9836737},
  ISSN={2693-2865},
  month={June},}@INPROCEEDINGS{10652687,
  author={Hamza, Mohamed Magdi and Ashraf, Abdelaziz and Gomaa, Wael H.},
  booktitle={2024 Intelligent Methods, Systems, and Applications (IMSA)}, 
  title={Unlocking Arabic Script: OCR Technology for Efficient Text Extraction}, 
  year={2024},
  volume={},
  number={},
  pages={299-304},
  abstract={Optical Character Recognition (OCR) has evolved as a key technique for converting handwritten text and digits into editable and searchable digital representations. In this study, we investigate OCR for Arabic language documents, with an emphasis on handwritten letters and numerals. Despite developments in OCR, the complexity of Arabic script provide hurdles for reliable recognition. Our project intends to investigate post-processing approaches for improving the quality and usefulness of OCR results for handwritten Arabic content. Using sophisticated neural network designs, notably Convolutional Neural Networks (CNNs), we examine picture preprocessing, language-specific corrections, and layout analysis to improve OCR performance. Through empirical assessment, our model accomplished amazing accuracies of 98.5 percent for Arabic digits and 97.8 percent for Arabic letters, illustrating the viability of our comprehensive approach to digitizing manually written Arabic records and contributing to the progression of OCR innovation for multilingual applications.},
  keywords={Training;Handwriting recognition;Visualization;Accuracy;Text analysis;Optical character recognition;Neural networks},
  doi={10.1109/IMSA61967.2024.10652687},
  ISSN={},
  month={July},}@ARTICLE{10314996,
  author={Alarood, Ala Abdulsalam and Abubakar Ibrahim, Adamu and Alsubaei, Faisal S.},
  journal={IEEE Access}, 
  title={Attacks Notification of Differentiated Services Code Point (DSCP) Values Modifications}, 
  year={2023},
  volume={11},
  number={},
  pages={126950-126966},
  abstract={The DSCP is an integral component within the Internet Protocol (IP) header of a packet, serving the purpose of categorizing and administering network traffic, as well as facilitating the provision of Quality of Service (QoS) on IP networks. In the context of network communication, it is feasible for an adversary to transmit packets with a DSCP value of “x,” which represents a high priority. This action aims to prioritize the specified packet over other network traffic packets without triggering any notifications during the transmission session. It is possible to use identical DSCP values for both offensive and defensive purposes. This study therefore proposed a method for generating attack notifications in response to changes in DSCP values by using binary vectors to represent entries that detect attacks and those that do not. The method returns a list of Boolean values, each of which indicates whether or not the corresponding packet was classified as an attack. The study employed an experimental research methodology to generate transmission scenarios in which an attacker would attempt to transmit packets with a malicious DSCP value so that they would be prioritized over other traffic. A function was developed to detect deviation from normal and modification values involving DSCP value operations of normal traffic and generate alert. The finding of the experimental analysis indicates the vector, represents normal traffic because it does not have a DSCP value associated with an attack. The vectors representing spoofed, Assured Forwarding (AF), Class Selector (CS) and Expedited Forwarding (EF) respectively and generate an alert based on their values. This has contributed in detecting when an attacker tries to send packets with modified DSCP value in order to get them prioritized over the other packet on the normal traffic.},
  keywords={IP networks;Security;Quality of service;Telecommunication traffic;Inspection;Monitoring;Computer science;Vector processors;Cyberattack;Assured forwarding;class selector;differentiated services;expedited forwarding;vector space},
  doi={10.1109/ACCESS.2023.3332119},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9943224,
  author={Xiang, Tingmei and Huang, Feijun and Jiang, Wenna},
  booktitle={2022 International Conference on Artificial Intelligence and Autonomous Robot Systems (AIARS)}, 
  title={Anti-Counterfeiting Digital Pen for Multi-Dimensional Force Information Measurement Based on Biometric Technology}, 
  year={2022},
  volume={},
  number={},
  pages={433-436},
  abstract={Do you remember, in the James Bond movie, James Bond carried a smart pen with him. This is an amazing and fun pen that shoots narcotic darts and turns into a mini grenade. This pen looks amazing and is super fun. Some companies in the United Kingdom and the United States have set up some digital pens imitating the shape of the 007 agent pen, and have anti-counterfeiting functions. The smart pen is a ballpoint pen. When writing, the sensor in the pen will acquire the three-dimensional force, three-dimensional acceleration and inclination angle during use. These data are transmitted to the PC by wireless data communication lines. This is a unique, fun pen. The method used in this paper is biometric technology. With the development of Internet of Things technology, artificial intelligence, big data and other technologies, more and more network devices are integrated into our daily life. Only using traditional security solutions will inevitably have limitations. The latest biometric technology can improve the security in the authentication process to a certain extent.},
  keywords={Wireless communication;Wireless sensor networks;Force measurement;Biometrics (access control);Shape;Force;Writing;Biometric technology;multi-dimensional force perception;information measurement anti-counterfeiting digital pen},
  doi={10.1109/AIARS57204.2022.00104},
  ISSN={},
  month={July},}@INPROCEEDINGS{10704300,
  author={Yang, Ng Tze and Abdul Razak, Siti Fatimah and Yogarayan, Sumendra and Kamis, Noor Hisham},
  booktitle={2024 International Conference on Artificial Intelligence, Blockchain, Cloud Computing, and Data Analytics (ICoABCD)}, 
  title={Assessing Implications of Black Hole Attacks on VANET Performance}, 
  year={2024},
  volume={},
  number={},
  pages={90-94},
  abstract={VANET safety and security are critical for optimizing road traffic and reducing accidents. Vehicles travel between locations on a dynamic basis, and because there is minimum level of security architecture in place, routing protocol are open to multiple types of attacks. Black hole attacks are one of these risks; they affect the safety applications of VANET by allowing malicious nodes to enter the wireless network and selectively delete incoming packets. In this study, we use NS3 simulations to examine how black hole attack affect vehicular ad hoc networks (VANETs). Our experiments involve comparing network performance metrics, including throughput and delay, under two scenarios: with and without black hole attacks. By introducing malicious nodes that selectively drop or manipulate packets, we emulate real-world security threats faced by VANETs. Our findings demonstrate that the attacks double the average end-to-end delay, increase packet loss ratio, and decrease network throughput in a VANET environment. These results highlight how urgently trustworthy data transmission in VANET environments is required for reliable communication in intelligent transport systems.},
  keywords={Wireless networks;Vehicular ad hoc networks;Packet loss;Transportation;Throughput;Delays;Safety;Complexity theory;Security;Vehicle dynamics;VANET;security;AODV;intelligent transportation systems;mobility;vehicle nodes},
  doi={10.1109/ICoABCD63526.2024.10704300},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{8805871,
  author={Zheng, Jin and Zheng, Limin},
  booktitle={2019 International Conference on Communications, Information System and Computer Engineering (CISCE)}, 
  title={A Dictionary-Based Convolutional Recurrent Neural Network Model for Sentiment Analysis}, 
  year={2019},
  volume={},
  number={},
  pages={606-611},
  abstract={Natural Language Processing is an important direction in the field of computer science and artificial intelligence. It studies various theories and methods that can realize effective communication between human and computer in natural language. Sentiment analysis is a common and important task in natural language processing. It is often used to analyze and judge the sentiment types of text description. Movie reviews can provide valuable reference information for people to choose high quality movies. In order to effectively interpret movie reviews and understand the sentiment factors in movie reviews, the text proposes a deep learning method based on sentiment dictionary to realize the sentiment analysis of film reviews and to restore the real feelings of users as far as possible. The method first preprocesses the collected movie reviews data, uses the skip-gram model in the word2vec network to automatically generate word vectors, and transforms the sentiment vocabulary marked by Dalian University of Technology into word vectors. Then, the words vectors generated by text and dictionary are input into the convolutional neural network to learn sentence representation, extract local features. Finally, the Long Short-Term Memory network in recurrent neural network captures the semantic information and long-term dependencies between sentences, and inputs them into the logistic regression classifier to realize the sentiment analysis of movie reviews. The proposed method is compared with the Naive Bayesian and Support Vector Machine in the traditional machine learning method, the convolutional neural network and the recurrent neural network in the deep learning method. The experimental results show that the proposed model can extract more abundant features and achieve state-of-the-art classification effect than the baseline models.},
  keywords={Handheld computers;Neural networks;Deep learning;Conferences;Information systems;Sentiment analysis;Artificial Intelligence;Natural Language Processing;neural network;styling;sentiment analysis},
  doi={10.1109/CISCE.2019.00142},
  ISSN={},
  month={July},}@INPROCEEDINGS{10204743,
  author={Zhang, Wenxuan and Cun, Xiaodong and Wang, Xuan and Zhang, Yong and Shen, Xi and Guo, Yu and Shan, Ying and Wang, Fei},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation}, 
  year={2023},
  volume={},
  number={},
  pages={8652-8661},
  abstract={Generating talking head videos through a face image and a piece of speech audio still contains many challenges. i.e., unnatural head movement, distorted expression, and identity modification. We argue that these issues are mainly caused by learning from the coupled 2D motion fields. On the other hand, explicitly using 3D information also suffers problems of stiff expression and incoherent video. We present SadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation. To learn the realistic motion coefficients, we explicitly model the connections between audio and different types of motion coefficients individually. Precisely, we present ExpNet to learn the accurate facial expression from audio by distilling both coefficients and 3D-rendered faces. As for the head pose, we design PoseVAE via a conditional VAE to synthesize head motion in different styles. Finally, the generated 3D motion coefficients are mapped to the unsupervised 3D keypoints space of the proposed face render to synthesize the final video. We conducted extensive experiments to demonstrate the superiority of our method in terms of motion and video quality.11The code and demo videos are available at https://sadtalker.github.io.},
  keywords={Visualization;Computer vision;Three-dimensional displays;Codes;Face recognition;Animation;Faces;Image and video synthesis and generation},
  doi={10.1109/CVPR52729.2023.00836},
  ISSN={2575-7075},
  month={June},}@ARTICLE{10299625,
  author={AlMohimeed, Abdulaziz and Saad, Redhwan M. A. and Mostafa, Sherif and El-Rashidy, Nora Mahmoud and Farrag, Sarah and Gaballah, Abdelkareem and Elaziz, Mohamed Abd and El-Sappagh, Shaker and Saleh, Hager},
  journal={IEEE Access}, 
  title={Explainable Artificial Intelligence of Multi-Level Stacking Ensemble for Detection of Alzheimer’s Disease Based on Particle Swarm Optimization and the Sub-Scores of Cognitive Biomarkers}, 
  year={2023},
  volume={11},
  number={},
  pages={123173-123193},
  abstract={Alzheimer’s disease (AD) is a progressive neurological disorder characterized by memory loss and cognitive decline, affecting millions worldwide. Early detection is crucial for effective treatment, as it can slow disease progression and improve quality of life. Machine learning has shown promise in AD detection using various medical modalities. In this paper, we propose a novel multi-level stacking model that combines heterogeneous models and modalities to predict different classes of AD. The modalities include cognitive sub-scores (e.g., clinical dementia rating – sum of boxes, Alzheimer’s disease assessment scale) from the Alzheimer’s Disease Neuroimaging Initiative dataset. In the proposed approach, in level 1, we used six base models (Random Forest (RF), Decision Tree (DT), Support Vector Machine (SVM), Logistic Regression (LR), K-nearest Neighbors (KNN), and Native Bayes (NB)to train each modality (ADAS, CDR, and FQA). Then, we build stacking training that combines the outputs of each base model for the training set and staking testing that combines the outcomes of each model for the testing set. In level 2, three stacking models are produced for each modality that trains and evaluates based on the output of 6 base models based on (RF, LR, DT, SVM, KNN, and NB) are combined in training stacking for the training set and testing stacking for the testing set. Stacking training is used to train meta-learners (RF), and stacking testing is used to evaluate meta-learners (RF). Finally, in level 3, the output prediction of the stacking model from each modality (ADAS, CDR, and FQA) in the training and testing datasets is merged to build a new dataset, which is staking training and stacking testing. Training stacking is used to train the meta-learner, and the testing set is used to evaluate the meta-learner and produce the final prediction. Our research also aims to provide model explanations, ensuring efficiency, effectiveness, and trust through explainable artificial intelligence (XAI). Feature selection optimization based on Particle Swarm Optimization is used to select the most appropriate sub-scores. The proposed model shows significant potential for improving early disease diagnosis. The results demonstrate that the multi-modality approach outperforms single-modality approaches. Moreover, the proposed multi-level stacking models achieve the highest performance with selected features compared to regular ML classifiers and stacking models using full multi-modalities, achieving accuracy, precision, recall, and F1-scores of 92.08%, 92.07%, 92.08%, and 92.01% for two classes, and 90.03%, 90.19%, 90.03%, and 90.05% for three classes, respectively.},
  keywords={Stacking;Support vector machines;Magnetic resonance imaging;Diseases;Alzheimer's disease;Brain modeling;Data models;Machine learning;multi-level stacking machine learning;ensemble learning;sub scores of cognitive;Alzheimer’s disease;explainable artificial intelligence;particle swarm optimization},
  doi={10.1109/ACCESS.2023.3328331},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10024470,
  author={Dickson, Anthony and Shanks, Jeremy and Ventura, Jonathan and Knott, Alistair and Zollmann, Stefanie},
  booktitle={2022 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)}, 
  title={VRVideos: A flexible pipeline for Virtual Reality Video Creation}, 
  year={2022},
  volume={},
  number={},
  pages={199-202},
  abstract={Recent advances in Neural Radiance Field (NeRF)-based methods have enabled high-fidelity novel view synthesis for video with dynamic elements. However, these methods often require expensive hardware, take days to process a second-long video and do not scale well to longer videos. We create an end-to-end pipeline for creating dynamic 3D video from a monocular video that can be run on consumer hardware in minutes per second of footage, not days. Our pipeline handles the estimation of the camera parameters, depth maps, 3D reconstruction of dynamic foreground and static background elements, and the rendering of the 3D video on a computer or VR headset. We use a state-of-the-art visual transformer model to estimate depth maps which we use to scale COLMAP poses and enable RGB-D fusion with estimated depth data. In our preliminary experiments, we rendered the output in a VR headset and visually compared the method against ground-truth datasets and state-of-the-art NeRF-based methods.},
  keywords={Headphones;Visualization;Solid modeling;Three-dimensional displays;Pipelines;Estimation;Virtual reality;Virtual Reality;Image Processing;Computer Vision;Reconstruction;Artificial Intelligence;Vision and Scene Understanding},
  doi={10.1109/AIVR56993.2022.00039},
  ISSN={2771-7453},
  month={Dec},}@INPROCEEDINGS{8940386,
  author={Yu, Zhao and lin, Xun},
  booktitle={2019 First International Conference on ​Transdisciplinary AI (TransAI)}, 
  title={Short Paper: The Impact of Environmental Education with Immersive Environment on Learning Performance and Environmental Identity}, 
  year={2019},
  volume={},
  number={},
  pages={18-21},
  abstract={Nowadays, the technology of immersive environment and multimodal interaction like the virtual reality (VR) devices provide people with a way to visualize the natural environment. These applications in the field of environmental education will contribute to the study of their positive role in solving societal issues around AI (ethical, legal, and social implications). Users can use it to create a real experience in the environment, not just the images they see. This empirical study suggests that an environmental education programs by immersive and normal environmental films can be effective in helping to improving learning performance and environmental identity. The method of analysis is quantitative analysis, combined with qualitative. The experimental group used immersive applications to view VR video clips while the control group used a desktop computer to observe the same video clips. The subjects were given the same questionnaire after video viewing. The questionnaire included environmental knowledge related to video clips and the Connectedness to Nature scale to measure participants' trait levels of feeling emotionally connected to the natural world. The results answered the research questions well and supported these hypotheses: (1) The use of VR equipment will affect the learning performance of the subjects. The learning effect of the experimental group is better than that of the control group. (2) The use of VR equipment could improve the environmental identity of the subjects. Compared with the control group, the result of scale in the experimental group showed a higher level of environmental identity.},
  keywords={Education;Earth;Statistical analysis;Atmospheric measurements;Particle measurements;Standards;Films;Immersive environment;Multimodal interaction;Connectedness to Nature Scale;Environmental Education},
  doi={10.1109/TransAI46475.2019.00011},
  ISSN={},
  month={Sep.},}@ARTICLE{10308700,
  author={Shen, Leixian and Zhang, Yizhi and Zhang, Haidong and Wang, Yun},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Data Player: Automatic Generation of Data Videos with Narration-Animation Interplay}, 
  year={2024},
  volume={30},
  number={1},
  pages={109-119},
  abstract={Data visualizations and narratives are often integrated to convey data stories effectively. Among various data storytelling formats, data videos have been garnering increasing attention. These videos provide an intuitive interpretation of data charts while vividly articulating the underlying data insights. However, the production of data videos demands a diverse set of professional skills and considerable manual labor, including understanding narratives, linking visual elements with narration segments, designing and crafting animations, recording audio narrations, and synchronizing audio with visual animations. To simplify this process, our paper introduces a novel method, referred to as Data Player, capable of automatically generating dynamic data videos with narration-animation interplay. This approach lowers the technical barriers associated with creating data videos rich in narration. To enable narration-animation interplay, Data Player constructs references between visualizations and text input. Specifically, it first extracts data into tables from the visualizations. Subsequently, it utilizes large language models to form semantic connections between text and visuals. Finally, Data Player encodes animation design knowledge as computational low-level constraints, allowing for the recommendation of suitable animation presets that align with the audio narration produced by text-to-speech technologies. We assessed Data Player's efficacy through an example gallery, a user study, and expert interviews. The evaluation results demonstrated that Data Player can generate high-quality data videos that are comparable to human-composed ones.},
  keywords={Videos;Data visualization;Animation;Visualization;Interviews;Electronic mail;Data models;Visualization;Narration-animation interplay;Data video;Human-AI collaboration},
  doi={10.1109/TVCG.2023.3327197},
  ISSN={1941-0506},
  month={Jan},}@INPROCEEDINGS{10731272,
  author={Maehigashi, Akihiro and Fukuchi, Yosuke and Yamada, Seiji},
  booktitle={2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN)}, 
  title={Empirical investigation of how robot head motion influences acceptance of heatmap-based XAI: Designing XAI with social robot}, 
  year={2024},
  volume={},
  number={},
  pages={344-350},
  abstract={This study investigated how a robot head motion towards an AI attention heatmap during a visual identification task influences a human user’s trust in eXplainable AI (XAI). The findings revealed that the robot head motion presented in a video increased the user’s acceptance of AI-generated results compared to the robot eye gaze displayed in a static image with or without the AI attention heatmap. However, displaying the heatmap improved task performance more than displaying no heatmap with or without the robot. Overall, these results suggest a possibility that showing a robot head motion towards an AI attention heatmap in a movie can serve as an interpretable XAI for visual tasks.},
  keywords={Heating systems;Visualization;Head;Explainable AI;Social robots;Motion pictures;Behavioral sciences;Artificial intelligence;Robots},
  doi={10.1109/RO-MAN60168.2024.10731272},
  ISSN={1944-9437},
  month={Aug},}@ARTICLE{9139251,
  author={Yu, Cheng and Wang, Wenmin and Yan, Jianhao},
  journal={IEEE Access}, 
  title={Self-Supervised Animation Synthesis Through Adversarial Training}, 
  year={2020},
  volume={8},
  number={},
  pages={128140-128151},
  abstract={In this paper, we propose a novel deep generative model for image animation synthesis. Based on self-supervised learning and adversarial training, the model can find labeling rules and mark them without origin sample labels. In addition, our model can generate continuous changing images based on the automatically labels learning. The labels learning model can be implemented on a large number of out-of-order samples to generate two types of pseudo-labels, discrete labels and continuous labels. The discrete labels can generate different animation clips, and the continuous labels can generate different frames in the same clip. Embedding pseudo-labels with latent variables into latent space, our model discovers regularities and features from latent space. Animation features are fully characterized by the pseudo-labels learned from the self-supervised module. Using upgraded adversarial training steps, the model learns to map animation features to pseudo-labels from the latent space and then organizes pseudo-labels embedding into latent variables to generate animation features. By adapting dimensions of pseudo-labels, we match fine features with latent variables. Such as using the two types of pseudo-labels, our model can also generate different styles of videos from the same dataset. The specific implementation tricks depend on the different pseudo-label dimensions and the number of pseudo-label dimensions. Comparing the results of our model with other state-of-the-art approaches, the model does not use complicated components, such as 3D convolution layers and recurrent neural networks. Our experimental results show that an appropriate number of the pseudo-label dimensions can better characterize animation features. In this case, an animation which reached human-level perception can be synthesized. The performance of animation synthesis has reached relatively superior results on several challenging datasets.},
  keywords={Animation;Training;Videos;Generative adversarial networks;Task analysis;Image synthesis;Gallium nitride;Self-supervised learning;animation synthesis;pseudo-label;adversarial training},
  doi={10.1109/ACCESS.2020.3008523},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10920245,
  author={Suo, Xiaohua and Zhang, Hui and Xu, Feibing and Li, Yidong},
  booktitle={2024 IEEE 27th International Conference on Intelligent Transportation Systems (ITSC)}, 
  title={LSTV-V2V: A Large-Scale Traffic Virtual Dataset for Vehicle-to-Vehicle Cooperative Perception}, 
  year={2024},
  volume={},
  number={},
  pages={2096-2103},
  abstract={Cooperative perception system is essential to address occlusion and small object perception issues in autonomous driving. However, autonomous vehicle (AV) perception methods based on deep learning rely on large-scale and accurately annotated traffic datasets. Compared to real datasets, which are difficult to collect and annotate and limited in quantity and diversity of scenes, the virtual dataset synthesis method is a convenient and efficient approach. To improve annotation accuracy while speeding up modeling, we propose a pipeline for constructing artificial traffic scenes and generating virtual datasets based on autonomous driving simulation software from a parallel vision perspective. Furthermore, to facilitate the development of cooperative perception, we propose a novel efficient traffic scene dataset for V2V cooperative perception named Large-Scale Traffic Virtual Dataset (LSTV-V2V). Our dataset contains 47 traffic scenarios, 8380 frames, and 191864 annotated 3D vehicle bounding boxes. Experimental results demonstrate the effectiveness of our virtual dataset in cooperative perception tasks.},
  keywords={Deep learning;Three-dimensional displays;Annotations;Pipelines;Vehicular ad hoc networks;Object detection;Software;Autonomous vehicles;Vehicle-to-everything;Intelligent transportation systems;Parallel Intelligence;Virtual Dataset;Cooperative Perception;Autonomous Driving},
  doi={10.1109/ITSC58415.2024.10920245},
  ISSN={2153-0017},
  month={Sep.},}@INPROCEEDINGS{10837618,
  author={Mormul, Yevhenii and Przybyszewski, Jan and Nakoud, Andrew and Cuffe, Paul},
  booktitle={2024 21st International Conference on Information Technology Based Higher Education and Training (ITHET)}, 
  title={Reliance on Artificial Intelligence Tools May Displace Research Skills Acquisition Within Engineering Doctoral Programmes: Examples and Implications}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={The escalation in capabilities of Large Language Models has triggered urgent discussions about their implications for tertiary education, particularly regarding how they might facilitate academic misconduct in graded engineering coursework. However, graduate research education — where a student works closely with a supervisor over years to develop both implicit and explicit research skills — has received comparatively less attention in this discussion. This paper seeks to develop this discourse by presenting targeted case studies that explore the opportunities and threats posed by artificial intelligence to engineering doctoral education. For instance, using a specimen exercise from a PhD-level research skills module, we demonstrate how artificial intelligence tools can now deeply penetrate research workflows in technical computing and scripting. We likewise investigate the capabilities of chatbot tools to assist engineering PhD candidates with the broader research skills central to their training and development. These include writing and proofreading theses and research papers, producing data visualizations, simulating peer review processes, and preparing scientific diagrams. By evaluating the capabilities and limitations of extant artificial intelligence in these areas, we can discuss both the potential benefits and ethical concerns of doctoral students engaging with such assistance.},
  keywords={Training;Ethics;Reviews;Large language models;Data visualization;Medical services;Artificial intelligence;Information technology;Standards;Faces},
  doi={10.1109/ITHET61869.2024.10837618},
  ISSN={2473-2060},
  month={Nov},}@INPROCEEDINGS{10866940,
  author={Srivastava, Yashi and Almusawi, Muntather and Badhoutiya, Arti and Dhasmana, Gulshan and Jain, Alok and Aishwarya, B K and Boob, Nandini Shirish and Srivastava, Arun Pratap},
  booktitle={2024 1st International Conference on Sustainable Computing and Integrated Communication in Changing Landscape of AI (ICSCAI)}, 
  title={Photovoltaic Panel Quality Evaluation Based on Power Generated Involving Copper Indium Gallium Selenide Solar Cells}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={When it comes to energy-efficient manufacturing, this report tackles some of the most important concerns raised by companies and academic institutions. Both production management and the integration of information and communication technology systems with respect to energy efficiency performance criteria are encompassed in the research question (RQ). The requirements of the corporate sector were considered alongside solutions that had previously been detailed in the literature. Findings show that methods exist for keeping tabs on, managing, and improving production processes. Organizational, facility, and procedure-level energy management in manufacturing is an area where these methods are seldom effective. A gap persists in industrial organizations between possible solutions and their actual execution. To close the gap between theory and practise, research into effective and efficient energy management in manufacturing is necessary. Managers in the manufacturing sector may use this data to make more informed, faster business decisions based on real energy usage. Key performance indicators (KPIs) for energy efficiency should be defined and specified in a way that allows for quantitative measurement at the plant level. Seeing how energy efficiency stacks up against other metrics is essential for decision-makers looking at production performance areas from multiple angles.},
  keywords={Photovoltaic systems;Gallium;II-VI semiconductor materials;Photovoltaic cells;Solar energy;Terbium;Energy efficiency;Manufacturing;Indium;Cadmium compounds;solar;energy;electricity;power;renewable energy},
  doi={10.1109/ICSCAI61790.2024.10866940},
  ISSN={},
  month={July},}@INPROCEEDINGS{10592379,
  author={Khadmaoui-Bichouna, Mohamed and Alcaraz-Calero, Jose M. and Wang, Qi},
  booktitle={2024 International Wireless Communications and Mobile Computing (IWCMC)}, 
  title={5G RAN service classification using Long Short Term Memory Neural Network}, 
  year={2024},
  volume={},
  number={},
  pages={467-472},
  abstract={5G brings many benefits such as enlarged capacity and improved connectivity. However, it also poses challenges especially due to a significant increase in the amount of traffic on the network. This creates difficulties for operators to maintain the Quality of Service (QoS) for each of the services offered. Therefore, in order to improve the performance of such capabilities and, consequently, the experience of the users, it is necessary to identify which traffic requires more prioritisation. This would help allocate more resources to those services. This concept makes the identification and classification of traffic to gain more and more relevance and importance. In this paper, we propose a Long Short-Term Memory (LSTM) model to classify 5G Radio Access Network (RAN) behaviour into four different scenarios: streaming, video conferencing, Voice over IP (VoIP) and gaming. The results obtained show a $93 \%$ accuracy.},
  keywords={Measurement;Wireless communication;Accuracy;5G mobile communication;Neural networks;Quality of service;Software;5G;Artificial Intelligence;LSTM;Deep Learning;Service Classification},
  doi={10.1109/IWCMC61514.2024.10592379},
  ISSN={2376-6506},
  month={May},}@ARTICLE{10501937,
  author={Balasubramanian, Karthikeyan Kalyanasundaram and Salvo, Mirco Di and Rocchia, Walter and Decherchi, Sergio and Crepaldi, Marco},
  journal={IEEE Access}, 
  title={Designing RISC-V Instruction Set Extensions for Artificial Neural Networks: An LLVM Compiler-Driven Perspective}, 
  year={2024},
  volume={12},
  number={},
  pages={55925-55944},
  abstract={The demand for Artificial Intelligence (AI) based solutions is exponentially increasing in all application fields, including low-power devices on the edge. However, due to their limited computational capabilities, these devices, which run Central Processing Units (CPUs) tailored to embedded applications, are typically not optimized to run complex neural networks. Providing ad-hoc extensions to the instruction set architecture of a RISC-V processor can be a viable solution to address this issue. In this work, we propose the use of the PyTorch Graph Lowering (Glow)–LLVM toolchain to understand the impact of compiled code of AI models on a RISC-V machine and extend its instruction set to improve runtime performance. This approach allows code profiling, detection of computational bottlenecks, and provisioning of the necessary CPU enhancements to be implemented in the LLVM backend before hardware implementation. After profiling known Artificial Neural Networks (ANNs) quantized to int8 (particularly, a single perceptron, RESNET18, VGG11, and LENET5), we have identified and devised three additional instructions, we named LWM, LWA and LWS respectively indicating Load Word-and-Multiply, -Add, and -Subtract. As a result, we obtained an edge AI-oriented, significantly improved processor description in terms of inference time and program density, ready to be hardware-designed. For  $128\times 128$  RGB images, the custom extensions enabled up to  $13\times $  speed up compared to RV32I and  $5\times $  compared to RV32IM, with a maximum of 11.7% lower code. Together with these findings the paper systematically highlights the main methodological steps to include new instructions in an LLVM backend.},
  keywords={Artificial intelligence;Hardware;Codes;Registers;Computer architecture;Standards;Computational modeling;Reduced instruction set computing;Hardware design languages;RISC-V;AI;RISC-V custom instructions extensions;LLVM;instruction set architecture;hardware-software co-design},
  doi={10.1109/ACCESS.2024.3389673},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10878667,
  author={Liang, Ruihong and Wang, Hui and Gao, ShengJie and Chen, Jing},
  booktitle={2024 11th International Forum on Electrical Engineering and Automation (IFEEA)}, 
  title={Design and Implementation of an AI-Based Enterprise Mobile Platform Automated Testing System}, 
  year={2024},
  volume={},
  number={},
  pages={1365-1372},
  abstract={As enterprise mobile applications proliferate, ensuring their quality and security has become increasingly essential. Common challenges include content duplication, security vulnerabilities, sensitive data leaks, and compatibility issues across diverse devices, all of which can harm user experience and corporate reputation. This paper presents an AI-driven automated testing system designed to address these issues. The system integrates advanced technologies such as OCR, CNN, NLP, BERT, Twin Network Model, Isolation Forest, and K-Means Clustering to analyze page elements, intelligently select appropriate test components, and generate automated testing scripts. By dynamically refining testing strategies based on feedback and results, the system delivers a reliable and efficient quality assurance framework, empowering enterprises to meet the demands of digital transformation.},
  keywords={Analytical models;Quality assurance;Digital transformation;Optical character recognition;Forestry;User experience;Security;Reliability;Time factors;Testing;AI feature analysis;automated testing;mobile platform applications;deep learning},
  doi={10.1109/IFEEA64237.2024.10878667},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10867088,
  author={Mohammed, Shehu and Malhotra, Neha},
  booktitle={2024 4th International Conference on Ubiquitous Computing and Intelligent Information Systems (ICUIS)}, 
  title={Machine Learning and Artificial Intelligence for Enhanced Alzheimer's Disease Detection: A State-of-the-Art Review}, 
  year={2024},
  volume={},
  number={},
  pages={19-26},
  abstract={Alzheimer's disease (AD) is a substantial public health challenge due to its rising prevalence on a global scale. Timely identification and tailored treatment are essential for enhancing patient results and maximizing healthcare resources. This paper offers a thorough examination of the latest progress in applying machine learning (ML) and artificial intelligence (AI) methods to improve Alzheimer's disease (AD) detection and customize care tactics. We will first discuss the etiology of Alzheimer's disease and emphasize the need of early identification. We will explore different machine learning and artificial intelligence techniques such as deep learning, support vector machines, and ensemble approaches used for Alzheimer's disease detection and prognosis. We investigate combining many types of data sources, including neuroimaging, genetic markers, and clinical assessments, to improve predictive models. We also examine how AI-driven decision support systems help create tailored care plans and optimize treatment for people with Alzheimer's disease. We discuss present obstacles and prospective paths in the area, such as data diversity, model explainability, and ethical concerns. This study seeks to offer insights into the current state of machine learning (ML) and artificial intelligence (AI) applications in Alzheimer's disease (AD) research and clinical practice, with the goal of advancing early diagnosis and individualized management strategies for this severe neurological ailment.},
  keywords={Support vector machines;Ethics;Machine learning algorithms;Reviews;Soft sensors;Medical treatment;Collaboration;Machine learning;Ubiquitous computing;Alzheimer's disease;Machine Learning;Artificial Intelligence;Alzheimer's Disease;Early Detection;Personalized Care;Neuroimaging;Decision Support Systems},
  doi={10.1109/ICUIS64676.2024.10867088},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10377007,
  author={Azadi, Samaneh and Shah, Akbar and Hayes, Thomas and Parikh, Devi and Gupta, Sonal},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation}, 
  year={2023},
  volume={},
  number={},
  pages={14993-15002},
  abstract={Text-guided human motion generation has drawn significant interest because of its impactful applications spanning animation and robotics. Recently, application of diffusion models for motion generation has enabled improvements in the quality of generated motions. However, existing approaches are limited by their reliance on relatively small-scale motion capture data, leading to poor performance on more diverse, in-the-wild prompts. In this paper, we introduce Make-An-Animation, a text-conditioned human motion generation model which learns more diverse poses and prompts from large-scale image-text datasets, enabling significant improvement in performance over prior works. Make-An-Animation is trained in two stages. First, we train on a curated large-scale dataset of (text, static pseudo-pose) pairs extracted from image-text datasets. Second, we fine-tune on motion capture data, adding additional layers to model the temporal dimension. Unlike prior diffusion models for motion generation, Make-An-Animation uses a U-Net architecture similar to recent text-to-video generation models. Human evaluation of motion realism and alignment with input text shows that our model reaches state-of-the-art performance on text-to-motion generation. Generated samples can be viewed at https://azadis.github.io/make-an-animation.},
  keywords={Solid modeling;Computer vision;Three-dimensional displays;Dynamics;Computer architecture;Animation;Motion capture},
  doi={10.1109/ICCV51070.2023.01381},
  ISSN={2380-7504},
  month={Oct},}@INPROCEEDINGS{10143933,
  author={Ai, Zihao and Su, Junbo and Zheng, Xuefei},
  booktitle={2023 International Conference on Artificial Intelligence and Education (ICAIE)}, 
  title={Teaching Students How to Design an Adventure Game with Web3D Technology}, 
  year={2023},
  volume={},
  number={},
  pages={91-96},
  abstract={In recent years, due to the prosperity of the game industry, many students hope to take game development as their future work, and they have applied for college majors related to game development. The gradually improved discipline construction of game development in various colleges and universities, coupled with the huge impact of the pandemic, renders the teaching having been changing from offline, face-to-face teaching to online, video teaching. How to design an online teaching so that teachers can teach students game development basics is of utmost significance. In this paper, we designed an online teaching scheme, by applying which students can easily and successfully acquire knowledge from finishing a small game project utilizing Web3D technology. In this simple game project for teaching, the characters can be manipulated to avoid the attack of an intelligent NPC enemy located in an indoor scene.},
  keywords={Knowledge engineering;Industries;Computer science;Distance learning;Pandemics;Education;Games;Teaching Scheme;Online Teaching;Game Development;Web3D},
  doi={10.1109/ICAIE56796.2023.00033},
  ISSN={},
  month={March},}@INPROCEEDINGS{10724260,
  author={Niaz Ahamed, V.M. and Sivaraman, K},
  booktitle={2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={VANET Congestion Control and Stability Performance Analysis using Seagull Optimization Deep Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Significant matters in VANET environment include high vehicle versatility, changing vehicle thickness and dynamics between vehicle separating. Therefore, improved routing protocol enhances overall performance of VANET through intermittent service availability. In this regard, a group-based AI technique is implemented to predict VANET mobility. Effective routing based on a hybrid metaheuristic algorithm combined with Ensemble Learning produces significantly improved results. Vehicle Ad Hoc Networks (VANETs) are currently used in various applications such as transportation using vehicle-to-vehicle multi hop wireless communication since they offer scalable and cost-effective solutions for it. On the other side, multi-hop communication development is regarded as a challenging task in VANET conditions due to rapid alterations of periodic and topological network breakdowns. In general, mobile ad hoc routing protocols, this leads to failure of routing or its invalidation. In order to solve the problem of routing and get multi-constrained QoS (quality of service) measures in VANETs, numerous research works have been conducted on it. Another aspect for application is traffic collision control. Unfortunately, there are hardly any sensors anywhere which make it basically impossible to obtain continuous information about traffic states. Accurate predictions about traffic flow are difficult because road accidents and public events cannot be predicted in advance and both these can essentially affect the traffic flow. This article reports the framework of Seagull Optimization Deep Learning for predicting accuracy and experiments various nodes. This framework exhibition focuses on different vehicle flows, position rates, and road organization typologies. Our framework effectively reduces the general waiting time of vehicles during aggregation and shortens the vehicle travel season even when the location rate is low.},
  keywords={Deep learning;Accuracy;Vehicular ad hoc networks;Quality of service;Organizations;Routing;Routing protocols;Sensors;Artificial intelligence;Testing;VANET;Deep Learning;Congestion Control;vehicle distribution;Application of systems;vehicle density and speed;routing optimization},
  doi={10.1109/ICCCNT61001.2024.10724260},
  ISSN={2473-7674},
  month={June},}@INPROCEEDINGS{10808057,
  author={Paweroi, Rio Mukhtarom and Köppen, Mario},
  booktitle={2024 2nd International Conference on Intelligent Metaverse Technologies & Applications (iMETA)}, 
  title={Framework for Integration of Generative AI into Metaverse Asset Creation}, 
  year={2024},
  volume={},
  number={},
  pages={027-033},
  abstract={Metaverse, a virtual world, is developing rapidly and is widely used in multi-sector. The number of users is projected to increase year over year. Due to the development of the metaverse platform, digital asset creation is demanding. Creating high-quality and diverse 3D digital objects is challenging. This study proposes the frameworks for integrating generative AI to create diverse 3D assets into the metaverse. We study different approaches for asset creation, i.e., generative 3D model-based, generative image projection-based, and generative language script-based. Creators can use this workflow to optimize the creation of 3D assets. Moreover, this study compares the results of generative AI and procedural generation on generating diverse 3D objects. The result shows that generative AI can simplify 3D creation and generate more diverse objects.},
  keywords={Solid modeling;Three-dimensional displays;Procedural generation;Generative AI;Metaverse;Complexity theory;Usability;Metaverse;3D Asset Creation;Generative AI;3D Asset Diversity},
  doi={10.1109/iMETA62882.2024.10808057},
  ISSN={},
  month={Nov},}@ARTICLE{9966322,
  author={Bairi, Zakaria and Ben-Ahmed, Olfa and Amamra, Abdenour and Bradai, Abbas and Beghdad Bey, Kadda},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={PSCS-Net: Perception Optimized Image Reconstruction Network for Autonomous Driving Systems}, 
  year={2023},
  volume={24},
  number={2},
  pages={1564-1579},
  abstract={The progress achieved in transportation systems and artificial intelligence has amplified the use of intelligent transportation systems and Autonomous Vehicles (AVs). Indeed, AV systems have attracted much research in recent years, which enabled multiple autonomous driving tasks, including scene understanding, visual prediction, decision-making, and communication. The latter may create a bottleneck in low-resource autonomous driving systems that send the collected images to remote edge servers for processing and decision-making. Such an issue can be addressed by compressing the images in the AV and ensuring a good-quality reconstruction at the edge. In this paper, we propose a deep neural network for Compressed Sensing (CS) based image reconstruction that integrates image semantic perception to improve the reconstruction process for visual prediction tasks. The reconstruction process is optimized using a perception-inspired loss in an end-to-end model learning process. The trained model is evaluated on autonomous driving car datasets. Obtained experimental results outperform state-of-the-art approaches in terms of both image reconstruction quality and processing time. Finally, we perform semantic urban scene segmentation on the reconstructed image to evaluate reconstruction quality for visual task prediction. Obtained results on three semantic urban scene datasets demonstrate the efficiency of the proposed approach.},
  keywords={Image reconstruction;Image coding;Autonomous vehicles;Semantics;Visualization;Sensors;Task analysis;Image;autonomous vehicles;perception;compressed sensing;auto-encoder;deep learning},
  doi={10.1109/TITS.2022.3223167},
  ISSN={1558-0016},
  month={Feb},}@ARTICLE{10038651,
  author={Lu, Zhenyu and Shu, Wanneng and Li, Yan},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={An Anti-Collision Algorithm for Self-Organizing Vehicular Ad-Hoc Network Using Deep Learning}, 
  year={2024},
  volume={25},
  number={3},
  pages={2779-2791},
  abstract={The rapid increase of the number of motor vehicles over the last several decades has driven a corresponding increase in the severity of traffic congestion, which has exhibited a considerable human impact. Intelligent anti- collision control via inter-vehicle communication technology can facilitate vehicles adhering to spacing speed standards and improve road utilization and traffic efficiency. In this paper, we apply a deep learning image estimation model based on joint attention mechanism. The network framework uses a deep estimation network Yolov5 and a location based VANET information fast transmission strategy to work together. This paper mainly considers vehicle distance measurement technology as a point of entry to study multi-sensor information fusion for vehicle collision prevention technology based on the self-organizing vehicular ad-hoc network (VANET). This paper also proposes a solution based on the strategy of one-way transmission of shared information and dynamic valuation of a cluster distance threshold with vehicle density. The proposed vehicle anti-collision control algorithm is designed to realize dynamic vehicle control via inter-vehicle communication. In this paper, the two-way coupling of traffic flow and network simulator is used to randomly generate vehicle nodes on the road, and the behavior of the anti-collision system is simulated. The experimental results show that the predetermined control goal is achieved, which demonstrates the effectiveness of the proposed algorithm.},
  keywords={Vehicular ad hoc networks;Distance measurement;Roads;Feature extraction;Estimation;Heuristic algorithms;Deep learning;Self-organizing vehicular ad-hoc network;multi-sensor;Yolov5 network;deep learning;information fusion;distance measurement;vehicle anti-collision},
  doi={10.1109/TITS.2023.3241148},
  ISSN={1558-0016},
  month={March},}@INPROCEEDINGS{10507445,
  author={Chen, Yikang},
  booktitle={2023 9th International Conference on Computer and Communications (ICCC)}, 
  title={Efficient Track Association of Ships through AIS and shipboard Video Fusion via Ad Hoc Networks}, 
  year={2023},
  volume={},
  number={},
  pages={361-367},
  abstract={This paper presents a shipboard equipment-based method that fuses multi-camera and AIS sensing information, enabling intelligent sensing and recognition of encountered ships. This approach maximizes the strengths of both AIS and shipboard cameras, achieving more precise, comprehensive, and rapid sensing of surrounding vessels. To address issues with measurement accuracy and range in existing methods, a dual-camera ship-ranging technique is proposed for spatial localization using video image data. Utilizing multi-camera setups, ship targets are detected and tracked. The ship detection algorithm provides target frame information, which is then used in the dual-camera ranging method to convert ship pixel coordinates into the world coordinate system. To improve track correlation rates, a novel two-branch track correlation network, with an LSTM backbone network, is introduced, enabling ship track correlation through deep learning. By establishing an AIS and video track association model based on deep learning algorithms and creating a ship track association dataset, an efficient and reliable track association process is achieved.},
  keywords={Deep learning;Correlation;Target tracking;Position measurement;Distance measurement;Sensors;Velocity measurement;Multi-camera;AIS sensing;Shipboard equipment;Ship ranging},
  doi={10.1109/ICCC59590.2023.10507445},
  ISSN={2837-7109},
  month={Dec},}@INPROCEEDINGS{8798208,
  author={Danieau, Fabien and Gubins, Ilja and Olivier, Nicolas and Dumas, Olivier and Denis, Bernard and Lopez, Thomas and Mollet, Nicolas and Frager, Brian and Avril, Quentin},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Automatic Generation and Stylization of 3D Facial Rigs}, 
  year={2019},
  volume={},
  number={},
  pages={784-792},
  abstract={In this paper, we present a fully automatic pipeline for generating and stylizing high geometric and textural quality facial rigs. They are automatically rigged with facial blendshapes for animation, and can be used across platforms for applications including virtual reality, augmented reality, remote collaboration, gaming and more. From a set of input facial photos, our approach is to be able to create a photorealistic, fully rigged character in less than seven minutes. The facial mesh reconstruction is based on state-of-the art photogrammetry approaches. Automatic landmarking coupled with ICP registration with regularization provide direct correspondence and registration from a given generic mesh to the acquired facial mesh. Then, using deformation transfer, existing blendshapes are transferred from the generic to the reconstructed facial mesh. The reconstructed face is then fit to the full body generic mesh. Extra geometry such as jaws, teeth and nostrils are retargeted and transferred to the character. An automatic iris color extraction algorithm is performed to colorize a separate eye texture, animated with dynamic UVs. Finally, an extra step applies a style to the photorealis-tic face to enable blending of personalized facial features into any other character. The user's face can then be adapted to any human or non-human generic mesh. A pilot user study was performed to evaluate the utility of our approach. Up to 65% of the participants were successfully able to discern the presence of one's unique facial features when the style was not too far from a humanoid shape.},
  keywords={Face;Pipelines;Cameras;Three-dimensional displays;Geometry;Strain;Computational modeling;I.2.10 [artificial intelligence]: Vision and Scene Understanding—Intensity, color, photometry, and thresholding;I.3.7 [computer graphics]: Three-Dimensional Graphics and, Realism—Animation;character;animation;pipeline;virtual reality},
  doi={10.1109/VR.2019.8798208},
  ISSN={2642-5254},
  month={March},}@ARTICLE{10464342,
  author={Zhang, Jiale and Liu, Chengxin and Xian, Ke and Cao, Zhiguo},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Hierarchical Feature Warping and Blending for Talking Head Animation}, 
  year={2024},
  volume={34},
  number={8},
  pages={7301-7314},
  abstract={Talking head animation transforms a source anime image to a target pose, where the transformation includes the change of facial expression and head movement. In contrast to existing approaches that operate on the low-resolution image ( $256\times 256$ ), we study this task at a higher resolution, e.g.,  $512\times 512$ . High-resolution talking head animation, however, raises two major challenges: i) how to achieve smooth global transformation while maintaining rich details of anime characters under large-displacement pose variations; ii) how to address the shortage of data, because no related dataset is publicly available. In this paper, we present a Hierarchical Feature Warping and Blending (HFWB) model, which tackles talking head animation hierarchically. Specifically, we use low-level features to control global transformation and high-level features to determine the details of anime characters, under the guidance of feature flow fields. These features are then blended by selective fusion units, outputting transformed anime images. In addition, we construct an anime pose dataset—AniTalk-2K, aiming to alleviate the shortage of data. It contains around 2000 anime characters with thousands of different face/head poses at a resolution of  $512\times 512$ . Extensive experiments on AniTalk-2K demonstrate the superiority of our approach in generating high-quality anime talking heads over state-of-the-art methods.},
  keywords={Image resolution;Generators;Animation;Iterative methods;Generative adversarial networks;Pose estimation;Image processing;Talking head animation;generative adversarial networks;pose transformation;anime image generation;anime dataset},
  doi={10.1109/TCSVT.2024.3375330},
  ISSN={1558-2205},
  month={Aug},}@INPROCEEDINGS{10746990,
  author={Alahmed, Yazan and Abadla, Reema and Al Ansari, Mohammed Jassim},
  booktitle={2024 Fifth International Conference on Intelligent Data Science Technologies and Applications (IDSTA)}, 
  title={Enhancing Safety in Autonomous Vehicles through Advanced AI-Driven Perception and Decision-Making Systems}, 
  year={2024},
  volume={},
  number={},
  pages={208-217},
  abstract={Accidents, congested roads, consumption of energy, and emissions may all decrease significantly with the rise of self-driving cars, providing a promising response to social and environmental issues. In this study, researchers investigate the way the integration of innovative AI-driven perception and decision-making systems into AVs impacts safety. AVs have the potential to change transportation by reducing accidents caused mainly by human error. They may operate on their own or in conjunction with human drivers. The primary goal is to investigate and enhance AV safety by creating highly sophisticated perception and decision-making technologies driven by machine learning. Pragmatism research philosophy, experimental research design, and inductive approach serve as the selected methods of the study. In addition, secondary qualitative data analysis methods help evaluate the entire study. The outcomes of the study demonstrated that the advancement of autonomous vehicles depends significantly on the creation of AI-driven sensors. “Vehicle-to-vehicle (V2X) networks” for communication and safety features increase security, while integrated camera systems with acoustic and thermal sensors improve sensing capacities. Deep learning techniques, especially “convolutional neural networks (CNN)” and “fully convolutional networks (FCN)”, facilitate accurate object recognition and segmentation. Vehicles’ ability to handle difficult road conditions is further improved by a real-time risk evaluation and trajectory planning based on human-like behavioral modeling.},
  keywords={Decision making;Vehicular ad hoc networks;Thermal sensors;Real-time systems;Sensor systems;Safety;Autonomous automobiles;Convolutional neural networks;Vehicle-to-everything;Vehicles;Cybersecurity;Machine Learning;Real-time Risk Assessment;Traffic Safety;Advanced Driver Assistance Systems (ADAS);Vehicular Communication Systems;Sensor Fusion},
  doi={10.1109/IDSTA62194.2024.10746990},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10776138,
  author={Jusman, Yessi and Salsabiela, Rizqa and Ferdiansyah, Ricko and Lubis, Julnila Husna and Imanulloh, Farkhan Fajar and Dalilla, Aufi Fauzziah and Fatullah, Alif and Mohamed, Zeehaida},
  booktitle={2024 11th International Conference on Electrical Engineering, Computer Science and Informatics (EECSI)}, 
  title={Effectiveness of Various Smoothing Filters on Malaria Cell Image Quality}, 
  year={2024},
  volume={},
  number={},
  pages={242-247},
  abstract={Malaria is one of the most vulnerable diseases attacking human health, with rising cases from year to year. Therefore, technological developments are required to overcome this disease. Currently, the examination of malaria is carried out by examining human blood using a microscope, and the results are diagnosed using artificial intelligence (AI) technology. To improve the diagnosis results, this research compared the effectiveness of three image smoothing methods (average, Gaussian, and median filters) in reducing Gaussian and Salt and Pepper noise, commonly present in malaria cell images. These filters were tested on four different noise levels and analyzed based on PSNR, SNR, MSE, and SSIM values to determine the most effective filter. The results unveiled that the average filter depicted the most remarkable performance on Gaussian noise, while the median filter produced the highest outcome on Salt and Pepper noise.},
  keywords={Salt;Visualization;Filters;Smoothing methods;Malaria;Gaussian noise;Microscopy;Artificial intelligence;Noise level;Signal to noise ratio;malaria;artificial intelligence;average frilter;Gaussian filter;median filter},
  doi={10.1109/EECSI63442.2024.10776138},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10746166,
  author={Gujar, Praveen and Paliwal, Gunjan and Panyam, Sriram},
  booktitle={2024 IEEE Eighth Ecuador Technical Chapters Meeting (ETCM)}, 
  title={Generative AI and the Future of Interactive and Immersive Advertising}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Generative AI is revolutionizing interactive and immersive advertising by enabling more personalized, engaging experiences through advanced technologies like VR, AR, and MR. This transformation is reshaping how advertisers create, deliver, and optimize content, allowing for two-way communication and blurring lines between digital and physical worlds. AI enhances user engagement through predictive analytics, real-time adaptation, and natural language processing, while also optimizing ad placement and personalization. Future trends include integration with emerging technologies like $\mathbf{5 G}$ and IoT, fully immersive experiences, and hyper-personalization. However, challenges such as privacy concerns, transparency issues, and ethical considerations must be addressed. As AI continues to evolve, it promises to create unprecedented opportunities for brands to connect with audiences in meaningful ways, potentially blurring the line between advertising and interactive entertainment. The industry must proactively address these challenges to ensure AI-driven advertising enhances user experiences while respecting privacy and maintaining trust.},
  keywords={Privacy;Ethics;Generative AI;Entertainment industry;Market research;User experience;Real-time systems;Natural language processing;Advertising;Predictive analytics;Generative AI;Advertising Technology (AdTech);Interactive Advertising;Immersive Advertising;Virtual Reality (VR);Augmented Reality (AR);Mixed Reality (MR);Personalization;User Engagement;Ad Creation;Machine Learning},
  doi={10.1109/ETCM63562.2024.10746166},
  ISSN={},
  month={Oct},}@ARTICLE{10460540,
  author={Di Tecco, Antonio and Foglia, Pierfrancesco and Prete, Cosimo Antonio},
  journal={IEEE Access}, 
  title={Video Quality Prediction: An Exploratory Study With Valence and Arousal Signals}, 
  year={2024},
  volume={12},
  number={},
  pages={36558-36576},
  abstract={With the explosion of online video consumption, assessing and anticipating how users will evaluate the content they watch has become increasingly important. Traditional methods based on explicit user feedback are often limited in their ability to do this, as they can be time-consuming and expensive to collect. This study explores techniques to predict users’ ratings about a video’s ability to evoke emotions through emotional signals. In particular, it is proposed a method of emotional analysis that uses valence and arousal data as key signals for predicting user ratings through systems that use machine-learning techniques. Hence, an experiment in the wild involved 112 participants who completed questionnaires to create a dataset of emotional data and video quality ratings to train different intelligent systems. The best system comprised a Medium Gaussian Support Vector Machine (SVM) classifier that detected users’ ratings between Ineffective and Effective based on valence and arousal features as input, achieving an accuracy higher than 87%. The result demonstrated that it is possible to predict users’ ratings on the ability of the movie to elicit emotion, using users’ emotional states in terms of valence and arousal. The system has several advantages, such as eliminating the need for user reports, predicting user ratings in real-time more quickly and dynamically, and utilizing only the initial emotional state to predict users’ ratings. In addition, it has potential applications in advertising, education, and entertainment fields. Advertisers could better understand how consumers perceive their products and create more effective advertising campaigns; educational institutions could develop more engaging and effective learning materials; entertainment providers could create more popular and successful content.},
  keywords={Streaming media;Emotion recognition;Face recognition;Support vector machines;Physiology;Emotional responses;Video recording;User experience;Machine learning;Pattern recognition;Arousal;correlation;emotion;emotional experience;emotional response;machine learning;pattern recognition;support vector machine;user rating;valence},
  doi={10.1109/ACCESS.2024.3374056},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9578837,
  author={Siyao, Li and Zhao, Shiyu and Yu, Weijiang and Sun, Wenxiu and Metaxas, Dimitris and Loy, Chen Change and Liu, Ziwei},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Animation Video Interpolation in the Wild}, 
  year={2021},
  volume={},
  number={},
  pages={6583-6591},
  abstract={In the animation industry, cartoon videos are usually produced at low frame rate since hand drawing of such frames is costly and time-consuming. Therefore, it is desirable to develop computational models that can automatically interpolate the in-between animation frames. However, existing video interpolation methods fail to produce satisfying results on animation data. Compared to natural videos, animation videos possess two unique characteristics that make frame interpolation difficult: 1) cartoons comprise lines and smooth color pieces. The smooth areas lack textures and make it difficult to estimate accurate motions on animation videos. 2) cartoons express stories via exaggeration. Some of the motions are non-linear and extremely large. In this work, we formally define and study the animation video interpolation problem for the first time. To address the aforementioned challenges, we propose an effective framework, AnimeInterp, with two dedicated modules in a coarse-to-fine manner. Specifically, 1) Segment-Guided Matching resolves the "lack of textures" challenge by exploiting global matching among color pieces that are piece-wise coherent. 2) Recurrent Flow Refinement resolves the "non-linear and extremely large motion" challenge by recur-rent predictions using a transformer-like architecture. To facilitate comprehensive training and evaluations, we build a large-scale animation triplet dataset, ATD-12K, which comprises 12,000 triplets with rich annotations. Extensive experiments demonstrate that our approach outperforms existing state-of-the-art interpolation methods for animation videos. Notably, AnimeInterp shows favorable perceptual quality and robustness for animation scenarios in the wild. The proposed dataset and code are available at https://github.com/lisiyao21/AnimeInterp/.},
  keywords={Training;Interpolation;Image color analysis;Annotations;Animation;Transformers;Robustness},
  doi={10.1109/CVPR46437.2021.00652},
  ISSN={2575-7075},
  month={June},}@ARTICLE{9186590,
  author={Hasan Alwan, Wafaa and Fazl-Ersi, Ehsan and Vahedian, Abedin},
  journal={IEEE Access}, 
  title={Identifying Influential Users on Instagram Through Visual Content Analysis}, 
  year={2020},
  volume={8},
  number={},
  pages={169594-169603},
  abstract={In recent years, social networks have attracted many users' interests. People express their daily experiences and emotions through social networks and become aware of others interests and thoughts. In these networks, influential users play an important role in broadcasting information to their communities. As a result, discovering such users in social networks has become very important, in particular, for marketing purposes. Various solutions already exist for identifying influential users, most of which are structure-based, in that they investigate the influence of a user according to his location in the network graph. This article proposes a novel approach for identifying influential users on Instagram, by examining User Generated Contents (UGC). More specifically, the proposed method combines various types of features extracted from the images posted on Instagram to determine whether or not a user is influential, without requiring any information about the structure of the network. An extensive set of experiments are performed to validate the effectiveness of the proposed method in identifying influential users.},
  keywords={Feature extraction;Twitter;User-generated content;Videos;Companies;Object recognition;Online social networks;influencers;content analysis;feature extraction;SVM;combined classifiers},
  doi={10.1109/ACCESS.2020.3020560},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9356089,
  author={Basnyat, Bipendra and Singh, Neha and Roy, Nirmalya and Gangopadhyay, Aryya},
  booktitle={2020 IEEE 17th International Conference on Mobile Ad Hoc and Sensor Systems (MASS)}, 
  title={Vision Powered Conversational AI for Easy Human Dialogue Systems}, 
  year={2020},
  volume={},
  number={},
  pages={684-692},
  abstract={In this paper, we propose an end to end goal-oriented conversational AI agent that can provide contextual information from a potential hazard site. We posit the conversational agent as a FloodBot capable of seeing, sensing, assessing hazard condition, and ultimately conversing about them. We present our domain-specific FloodBot design-solution and learning-experience from the real-time deployment in a flash flood devastated city that uses state-of-the-art deep learning models. We specifically used computer vision and pertinent natural language processing technologies to empower the conversation power of the FloodBot. To deliver such practical and usable AI, we chain multiple deep learning frameworks and create a human-friendly question-answer based dialogue system. We present our deployment details from the last five months and validate the results using ongoing COVID19's impact on the area as well.},
  keywords={Deep learning;Urban areas;Sensor systems;Real-time systems;Natural language processing;Sensors;Artificial intelligence;Chatbot;Deep Learning;Natural Language Processing;Computer Vision;Mobile computing},
  doi={10.1109/MASS50613.2020.00088},
  ISSN={2155-6814},
  month={Dec},}@INPROCEEDINGS{10169246,
  author={Sudhakar, K. N and Shanthi, M.B},
  booktitle={2023 International Conference on Sustainable Computing and Smart Systems (ICSCSS)}, 
  title={Deepfake: An Endanger to Cyber Security}, 
  year={2023},
  volume={},
  number={},
  pages={1542-1548},
  abstract={‘Deepfake’ got originated from the technology ‘deep learning’ working behind it and the type of information it generates ‘fake’ after manipulating the original information. It’s an AI-based innovation used to make counterfeit recordings and sound that look and sound genuine. The inventions and implication of digital technologies in all spheres of mankind, has posed a great challenge to mankind to come up with secure solutions against a digital problem called Deepfake resulting from the application of deep learning thereby compromising authentication or originality. The said technology creates digital images and videos all new and totally fake. But the consequence it creates on society is totally a negative impact. With the aid of AI in developing hyper realistic videos, Deepfake has extended its giant wings to harm societal health and resulting in a critical challenge against the authenticity of the source. The Internet has become the platform to deliver these Deepfakes to unlimited destinations within no time. There are lot many researches have been carried on how to detect this deep fakes. Most of the research works have used deep learning models like Convolution Neural Network (CNN) for analyzing the convolution traces in deepfakes. Some of the research works have used Recurrent Neural Networks (RNN) by combining the Long Short-Term Memory (LSTM) with Blockchain. This research study has presented the comprehensive literature study, which highlights the various approaches used in generation and detection of deepfakes.},
  keywords={Deep learning;Deepfakes;Technological innovation;Recurrent neural networks;Convolution;Programming;Media;Deepfake;Deep Learning;AI;Authenticity;Counter Techniques Algorithm;Digital Era},
  doi={10.1109/ICSCSS57650.2023.10169246},
  ISSN={},
  month={June},}@ARTICLE{8896000,
  author={Zhong, Fangwei and Sun, Peng and Luo, Wenhan and Yan, Tingyun and Wang, Yizhou},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={AD-VAT+: An Asymmetric Dueling Mechanism for Learning and Understanding Visual Active Tracking}, 
  year={2021},
  volume={43},
  number={5},
  pages={1467-1482},
  abstract={Visual Active Tracking (VAT) aims at following a target object by autonomously controlling the motion system of a tracker given visual observations. To learn a robust tracker for VAT, in this article, we propose a novel adversarial reinforcement learning (RL) method which adopts an Asymmetric Dueling mechanism, referred to as AD-VAT. In the mechanism, the tracker and target, viewed as two learnable agents, are opponents and can mutually enhance each other during the dueling/competition: i.e., the tracker intends to lockup the target, while the target tries to escape from the tracker. The dueling is asymmetric in that the target is additionally fed with the tracker's observation and action, and learns to predict the tracker's reward as an auxiliary task. Such an asymmetric dueling mechanism produces a stronger target, which in turn induces a more robust tracker. To improve the performance of the tracker in the case of challenging scenarios such as obstacles, we employ more advanced environment augmentation technique and two-stage training strategies, termed as AD-VAT+. For a better understanding of the asymmetric dueling mechanism, we also analyze the target's behaviors as the training proceeds and visualize the latent space of the tracker. The experimental results, in both 2D and 3D environments, demonstrate that the proposed method leads to a faster convergence in training and yields more robust tracking behaviors in different testing scenarios. The potential of the active tracker is also shown in real-world videos.},
  keywords={Target tracking;Training;Visualization;Task analysis;Cameras;Object tracking;Active object tracking;adversarial training;reinforcement learning},
  doi={10.1109/TPAMI.2019.2952590},
  ISSN={1939-3539},
  month={May},}@ARTICLE{10740275,
  author={Mande, Spandana and Ramachandran, Nandhakumar and Salma Asiya Begum, Shaik and Moreira, Fernando},
  journal={IEEE Access}, 
  title={Optimized Reinforcement Learning for Resource Allocation in Vehicular Ad Hoc Networks}, 
  year={2024},
  volume={12},
  number={},
  pages={167040-167048},
  abstract={The vehicular ad hoc networks VANET is an essential part of intelligent transportation systems (ITSs) since it may offer various multimedia services and safety services to pedestrians, passengers, and even drivers. A wireless communication protocol called dedicated short-range communication (DSRC) was created for toll collection systems. Nevertheless, DSRC standards are extremely constrained, necessitating the development of next-generation communication protocols appropriate for VANET. Here, intended to develop an Optimized Reinforcement Learning (ORL) for obtaining resource allocation in VANET. This proposed methodology is developed for achieving resource allocation with efficient data transmission. This proposed approach is utilized to adjust the control channel interval (CCI) and service channel interval (SCI) to empower network performance. Additionally, it is utilized to reduce data collisions and optimize the network’s backoff distribution. The proposed method is a combination of reinforcement learning (RL) and adaptive coati optimization (ACO). The coati optimization mimics the characteristics of coati in natures in which it depends upon the coati escape from predators and hunting and attacking behaviour at various climates.The RL is utilized to obtain an efficient channel access algorithm. In the RL, the Q value is optimally selected by using ACO. Based on this algorithm, the proposed method is utilized to enhance the performance of VANET data transmission by achieving optimal resource allocation. The proposed method is implemented in MATLAB, and performances are evaluated using performance measures. Additionally, to validate the performance of the proposed methodology, it is compared with conventional techniques.},
  keywords={Vehicular ad hoc networks;Resource management;Reinforcement learning;Automobiles;Optimization;Data models;Reliability;Long short term memory;Vehicle-to-infrastructure;Switches;VANET;resource allocation;adaptive coati optimization and reinforcement learning},
  doi={10.1109/ACCESS.2024.3489395},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10574926,
  author={Loevenich, Johannes F. and Lopes, Roberto Rigolin F.},
  booktitle={NOMS 2024-2024 IEEE Network Operations and Management Symposium}, 
  title={DRL meets GNN to improve QoS in Tactical MANETs}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={This paper proposes a hybrid AI model combining Graph Neural Network (GNN) and Deep Reinforcement Learning (DRL) to improve QoS in modern communication systems deployed to tactical networks. Our methodology consists of three interacting agents: an environment builder agent responsible for generating complex network graph environments, a DRL agent situated within the control plane that possesses a global view of the current network state and makes decisions based on information gathered from various layers of the multi-layer tactical system, and an adversary designed to improve the robustness of the DRL agent. Our initial results indicate that enabling GNNs and DRL together with adversarial training is a promising solution for enhancing the Quality of Service (QoS) of modern tactical communication systems operating in hostile environments that may host active adversaries.},
  keywords={Training;Computational modeling;Pipelines;Quality of service;Predictive models;Graph neural networks;Robustness;Tactical Communication Systems;Reinforcement Learning;Graph Neural Networks;Network Robustness},
  doi={10.1109/NOMS59830.2024.10574926},
  ISSN={2374-9709},
  month={May},}@ARTICLE{8947961,
  author={Khan, Umair Ali and Martínez-Del-Amor, Miguel Á. and Altowaijri, Saleh M. and Ahmed, Adnan and Rahman, Atiq Ur and Sama, Najm Us and Haseeb, Khalid and Islam, Naveed},
  journal={IEEE Access}, 
  title={Movie Tags Prediction and Segmentation Using Deep Learning}, 
  year={2020},
  volume={8},
  number={},
  pages={6071-6086},
  abstract={The sheer volume of movies generated these days requires an automated analytics for efficient classification, query-based search, and extraction of desired information. These tasks can only be efficiently performed by a machine learning based algorithm. We address the same issue in this paper by proposing a deep learning based technique for predicting the relevant tags for a movie and segmenting the movie with respect to the predicted tags. We construct a tag vocabulary and create the corresponding dataset in order to train a deep learning model. Subsequently, we propose an efficient shot detection algorithm to find the key frames in the movie. The extracted key frames are analyzed by the deep learning model to predict the top three tags for each frame. The tags are then assigned weighted scores and are filtered to generate a compact set of most relevant tags. This process also generates a corpus which is further used to segment a movie based on a selected tag. We present a rigorous analysis of the segmentation quality with respect to the number of tags selected for the segmentation. Our detailed experiments demonstrate that the proposed technique is not only efficacious in predicting the most relevant tags for a movie, but also in segmenting the movie with respect to the selected tags with a high accuracy.},
  keywords={Motion pictures;Deep learning;Task analysis;Semantics;Vocabulary;Motion segmentation;Tags prediction;movie segmentation;deep learning;transfer learning},
  doi={10.1109/ACCESS.2019.2963535},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10687965,
  author={Patil, Kanchan Pranay and Vijayakumar Bharathi, S},
  booktitle={2024 OPJU International Technology Conference (OTCON) on Smart Computing for Innovation and Advancement in Industry 4.0}, 
  title={From Pixels to Purchases:The Role of Computer-Generated Imagery and Virtual Influencers in Digital Marketing}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Artificially created characters known as virtual influencers have millions of followers on social media and affect digital natives’ engagement and decision-making in remarkable ways. This study identifies and conceptualizes six primary concepts of the TEARS model: trustworthiness, expertise, attractiveness, respect, similarity, impacting brand attachment, and Mimetic desire. Furthermore, we tested the impact of brand attachment and the mimetic desire of the followers toward purchase response as an outcome. This research’s primary data comes from an online survey of 381 Instagram users. The result indicates a significant positive impact of virtual influencers’ endorsement towards consumer purchase, with brand attachment and mimetic desire as the mediating variables. These findings extend the current knowledge on brand endorsement, influencer marketing, and virtual influencers in the artificial intelligence era. The study offers insights into improving brand attachments and produces implications to theory and practice for the efficacy of computer-generated imagery (CGI) for virtual influencers in advertising campaigns.},
  keywords={Surveys;Ethics;Technological innovation;Social networking (online);Decision making;Fourth Industrial Revolution;Web sites;virtual influencer marketing;TEARS Model;stimulus-organism-response;CGI;brand attachment;Mimetic desire;Customer Purchase Intentions},
  doi={10.1109/OTCON60325.2024.10687965},
  ISSN={},
  month={June},}@INPROCEEDINGS{10115207,
  author={Thorpe, Sean},
  booktitle={SoutheastCon 2023}, 
  title={A Vision for STEM Education at the University of Technology, Jamaica}, 
  year={2023},
  volume={},
  number={},
  pages={793-797},
  abstract={This vision paper articulates the author’s perspective for developing a quality education system at the University of Technology, Jamaica. The vision is consistent with the expectations under a well published provision of an Industry 4.0 framework, and the future of work in Science, Technology, Engineering and Mathematics (STEM). This is built on the principle that the Digital skills ecosystem requires a technology savvy workforce that apply smart innovation with a greater need for competencies around both Cyber-Security and Data science curriculum in all areas of Engineering and Computing (i.e., Electrical and Computing, Mechanical, Industrial, Chemical, Agricultural, Computer Science and Information Systems, Networking, Artificial Intelligence, Computer Animation, Game Development). While in the core Engineering and Computing disciplines the applications for a range of different products and services are significant, there is equal opportunity in applying these disciplines with their respective competencies within the areas of Health Care, Law, Sports, and Education as some of the University’s opportunities while leveraging the complete data cloud infrastructure. The future of tomorrow’s digital business is now and leveraging access to micro-credentials has become our new gold standard for redefining relevance with respect to building the technology talent pipeline and closing the technology skills gap of today’s workforce as a Faculty of Engineering and Computing (FENC).},
  keywords={Technological innovation;Philosophical considerations;Education;Buildings;Standards organizations;Pipelines;Organizations;Engineering;Computing;Education},
  doi={10.1109/SoutheastCon51012.2023.10115207},
  ISSN={1558-058X},
  month={April},}@ARTICLE{10504285,
  author={Zhou, Jiting and Zhao, Xinrui and Xu, Qian and Zhang, Pu and Zhou, Zhihao},
  journal={IEEE Access}, 
  title={MDCF-Net: Multi-Scale Dual-Branch Network for Compressed Face Forgery Detection}, 
  year={2024},
  volume={12},
  number={},
  pages={58740-58749},
  abstract={Face forgery detection aims to identify manipulated or altered facial images or videos created using artificial intelligence. Existing detection methods exhibit favorable performance on high-quality videos, but the videos in daily applications are commonly compressed into low-quality formats via social media. The detection difficulty is increased by the poor quality, indistinct detail features, and noises such as artifacts in these images or videos. To address this challenge, we propose a multi-scale dual-branch network for compressed face forgery, called MDCF-Net, effectively capturing cross-domain forgery features at various scales in compressed facial images. The MDCF-Net comprises two branches: an RGB domain branch utilizing Transformers to extract multi-scale fine-texture features from the original RGB images; a frequency domain branch designed to capture artifacts in low-quality videos by extracting global spectral features as a supplementary measure. Then, we introduce a feature fusion module (FFM) based on multi-head attention to merge diverse feature representations in a spatial-frequency complementary manner. Extensive comparative experiments on public datasets such as FaceForensics++, Celeb-DF, and WildDeepfake demonstrate the significant advantage of MDCF-Net in detecting highly compressed and low-quality forged images or videos, especially in achieving state-of-the-art performance on the FaceForensics++ low-quality dataset. Our approach presents a new perspective and technology for low-quality face forgery detection.},
  keywords={Feature extraction;Frequency-domain analysis;Face recognition;Forgery;Transformers;Deepfakes;Image coding;Face forgery;deepfake detection;transformers;frequency domain;two-branch;feature fusion},
  doi={10.1109/ACCESS.2024.3390217},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10887999,
  author={Bai, Jingqi and Zhou, Jingkai and Wang, Benzhi and Chen, Weihua and Yang, Yang and Lei, Zhen and Wang, Fan},
  booktitle={ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Layer-Animate for Transparent Video Generation}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Transparent videos with alpha channels play a crucial role in film production, advertising, and augmented reality fields. However, there is currently no available method for producing transparent videos. Traditional methods are time-consuming and labor-intensive, and employing alternative approaches for this task will result in inaccurate transparent regions, constrained motion, and artifacts. To address these challenges, we propose Layer-Animate, the first method capable of generating transparent videos. Our method comprises two stages: in the first stage, transparent images are generated as the base images to provide content and transparency information for the next stage. In the second stage, Inter-Frame Attention is applied to decouple content from motion, enabling the motion module to focus better on action. Layer-Animate is the first method used to generate transparent videos with accurate transparent regions, sufficient motion, and no artifacts, as demonstrated by notable improvements in qualitative and quantitative metrics.},
  keywords={Accuracy;Production;Signal processing;Acoustic measurements;Robustness;Acoustics;Speech processing;Advertising;Augmented reality;Videos;diffusion models;transparent videos;deep generative models;video generation},
  doi={10.1109/ICASSP49660.2025.10887999},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10707380,
  author={Zhao, Bixiao and Chen, Nanxi and Jia, Jinyuan and Feng, Enyang and Li, Xiaohen},
  booktitle={2024 IEEE International Conference on Web Services (ICWS)}, 
  title={VAIDE: Virtual AI Designer for Web3D Exhibition Layout Creation}, 
  year={2024},
  volume={},
  number={},
  pages={1314-1320},
  abstract={The design of a virtual exhibition layout is a meticulous process. It usually starts with floor plan design, adhering to guidelines such as visitor traffic flow, exhibit prominence, and aesthetic considerations. Based on floor plans, 3D models are constructed, followed by the creation of final scenes. However, there is a notable lack of an efficient method to streamline this complex design process, which often requires profound professional expertise. To address this gap, we propose a novel approach leveraging generative AI and a floor plan recognition algorithm to reconstruct 3D exhibition layouts, thereby enhancing designers’ efficiency. We have achieved the design of exhibition hall floor plans using Stable Diffusion and completed the automatic conversion from floor plans to 3D models, as well as the web-based operation and display platform. We undertake a comparative analysis between our solution and a CGAN-based method, evaluating the results through both subjective and objective measures. Furthermore, we present a comprehensive flowchart outlining the process of generating a museum exhibition hall on a web platform. Additionally, we demonstrate the design impact of our methodology in creating a virtual exhibition hall utilizing Web3D technology.},
  keywords={Training;Solid modeling;Three-dimensional displays;Web services;Generative AI;Layout;Virtual museums;Reliability engineering;Floors;Guidelines;Web3D;Stable Diffusion;AIGC;Exhibition layout design;Generative AI},
  doi={10.1109/ICWS62655.2024.00158},
  ISSN={2836-3868},
  month={July},}@INPROCEEDINGS{10850962,
  author={Aggoune, Aicha and Mihoubi, Zakaria},
  booktitle={2024 International Conference on Advanced Aspects of Software Engineering (ICAASE)}, 
  title={Towards Efficient Dataset Development: A Case Study of M2Q2+ in Movie QA Systems}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={The development and availability of high-quality datasets are critical to the success of machine learning and artificial intelligence (AI) systems. Datasets provide the essential foundation for training, validating, and testing models. The need for domain-specific datasets is particularly important in closed-domain systems like Movie NoSQL Question Answering (QA). A well-curated dataset enables models to translate natural language questions into NoSQL queries effectively. This paper introduces an efficient method for developing a tailored dataset for MongoDB-based Movie QA systems. The resulting dataset, M2$Q$2+, facilitates the translation of natural language questions into MongoDB queries (MQL). The dataset development process includes creating templates, performing data augmentation with the pre-trained T5-base model, and conducting a data revision process to produce the final dataset. A combination of evaluation methods and metrics was applied to ensure that M2$Q$2+ is high quality.},
  keywords={Training;Measurement;Translation;Machine learning;Syntactics;Motion pictures;Question answering (information retrieval);Data models;Testing;Software engineering;QA system;Dataset creation;NoSQL MongoDB;Movie domain;M2Q2 dataset},
  doi={10.1109/ICAASE64542.2024.10850962},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{8834478,
  author={Suhairi, M. and Wirawan and Endroyono and Irfansyah, Astria Nur},
  booktitle={2019 International Conference of Artificial Intelligence and Information Technology (ICAIIT)}, 
  title={Complexity Reduction for Multiview HEVC Codec Using FPGA}, 
  year={2019},
  volume={},
  number={},
  pages={163-168},
  abstract={Due to the increasing quality and resolution of video content, especially 3D video, the computational complexity for its processing also significantly increases. One of the popular format, HEVC has extensions called Multiview HEVC (MV-HEVC) and 3D-HEVC with high amounts of data and high resolution that resulting in increased computational complexity. This study aims to reduce the computational complexity of MV-HEVC videos by implementing mode decision such as ECU, CFM, ESD, and deblocking filters which are tested on Linux-based PC platforms and the Xilinx All Programmable SoC platform. From the experimental results obtained the reduction in computational complexity can be seen from the comparison of encoding time, the Xilinx All Programmable SoC platform is able to obtain encoding times 35.85% that are faster than Linux-based PCs. For the quality of the video produced between the two the platform is not significant from the bitrate and PSNR values.},
  keywords={Encoding;Computational complexity;Bit rate;Three-dimensional displays;Electrostatic discharges;Video coding;MV-HEVC;H.265;xilinx;zynq},
  doi={10.1109/ICAIIT.2019.8834478},
  ISSN={},
  month={March},}@INPROCEEDINGS{10884268,
  author={Markose, Suja and C.V., Raghu.},
  booktitle={2024 IEEE International Symposium on Smart Electronic Systems (iSES)}, 
  title={Development of an AI based Edge Computing System for Malayalam Vowel Classification}, 
  year={2024},
  volume={},
  number={},
  pages={25-29},
  abstract={Malayalam language is characterized as a phone-mic language wherein graphemes are directly associated with phonemes. Malayalam words are essentially compositions of the phonetic units, the vowels and the consonants in the order they appear in the written script. Vowels exhibit distinctive waveforms that recur multiple times, contributing to the perception of a unique sound. This endeavor seeks to undertake a classification of Malayalam vowels utilizing an embedded platform, thereby facilitating analysis of the language's phonetic structure for the recognition of the Malayalam words.},
  keywords={Training;Computational modeling;Speech recognition;Phonetics;Data models;Real-time systems;Natural language processing;Artificial intelligence;Speech processing;Edge computing;Malayalam vowels;vowel classification;edge computing},
  doi={10.1109/iSES63344.2024.00016},
  ISSN={2832-3602},
  month={Dec},}@ARTICLE{8901180,
  author={Zamir, Syed Waqas and Vazquez-Corral, Javier and Bertalmío, Marcelo},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Vision Models for Wide Color Gamut Imaging in Cinema}, 
  year={2021},
  volume={43},
  number={5},
  pages={1777-1790},
  abstract={Gamut mapping is the problem of transforming the colors of image or video content so as to fully exploit the color palette of the display device where the content will be shown, while preserving the artistic intent of the original content’s creator. In particular, in the cinema industry, the rapid advancement in display technologies has created a pressing need to develop automatic and fast gamut mapping algorithms. In this article, we propose a novel framework that is based on vision science models, performs both gamut reduction and gamut extension, is of low computational complexity, produces results that are free from artifacts and outperforms state-of-the-art methods according to psychophysical tests. Our experiments also highlight the limitations of existing objective metrics for the gamut mapping problem.},
  keywords={Image color analysis;Motion pictures;Computational modeling;Standards;Germanium;Measurement;TV;Gamut mapping algorithms;wide gamut imaging;color reproduction;vision models for color and contrast;gamut mapping for cinema},
  doi={10.1109/TPAMI.2019.2938499},
  ISSN={1939-3539},
  month={May},}@INPROCEEDINGS{10118099,
  author={Grenouillet, L. and Barbot, J. and Laguerre, J. and Martin, S. and Carabasse, C. and Louro, M. and Bedjaoui, M. and Minoret, S. and Kerdilès, S. and Boixaderas, C. and Magis, T. and Jahan, C. and Andrieu, F. and Coignus, J.},
  booktitle={2023 IEEE International Reliability Physics Symposium (IRPS)}, 
  title={Reliability assessment of hafnia-based ferroelectric devices and arrays for memory and AI applications (Invited)}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Ferroelectricity in doped HfO2 thin films was reported for the first time 12 years ago, generating strong interest in the non-volatile memory and logic community. Thanks to their CMOS compatibility and potential for scaling, hafnia-based Ferroelectric Random Access Memories (FeRAMs), Ferroelectric Tunnel Junctions (FTJ s) and Ferroelectric Field Effect Transistors (FeFETs) are not only a breakthrough with respect to conventional perovskite-based ferroelectric (FE) devices but also potentially a revolution from an application prospective, in particular considering the non-volatility and intrinsic energy efficiency of these devices. However, their maturity is currently too low to consider practical applications. In this paper, we therefore focus on the reliability assessment of Metal/FE/Metal (MFM) and Metal/FE/Dielectric/Metal (MFDM) stacks, either in the form of large area ferroelectric capacitors, or in the form of kbit arrays integrated in CMOS Back-End of Line.},
  keywords={Nonvolatile memory;Ferroelectric films;Ferroelectric devices;Random access memory;Iron;Hafnium compounds;Reliability;Ferroelectric;Hafnia;Ferroelectric Capacitor;Ferroelectric Random Access Memory;Ferroelectric Tunnel Junction},
  doi={10.1109/IRPS48203.2023.10118099},
  ISSN={1938-1891},
  month={March},}@INPROCEEDINGS{10467601,
  author={Bojjawar, Satish and Shanmugasundaram, R. and Benakop, Prabhu G and Raj, C. Mohan and Babu, W. Rajan},
  booktitle={2024 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT)}, 
  title={Intelligent Modular Controller for Implementing the Digital Protection of Transformers as AI Algorithms Techniques}, 
  year={2024},
  volume={},
  number={},
  pages={903-908},
  abstract={Accurate temperature control of power transformer is the challenging task to every power system engineers. Nowadays Mercury based temperature sensing and actuating systems are installed for controlling the power transformer temperature and tripping the breakers whenever the temperature is exceeded the desired preset value. But the drawback of these control methods is lack of accuracy in sensing of temperature and sluggish control or slow response. A new approach or method for controlling and monitoring the temperature of the power transformer is prescribed and the associated control components are discussed. The intelligent control system uses the Programmable Logic Controller (PLC) and Graphical Operating Terminal (GOT) Display unit for the control purpose. The ON command to cooling fan and TRIP signal to circuit breaker are given from the controller. The algorithm is written into the PLC in the form of ladder program. The reference values and actual values are displayed in the GOT. In this study, the required control modules and programming part are discussed to implement the intelligent control of temperature of power transformer. The controller is trained for the past history and necessary scripts are written to take the action in its own for the specific problem using AI based techniques.},
  keywords={Temperature sensors;Temperature measurement;Programmable logic devices;Programming;Temperature control;Sensors;Power transformers;Mercury sensed temperature;temperature measurement error;Programmable controller and Touch panel;AI based techniques for temperature control},
  doi={10.1109/IDCIoT59759.2024.10467601},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10872441,
  author={Li, Zhiqiang and Zhang, Ming and Ma, Yupeng and Wu, Shihong},
  booktitle={2024 2nd International Conference on Computer, Vision and Intelligent Technology (ICCVIT)}, 
  title={Script Event Prediction Based on Large Model Reflection Mechanism}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Script event prediction (SEP) involves predicting the most plausible next event given a sequence of historical events. This paper introduces ReGen-SEPM (Reflection and Generation for Script Event Prediction Model), a framework combining LLaMA-based generation and a reflection mechanism to improve prediction quality. The generative module produces initial predictions based on context and prompts, while the reflection module evaluates and refines them for semantic consistency and causal plausibility. A self-supervised learning mechanism further enhances the reflection module's effectiveness. Experiments on the New York Times Corpus show that ReGen-SEPM outperforms state-of-the-art baselines in accuracy, semantic similarity, and causal coherence, with ablation studies highlighting the contributions of reflection and self-supervised learning.},
  keywords={Adaptation models;Technological innovation;Accuracy;Quantization (signal);Computational modeling;Semantics;Self-supervised learning;Coherence;Predictive models;Reflection;script event prediction;large language models;reflection mechanism;llama},
  doi={10.1109/ICCVIT63928.2024.10872441},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10291023,
  author={Özer, Çağdaş and Takaoğlu, Mustafa and Dursun, Taner},
  booktitle={2023 IEEE International Conference on Artificial Intelligence, Blockchain, and Internet of Things (AIBThings)}, 
  title={Forecasting the Impact Of The News On Cryptocurrency via Machine Learning Algorithms}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={In this study, it was investigated which of the machine learning algorithms gave the most optimal results in determining the effect of news about cryptocurrencies on the prediction of Bitcoin (BTC) price. In this direction, three different data sets were created, and these data were tested separately in each machine learning algorithm examined. In this way, it is aimed to determine the effect of the data set on the success of machine learning algorithms. By doing web scraping with the script, we developed to determine the news data we used in the study, 684 news made by BBC, CNBC, Fox, Guardian, NY Post, Over News, Sun, USA Today, and VOA news sources were found and by sentiment analysis of them, 564 news related to cryptocurrencies were taken into account in the study. March 2020 and March 2021 were chosen as the dates of the selected news. (The period when the BTC price rose from 9Ks to 30Ks.). The reason for choosing a past time period is to precisely measure the consistency of the predictions made. While creating the data sets used in machine learning algorithms, high, low, volume, open, and close values in the dollar, Ethereum, and gold values were also taken into account, as well as news sources. In this way, it is aimed to make more realistic estimations by taking market data into account. Machine learning methods in the study; Decision tree, deep learning, random forest, support vector machine, gradient boosted trees, and a generalized linear model was used. The generalized linear model (model II), which gives the most successful result with a rate of 98.73%, is explained in detail in the study.},
  keywords={Support vector machines;Analytical models;Sentiment analysis;Machine learning algorithms;Bitcoin;Predictive models;Time measurement;blockchain technology;machine learning;bitcoin;ethereum;web scraping;sentiment analysis;generalized linear model},
  doi={10.1109/AIBThings58340.2023.10291023},
  ISSN={},
  month={Sep.},}@ARTICLE{10955340,
  author={Chakravarthi, Bandaru A and Shivakanth, Gandla},
  journal={IEEE Access}, 
  title={Integrating Multimodal AI Techniques and MRI Preprocessing for Enhanced Diagnosis of Alzheimer’s Disease: Clinical Applications and Research Horizons}, 
  year={2025},
  volume={13},
  number={},
  pages={63519-63531},
  abstract={A progressive neurological disorder, Alzheimer’s disease has a significant social and economic impact and has become more problematic for world health. Early and precise diagnosis is critical to enable timely intervention to improve patient outcomes. This study introduces a new multimodal AI framework integrating advanced MRI preprocessing techniques and speech analysis for the enhancement of the detection of AD. A strong preprocessing pipeline, including noise reduction, normalization, skull stripping, and segmentation, is utilized to prepare the MRI data. Cognitive and acoustic biomarkers are extracted through spectrograms and pre-trained linguistic models to create speech features. The architecture involves the application of Vision Transformers for spatial analysis and a hybrid CNN- RNN architecture to derive contextual insights, making the framework applicable to multimodal fusion for holistic diagnosis. The framework achieved an accuracy of 94.2% with precision, recall, and F1-scores higher than 92% in the experimental evaluation on a diverse dataset. Comparative analysis with several recent studies further emphasizes that the framework provides better diagnostic performance on key metrics. This work advances early AD detection by incorporating complementary data modalities, addressing existing gaps in unimodal approaches, and providing a scalable, interpretable solution for clinical application.},
  keywords={Magnetic resonance imaging;Alzheimer's disease;Artificial intelligence;Diseases;Feature extraction;Accuracy;Biological system modeling;Biomarkers;Neuroimaging;Training;Alzheimer’s disease (AD);CNN- RNN architecture;diagnostic accuracy;early diagnosis;machine learning in healthcare;MRI preprocessing;multimodal AI;neuroimaging;speech biomarkers;vision transformers (ViT)},
  doi={10.1109/ACCESS.2025.3557533},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10599456,
  author={Aigbodion, Victor Sunday and Ako, Paul Amaechi and Mbohwa, Charles},
  booktitle={2024 1st International Conference on Smart Energy Systems and Artificial Intelligence (SESAI)}, 
  title={Two-Step Spin Coating Method: Revealing the Effect of Thermal Annealing Temperature on the Properties of Hybrid Perovskite Solid State Solar Cells}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={In this work, perovskite solar cells were created using a two-step spin coating procedure. The perovskite solar cells were thermally annealed between 90 and 120 °C in order to ascertain the microstructure and solar cell characteristics. The result demonstrates that the thermal annealing temperature influences the perovskite layer's degree of crystallization. The best results were attained at 120 °C. The results show that thermal annealing and combined-effect two-step spin coating may improve the solar characteristics of lead-based perovskites.},
  keywords={Iodine compounds;Annealing;Temperature;Photovoltaic cells;Morphology;Lead;Titanium;microstructure;perovskites;spin coating;annealing temperature;solar cell characteristics},
  doi={10.1109/SESAI61023.2024.10599456},
  ISSN={},
  month={June},}@INPROCEEDINGS{10442795,
  author={Park, Ga Eun and Lee, Hae Won and Oh, Ju Yeon and Bae, Seong Geon},
  booktitle={2023 International Conference on Electrical, Communication and Computer Engineering (ICECCE)}, 
  title={A Study on the Comparison and Measurement of Improved CartoonGAN Performance for High-Quality Image Generation}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={With the rapid growth of computer vision, among them, generative artificial intelligence is being used in various industries. CartoonGAN, one of the generating artificial intelligence, can create new images by changing the original image into a cartoon style, which can be used in various fields such as webtoons and animations. To this end, a process of improving the performance of CartoonGAN is necessary, and research for this should be conducted. In this study, to solve the performance degradation that occurs when CartoonGAN is applied, we would like to suggest a direction to improve by applying technologies such as SRCNN and Tiling.},
  keywords={Industries;Performance evaluation;Computer vision;Visualization;Costs;Image synthesis;Animation;CartoonGAN;SRCNN;Tiling;Model;Computer Vision},
  doi={10.1109/ICECCE61019.2023.10442795},
  ISSN={},
  month={Dec},}@ARTICLE{8660632,
  author={Hao, Chuanyan and Chen, Yadang and Wu, Enhua},
  journal={IEEE Access}, 
  title={Efficient PatchMatch-Based Synthesis for Cartoon Animation}, 
  year={2019},
  volume={7},
  number={},
  pages={31262-31272},
  abstract={Automating the production of 2D hand-drawn animations is a significant and interesting component in computer graphics and vision. However, traditional methods in animation production pipeline always use physically or geometrically based models which are consuming due to complicated and massive computations, reducing their practicability. In this paper, we propose an efficient data-driven approach to create hand-drawn animations in an automatic manner. The key idea is to employ a correspondence match-based random search process to extract the geometry samples and the global motion pattern in an input animation sequence and then to generate a new output sequence through a coarse-to-fine sample-based synthesis algorithm. Our experiments demonstrate that our method achieves good results with high quality and performance, producing a range of artistic effects that previously required disparate and professional techniques.},
  keywords={Animation;Strain;Two dimensional displays;Tracking;Data structures;Boolean functions;Computer graphics;Animation;data-driven approach;exemplar-based approach;motion tracking and synthesis;PatchMatch;sample based synthesis},
  doi={10.1109/ACCESS.2019.2903148},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10284976,
  author={Martin, Eric Burton and Ghosh, Sudipto},
  booktitle={2023 International Conference on Emerging Trends in Networks and Computer Communications (ETNCC)}, 
  title={GitHub Copilot: A Threat to High School Security? Exploring GitHub Copilot's Proficiency in Generating Malware from Simple User Prompts}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper examines the potential implications of script kiddies and novice programmers with malicious intent having access to GitHub Copilot, an artificial intelligence tool developed by GitHub and OpenAI. The study assesses how easily one can utilize this tool to generate various common types of malware ranging from ransomware to spyware, and attempts to quantify the functionality of the produced code. Results show that with a single user prompt, malicious software such as DoS programs, spyware, ransomware, trojans, and wiperware can be created with ease. Furthermore, uploading the generated executables to VirusTotal revealed an average of 7/72 security vendors flagging the programs as malicious. This study has shown that novice programmers and script kiddies with access to Copilot can readily create functioning malicious software with very little coding experience. This paper discusses how this could potentially lead to an increase in internal attacks on schools due to the average age demographic of the target group. However, if used correctly this technology could potentially help this same demographic gain the skills needed for ethical hacking practices utilized in the cybersecurity space.},
  keywords={Ethics;Computer viruses;Market research;Encoding;Distance measurement;Ransomware;Trojan horses;GitHub Copilot;cybersecurity;malware generation;school network security;internal cyber-attacks;cybersecurity risk management in schools;script kiddies},
  doi={10.1109/ETNCC59188.2023.10284976},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10308385,
  author={Meacci, Valentino and Bosco, Edoardo and Ramalli, Alessandro and Boni, Enrico and Tortoli, Piero and Mazierli, Daniele and Spairani, Edoardo and Matrone, Giulia},
  booktitle={2023 IEEE International Ultrasonics Symposium (IUS)}, 
  title={Single-channel, ultraportable, real-time imaging system based on deep learning: a proof-of-concept}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={There is a growing demand for user-friendly technologies that may empower individuals to independently monitor their physiological parameters. Ultrasound systems are highly promising for healthcare self-monitoring. To address these challenges, in this work, we present the proof of concept (PoC) of a real-time ultraportable system with a single-channel transmission and reception strategy and a deep learning-based image reconstruction method. The PoC uses the ULA-OP 256 scanner, employing a monostatic synthetic aperture focusing technique, along with a convolutional neural network (CNN) trained to generate B-mode images. Testing was carried out using a CNN running on a mid-range graphics processing unit (GPU), both on phantoms and in vivo scenarios, comparing the image quality achieved by the CNN with that of a delay-and-sum (DAS) beamformer. The results highlight that the CNN outperformed the DAS, showing a better image quality. Furthermore, the CNN achieved a real-time frame rate of 37.0 frames per second, proving that ultraportable ultrasound systems are highly promising for the future of self-monitoring instruments.},
  keywords={Image quality;In vivo;Ultrasonic imaging;Instruments;Graphics processing units;Phantoms;Real-time systems;Artificial intelligence;Ultraportable ultrasound;Self-Monitoring;Real-time;GPU;Neural network;Synthetic aperture;ULA-OP 256},
  doi={10.1109/IUS51837.2023.10308385},
  ISSN={1948-5727},
  month={Sep.},}@INPROCEEDINGS{9300095,
  author={Fadhil, Jawaher Abdulwahab and Sarhan, Qusay Idrees},
  booktitle={2020 21st International Arab Conference on Information Technology (ACIT)}, 
  title={Internet of Vehicles (IoV): A Survey of Challenges and Solutions}, 
  year={2020},
  volume={},
  number={},
  pages={1-10},
  abstract={The technological revolution of the Internet of Things (IoT) increased the number of objects (e.g., vehicles) connected to the Internet, making our lives easier, safer, and smarter. Putting IoT objects on the wheel has created a new technology called the Internet of Vehicle (IoV). The massive progress in communication and computing concepts brought the IoV to real implementation. The IoV is the advanced version of Vehicular Ad hoc Network (VANET), mainly designed to provide safe driving. Although the number of vehicles connected to the IoV keeps growing, there are various challenges and opportunities of the IoV technology that are still not clear. In this paper, several research papers related to the IoV were examined to identify and categorize the existing challenges of implementing and employing IoV in urban cities. Furthermore, this work outlines the IoV technical limitations that must be addressed. Moreover, various existing and future solutions that tackle the identified challenges were briefly discussed.},
  keywords={Vehicular ad hoc networks;Servers;Software;Monitoring;Urban areas;Radiofrequency identification;Processor scheduling},
  doi={10.1109/ACIT50332.2020.9300095},
  ISSN={},
  month={Nov},}@ARTICLE{9362238,
  author={Yuan, Honglin and Veltkamp, Remco C.},
  journal={IEEE Robotics and Automation Letters}, 
  title={PreSim: A 3D Photo-Realistic Environment Simulator for Visual AI}, 
  year={2021},
  volume={6},
  number={2},
  pages={2501-2508},
  abstract={Recent years have witnessed great advancement in visual artificial intelligence (AI) research based on deep learning. To take advantage of deep learning, we need to collect a large amount of data in various environments and conditions. However, collecting such data is time-consuming and labor-intensive. Apart from that, developing and testing visual AI algorithms for multisensory models is expensive and in some cases dangerous processes in the real world. We present PreSim, a 3D environment simulator which provides photo-realistic simulations using a view synthesis module and supports flexible configuration of multimodal sensors to address both of these issues. For our view synthesis module we introduce novel depth refinement, adaptive view selection and layered rendering, to provide realistic imagery. We demonstrate that PreSim has several advantages: (i) it provides a photo-realistic 3D environment which allows seamlessly integrating multisensory models in the virtual world and enables them to perceive and navigate scenes, (ii) it has an internal view synthesis module which allows transforming algorithms developed and tested in simulation to physical platforms without domain adaption, (iii) it can generate a large amount of data for vision-based applications, such as depth estimation and object pose estimation.},
  keywords={Three-dimensional displays;Rendering (computer graphics);Visualization;Artificial intelligence;Solid modeling;Engines;Virtual environments;Simulation and animation;sensor fusion;RGB-D Perception},
  doi={10.1109/LRA.2021.3061994},
  ISSN={2377-3766},
  month={April},}@INPROCEEDINGS{9954679,
  author={Jo, Sihyeon and Yuan, Zhenyuan and Kim, Seong-Woo},
  booktitle={2022 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)}, 
  title={Interactive Storyboarding for Rapid Visual Story Generation}, 
  year={2022},
  volume={},
  number={},
  pages={1-4},
  abstract={Artificial Intelligence (AI) technologies have impacted almost every domain and its systems, including the entertainment industry. Although AI-based systems are expected to offer significant benefits in making content, it is still challenging to build a real-world AI application that can effectively contribute to content production. In this paper, we present a novel approach for developing a storyboard; a sequence of images displayed for previsualizing a motion picture, animation, motion graphic, or interactive media. We implement a prototype system, Gennie, that can interact with users and suggest AI-generated sketches for each scene of the storyboard.},
  keywords={Deep learning;Visualization;Prototypes;Entertainment industry;Production;Media;Motion pictures;Human-AI Interaction;multimodal embedding;large-scale pre-trained model},
  doi={10.1109/ICCE-Asia57006.2022.9954679},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9042158,
  author={Pascariu, Cristian and Barbu, Ionut-Daniel},
  booktitle={2019 11th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)}, 
  title={Ransomware Honeypot: Honeypot solution designed to detect a ransomware infection identify the ransomware family}, 
  year={2019},
  volume={},
  number={},
  pages={1-4},
  abstract={Ransomware poses a major threat to information integrity and availability. This type of malware will encrypt files on infected computers and demand ransom in return to restoring access. These scale to companies and institutions with devastating consequences. The goal of this paper is to propose a solution based on low cost hardware that will serve as a honeypot or trap for Ransomware viruses and it will alert defenders, giving them valuable data about the type of ransomware, which computer got infected first, and if there are any decryption software available.},
  keywords={Rasnsomware;hoenypots;samba;docker},
  doi={10.1109/ECAI46879.2019.9042158},
  ISSN={},
  month={June},}@INPROCEEDINGS{10934487,
  author={Zhou, Wei and Cang, Minnan},
  booktitle={2024 4th International Symposium on Artificial Intelligence and Intelligent Manufacturing (AIIM)}, 
  title={A Deep Learning-Based Non-Photorealistic Rendering (NPR) Generation Method}, 
  year={2024},
  volume={},
  number={},
  pages={981-984},
  abstract={With the advancement of deep learning technologies, non-photorealistic rendering (NPR) techniques have made significant breakthroughs in the field of computer graphics. This paper proposes a deep learning-based framework for NPR generation, combining convolutional neural networks (CNNs) and generative adversarial networks (GANs), aimed at automatically generating graphics in various artistic styles, such as sketches, ink paintings, and oil paintings. Through image style transfer and feature learning, the framework optimizes the preservation of details and enhances the artistic effects of the generated images. Experimental results demonstrate that the proposed method significantly outperforms traditional algorithms in terms of style transfer quality, generation efficiency, and adaptability. It shows strong potential for wide applications in animation, game design, and virtual reality, providing robust technical support for digital art creation.},
  keywords={Deep learning;Solid modeling;Digital art;Games;Rendering (computer graphics);Generative adversarial networks;Animation;Convolutional neural networks;Optimization;Painting;Non-Photorealistic Rendering;Deep Learning;Convolutional Neural Networks;Generative Adversarial Networks (GAN)},
  doi={10.1109/AIIM64537.2024.10934487},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10451613,
  author={Yang, Yuqi and Liu, Yuehu and Zhang, Chi},
  booktitle={2023 China Automation Congress (CAC)}, 
  title={Hierarchical Model-Based Imitation Learning for In-between Motion Synthesis}, 
  year={2023},
  volume={},
  number={},
  pages={9167-9171},
  abstract={Motion in-between problem is the key to solve the issue of long-term motion synthesis, which are widely applied in animation production, virtual reality, video games and film industry. Motion in-between can be defined as a process generating transitions between previous motion and future motion. Previous researchers focus on motion sequence generation, which omits the interaction with environment and can cause foot-sliding problem. In this work we present a novel, generic system based on imitation learning for motion in-between problem. This hierarchical system contains two parts: first part is motion primitives, which utilize reference motion to learn general knowledge of human motion; second part is policy controller, which uses multiple primitives to generate natural human motion. The primitives are controlled by the combine weights generated by policy controller. Experiment shows that our method can effectively improve motion quality and reduce the amount of reference motion data.},
  keywords={Solid modeling;Video games;Hierarchical systems;Graphics processing units;Virtual reality;Production;Parallel processing;Motion In-between;Hierarchical Model;Imitation Learning},
  doi={10.1109/CAC59555.2023.10451613},
  ISSN={2688-0938},
  month={Nov},}@INPROCEEDINGS{10920375,
  author={Ramchurn, Richard and Parkes, Joanne and Berger, Callum and Schiazza, Giovani},
  booktitle={2024 International Conference on Electrical and Computer Engineering Researches (ICECER)}, 
  title={Before We Disappear: The New Faces of Interactive Media}, 
  year={2024},
  volume={},
  number={},
  pages={1-12},
  abstract={Interactive films offer a novel viewing experience that diverges from traditional linear cinema. This paper presents an ethical approach to creating an adaptive interactive film using facial recognition software for emotion detection without personal data collection. We propose a new algorithm that dynamically determines scene order based on viewers' emotional responses, ensuring varied experiences across multiple screenings while maintaining narrative coherence. Our method addresses ethical concerns surrounding data protection and AI use in media. Quantitative analysis shows 91.5% facial recognition accuracy in cinematic environments and 87.6% viewer engagement rates. Initial testing demonstrates significant improvements in scene order generation and viewer satisfaction compared to traditional interactive films. This research contributes to the growing field of affective computing in interactive media, exploring the balance between personalisation and privacy.},
  keywords={Ethics;Emotion recognition;Statistical analysis;Films;Face recognition;Software algorithms;Media;Motion pictures;Software;Testing;interactive media;emotion recognition;brain-controlled films;personality-genre preference;valence and arousal;adaptive storytelling;ethical AI;personalised media;privacy-preserving technology;audience engagement;affective film-making;dynamic narrative},
  doi={10.1109/ICECER62944.2024.10920375},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10211027,
  author={Dharmalingam, Balakrishnan and Odat, Ibrahim and Mukherjee, Rajdeep and Piggott, Brett and Liu, Anyi},
  booktitle={2023 IEEE International Conference on Mobility, Operations, Services and Technologies (MOST)}, 
  title={Heterogeneous Generative Dataset for UASes}, 
  year={2023},
  volume={},
  number={},
  pages={229-230},
  abstract={In this poster, we present the construction of HGDAVE (Heterogeneous Generative Dataset for Unmanned Autonomous Systems), a new dataset for Connected and Autonomous Vehicles (CAVs) and Unmanned Aerial Vehicles (UAVs), namely Unmanned Autonomous Systems (UASes). The dataset will be used to train artificial intelligence (AI) models to detect cybersecurity and safety-related risks, malfunctions, and crashes. The dataset was collected from three sources: 1) script-generated flying or driving missions, 2) software fuzzer-generated crashes instances, and 3) cybersecurity exploits generated by ethical hackers. To collect the data, we utilized the Digital Twin (DT) to replicate the behavior of UASes, which provides data that can be used to analyze, develop, and detect new anomaly detection algorithms.},
  keywords={Ethics;Connected vehicles;Computer hacking;Software algorithms;Autonomous aerial vehicles;Software;Digital twins;UAS;Digital Twin;Unmanned Aerial Vehicles;Hardware in the loop;Software in the loop;Generative AI},
  doi={10.1109/MOST57249.2023.00034},
  ISSN={},
  month={May},}@INPROCEEDINGS{10377273,
  author={Peng, Ziqiao and Wu, Haoyu and Song, Zhenbo and Xu, Hao and Zhu, Xiangyu and He, Jun and Liu, Hongyan and Fan, Zhaoxin},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation}, 
  year={2023},
  volume={},
  number={},
  pages={20630-20640},
  abstract={Speech-driven 3D face animation aims to generate realistic facial expressions that match the speech content and emotion. However, existing methods often neglect emotional facial expressions or fail to disentangle them from speech content. To address this issue, this paper proposes an end-to-end neural network to disentangle different emotions in speech so as to generate rich 3D facial expressions. Specifically, we introduce the emotion disentangling encoder (EDE) to disentangle the emotion and content in the speech by cross-reconstructed speech signals with different emotion labels. Then an emotion-guided feature fusion decoder is employed to generate a 3D talking face with enhanced emotion. The decoder is driven by the disentangled identity, emotional, and content embeddings so as to generate controllable personal and emotional styles. Finally, considering the scarcity of the 3D emotional talking face data, we resort to the supervision of facial blendshapes, which enables the reconstruction of plausible 3D faces from 2D emotional data, and contribute a large-scale 3D emotional talking face dataset (3D-ETF) to train the network. Our experiments and user studies demonstrate that our approach outperforms state-of-the-art methods and exhibits more diverse facial movements. We recommend watching the supplementary video: https://ziqiaopeng.github.io/emotalk},
  keywords={Solid modeling;Computer vision;Three-dimensional displays;Neural networks;Fires;Virtual reality;User experience},
  doi={10.1109/ICCV51070.2023.01891},
  ISSN={2380-7504},
  month={Oct},}@ARTICLE{10778142,
  author={Liu, Zhe and Bao, Yihang and Zeng, Shuai and Qian, Ruiyi and Deng, Miaohan and Gu, An and Li, Jianye and Wang, Weidi and Cai, Wenxiang and Li, Wenhao and Wang, Han and Xu, Dong and Lin, Guan Ning},
  journal={Big Data Mining and Analytics}, 
  title={Large Language Models in Psychiatry: Current Applications, Limitations, and Future Scope}, 
  year={2024},
  volume={7},
  number={4},
  pages={1148-1168},
  abstract={With the advancements in Artificial Intelligence (AI) technology, Large Language Models (LLMs) provide outstanding capabilities for natural language understanding and generation, enhancing various domains. In psychiatry, LLMs can empower healthcare by analyzing vast amounts of medical data to improve diagnostic accuracy, enhance therapeutic communication, and personalize patient care with their strength in understanding and generating human-like text. In clinical AI, developing and utilizing robust and interpretable models has been a longstanding challenge. This survey investigates the current psychiatric practice of LLMs, along with a series of corpus resources that could be used for training psychiatric LLMs. We discuss the limitations concerning LLM reproducibility, capabilities, usability, interpretability in clinical settings, and ethical considerations. Additionally, we propose potential future directions for research, clinical application, and education in psychiatric LLMs. Finally, we discuss the challenge of integrating LLMs into the evolving landscape of healthcare in real-world scenarios.},
  keywords={Training;Surveys;Ethics;Large language models;Mental health;Medical services;Reproducibility of results;Data models;Psychiatry;Usability;Artificial Intelligence (AI);Large Language Model (LLM);psychiatry;medical application},
  doi={10.26599/BDMA.2024.9020046},
  ISSN={2097-406X},
  month={December},}@INPROCEEDINGS{9578462,
  author={Zhang, Zhimeng and Li, Lincheng and Ding, Yu and Fan, Changjie},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Flow-guided One-shot Talking Face Generation with a High-resolution Audio-visual Dataset}, 
  year={2021},
  volume={},
  number={},
  pages={3660-3669},
  abstract={One-shot talking face generation should synthesize high visual quality facial videos with reasonable animations of expression and head pose, and just utilize arbitrary driving audio and arbitrary single face image as the source. Current works fail to generate over 256×256 resolution realistic-looking videos due to the lack of an appropriate high-resolution audio-visual dataset, and the limitation of the sparse facial landmarks in providing poor expression details. To synthesize high-definition videos, we build a large in-the-wild high-resolution audio-visual dataset and propose a novel flow-guided talking face generation framework. The new dataset is collected from youtube and consists of about 16 hours 720P or 1080P videos. We leverage the facial 3D morphable model (3DMM) to split the framework into two cascaded modules instead of learning a direct mapping from audio to video. In the first module, we propose a novel animation generator to produce the movements of mouth, eyebrow and head pose simultaneously. In the second module, we transform animation into dense flow to provide more expression details and carefully design a novel flow-guided video generator to synthesize videos. Our method is able to produce high-definition videos and outperforms state-of-the-art works in objective and subjective comparisons*.},
  keywords={Visualization;Solid modeling;Three-dimensional displays;Image resolution;Face recognition;Mouth;Transforms},
  doi={10.1109/CVPR46437.2021.00366},
  ISSN={2575-7075},
  month={June},}@ARTICLE{9055736,
  author={Qian, Yongfeng and Jiang, Yingying and Hu, Long and Hossain, M. Shamim and Alrashoud, Mubarak and Al-Hammadi, Muneer},
  journal={IEEE Network}, 
  title={Blockchain-Based Privacy-Aware Content Caching in Cognitive Internet of Vehicles}, 
  year={2020},
  volume={34},
  number={2},
  pages={46-51},
  abstract={The Cognitive Internet of Vehicles (CIoV) introduces a cognitive engine in the traditional Internet of Vehicles, which can realize more intelligent functions such as vehicle deployment and resource allocation. Especially in terms of content caching, the cognitive engine can perceive the content requirements of users and match the content providers and content requesters to improve the caching hit rate. However, during this process, users may worry that their privacy data may be leaked. Content requesters need to submit data about points of interest in contents, which is part of their sensitive information. In addition, in terms of rapid speed of vehicles, the connection time is limited, such as vehicle-to-vehicle and vehicle-to-roadside unit (RSU), which leads to limited transaction time during obtaining contents. In order to solve these problems, in this article, we propose a blockchain-based privacy-aware content caching architecture in CIoV. In general, when vehicles need contents, in order to protect the privacy of vehicles, it is no longer necessary to submit a request to an RSU, but through broadcasting contents from an RSU or surrounding vehicles, where vehicles can selectively obtain contents. To improve the cache hit rate, the cognitive engine will perceive content requirements and recommend relevant content requirements to an RSU or content-providing vehicles based on machine learning or deep learning methods. However, this method of content acquisition will connect different vehicles, which brings the untrusted problem. Both content requesters and providers may worry about untrusted users connected with them. To this end, we adopt the blockchain technology to record the completed content transactions, which are written into the block after the consensus mechanism is completed, thus solving the problem of distrust between vehicles. Experiments demonstrate the privacy-aware content caching architecture based on blockchains effectiveness.},
  keywords={Engines;Privacy;Blockchain;Artificial intelligence;Internet;Data privacy;Vehicular ad hoc networks},
  doi={10.1109/MNET.001.1900161},
  ISSN={1558-156X},
  month={March},}@INPROCEEDINGS{10968614,
  author={N, Kishore and S, Lalitha},
  booktitle={2025 IEEE 14th International Conference on Communication Systems and Network Technologies (CSNT)}, 
  title={Humanizing NPCs-AI Driven Realistic Conversations and Reactions}, 
  year={2025},
  volume={},
  number={},
  pages={788-793},
  abstract={This paper introduces a novel framework for developing advanced non-playable characters (NPCs) that utilize artificial intelligence (AI) to achieve realistic interactions in virtual environments made using Unreal Engine. NPCs often rely on defined behaviors and scripted responses, which limit immersion and also give the same experience for every user, but this approach integrates Natural Language Processing (NLP) features and machine learning techniques to create NPCs capable of dynamic, conscious conversations that mimic human interaction to respond with emotional depth and adaptability making the experience unique for every user. By crafting a custom dataset that encapsulates distinct personality traits. Incorporating memory recall allows Non Playable Characters to learn from past interactions, enhancing their ability to provide personalized experiences over time. AI-driven NPCs significantly enhance user immersion by offering unique, evolving interactions tailored to individual players. This research not only advances the field of game development but also lays the framework for creating lifelike digital characters capable of enriching virtual narratives for game plots.},
  keywords={Measurement;Vocoders;Refining;Virtual environments;Games;Oral communication;Natural language processing;Speech synthesis;Optimization;Testing},
  doi={10.1109/CSNT64827.2025.10968614},
  ISSN={2473-5655},
  month={March},}
