@inproceedings{10.1145/3639701.3656325,
author = {Weber, Christoph Johannes and Burgkart, Sebastian and Rothe, Sylvia},
title = {wr-AI-ter: Enhancing Ownership Perception in AI-Driven Script Writing},
year = {2024},
isbn = {9798400705038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639701.3656325},
doi = {10.1145/3639701.3656325},
abstract = {The integration of artificial intelligence (AI) into creative domains is increasing, presenting both challenges and opportunities. In screenwriting, personal artistic expression is a fundamental aspect of the creator’s identity and work. The current use of AI in such creative processes can sometimes overshadow the creator’s vision and lead to a reduced sense of ownership over the final product. We introduce wr-AI-ter, an interactive application consisting of four basic stages: Ideation, Structure, Refinement, and Export. While some related work focuses on experts The application is intended to aid users with varying levels of screenwriting proficiency in generating screenplays using artificial intelligence, while preserving their sense of authorship. We conducted a user study with 23 participants, who had different expertise (screenwriting, documentary filmmaking, and VFX artistry). The results indicate that AI has the potential to accelerate the screenwriting process and improve the quality of scripts without compromising the sense of ownership.},
booktitle = {Proceedings of the 2024 ACM International Conference on Interactive Media Experiences},
pages = {145–156},
numpages = {12},
keywords = {computational creativity, human-computer interaction, natural language evaluation, natural language generation, ownership, screenplay},
location = {Stockholm, Sweden},
series = {IMX '24}
}

@inproceedings{10.1145/3672758.3672763,
author = {Peng, Li and Fu, Bihan},
title = {Development and Research of Generative Animation Based on AIGC},
year = {2024},
isbn = {9798400716942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672758.3672763},
doi = {10.1145/3672758.3672763},
abstract = {AIGC is the automatic generation of relevant content from a large number of models and databases based on the input of relevant instructions. This paper mainly discusses the development of artificial intelligence generation technology in animation in the form of images. With the rapid development of artificial intelligence technology, generative animation content has become an important way of animation production and production in modern society. Artificial intelligence-based generative animation is taking up an increasing proportion of the film, game, advertising and other industries, AIGC has been rapidly developed by virtue of its lower production cost and higher production efficiency, which constantly impacts the development of the animation industry and attracts the widespread attention of animation creators. This paper analyses the Generative Adversarial Network and Diffusion Model through the deep learning principle of AIGC technology, and explores the important impact of AIGC technology on the form of generative animation. Using computer discipline thinking to analyse the type of AIGC model for animation creation, and according to the model simulation results of AIGC technology used in generative animation production of technology, market aspects of its development prospects are summarised.},
booktitle = {Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering},
pages = {22–28},
numpages = {7},
location = {Xi' an, China},
series = {CAICE '24}
}

@inproceedings{10.1145/3641235.3664438,
author = {Kicklighter, Caleb and Seo, Jinsil Hwaryoung and Andreassen, Mayet and Bujnoch, Emily},
title = {Empowering Creativity with Generative AI in Digital Art Education},
year = {2024},
isbn = {9798400705175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641235.3664438},
doi = {10.1145/3641235.3664438},
abstract = {Artificial intelligence is dramatically changing the creative process for many practices. We see this as an opportunity to enrich student projects within our classroom. We created educational materials and conducted an initial study in the Fall of 2023. The study focuses on the impact that image-based generative AI tools could have on the creative process for students in the 3D Animation classroom. We found that, within our class, most students found AI useful for their productivity, but further work was needed to educate students and to create a safe space for students to explore how these tools can enhance their creative work.},
booktitle = {ACM SIGGRAPH 2024 Educator's Forum},
articleno = {13},
numpages = {2},
keywords = {3D Animation Education, Concept Development, Creativity, Generative AI, Iteration, Undergraduate Digital Art Education},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@inproceedings{10.1145/3706598.3714253,
author = {Oh, Jeongseok and Kim, SeungJun},
title = {MoWa: An Authoring Tool for Refining AI-Generated Human Avatar Motions Through Latent Waveform Manipulation},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714253},
doi = {10.1145/3706598.3714253},
abstract = {Creating expressive and realistic motion animations is a challenging task. Generative artificial intelligence (AI) models have emerged to address this challenge, offering the capability to synthesize human motion animations from text prompts. However, the effective integration of AI-generated motion into professional designer workflows remains uncertain. This study proposes MoWa, an authoring tool designed to refine AI-generated human motions to meet professional standards. A formative study with six professional motion designers identified the strengths and weaknesses of AI-generated motions. To address these weaknesses, MoWa utilizes latent space to enhance the expressiveness of motions, making them suitable for use in professional workflows. A user study involving twelve professional motion designers was conducted to evaluate MoWa’s effectiveness in refining AI-generated motions. The results indicated that MoWa streamlines the motion design process and improves the quality of the outcomes. These findings suggest that incorporating latent space into motion design tasks can improve efficiency.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1240},
numpages = {21},
keywords = {creativity support tool, graphics design, artificial intelligence},
location = {
},
series = {CHI '25}
}

@inbook{10.1145/3643834.3660685,
author = {Halperin, Brett A. and Lukin, Stephanie M},
title = {Artificial Dreams: Surreal Visual Storytelling as Inquiry Into AI 'Hallucination'},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3660685},
abstract = {What does it mean for stochastic artificial intelligence (AI) to “hallucinate” when performing a literary task as open-ended as creative visual storytelling? In this paper, we investigate AI “hallucination” by stress-testing a visual storytelling algorithm with different visual and textual inputs designed to probe dream logic inspired by cinematic surrealism. Following a close reading of 100 visual stories that we deem artificial dreams, we describe how AI “hallucination” in computational visual storytelling is the opposite of groundedness: literary expression that is ungrounded in the visual or textual inputs. We find that this lack of grounding can be a source of either creativity or harm entangled with bias and illusion. In turn, we disentangle these obscurities and discuss steps toward addressing the perils while harnessing the potentials for innocuous cases of AI “hallucination” to enhance the creativity of visual storytelling.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {619–637},
numpages = {19}
}

@inproceedings{10.1145/3605390.3605398,
author = {Di Mascio, Tania and Caruso, Federica and Peretti, Sara},
title = {How to Make an Artificial Intelligence Algorithm “Ecological”? Insights from a holistic perspective},
year = {2023},
isbn = {9798400708060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605390.3605398},
doi = {10.1145/3605390.3605398},
abstract = {Nowadays, Artificial Intelligence is growing in many daily activities. On the one hand, it has many positive effects and produces social benefits. On the other hand, its development and deployment raise issues related to biases, such as gender, disability, and culture. Moreover, Artificial Intelligence’s growing autonomy in decision-making could lead to decisions that conflict with human values or harm individuals and society. These issues stem from biased or incomplete datasets and a lack of transparency and accountability in the algorithms. Consequently, paying increasing attention to the ongoing discourse on Artificial Intelligence ethics: its autonomy in decision-making, and biases is necessary. A human-centric approach is a minimum requirement for designing algorithms since this approach is aligned with human values, dignity, and goals. Notwithstanding, its application does not guarantee a deep understanding of the context of use. According to recent theoretical perspectives, a deep interpretation of the context of use (i.e., a holistic perspective) could better regulate ethical aspects. This paper goes in this direction, presenting a human-centric and ecological approach as a design methodology. It has been experienced within Use Case 6 of the European FRACTAL project, which aims to develop intelligent totems for advertising and customer assistance in sentient shopping malls. The intelligence is realized by several artificial intelligent algorithms (e.g., gender recognition algorithms). By adopting Bronfenbrenner’s ecological approach, algorithms were made free from gender bias, mirroring the context of men’s and women’s use at shopping malls as it is currently, i.e., characterized by gender balance. This proposal contributes to the ongoing discourse on Artificial Intelligence ethics and the development of its ethical algorithms.},
booktitle = {Proceedings of the 15th Biannual Conference of the Italian SIGCHI Chapter},
articleno = {21},
numpages = {7},
keywords = {Human-Centric Approach, Holistic Perspective, Ecological Approach, Design Methodology, Artificial Intelligence},
location = {Torino, Italy},
series = {CHItaly '23}
}

@inproceedings{10.1145/3641235.3664452,
author = {DeYoung, Johannes and Davis, Claudia and Fanelli, Clinton and Stroud, Mike},
title = {Student / Futures: Creative Careers in Animation, Computer Graphics, and Interactive Techniques},
year = {2024},
isbn = {9798400705175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641235.3664452},
doi = {10.1145/3641235.3664452},
abstract = {Industry panelists share perspectives and insights for students, educators, and creative professionals who are considering careers in animation, computer graphics, creative technologies, and interactive techniques. Within rapid and ever-expanding fields of technological change, many creative industries are reaching critical inflection points. Changing workplace cultures, advances in machine learning and artificial intelligence, real-time graphics and virtual production systems, edge computing and ever-faster network communications are all radically transforming the forms and capacities of creative industries and cultural production. A variety of opportunities and unforeseen challenges are exposed along the way. In this panel, creative industry representatives discuss the general and specific states of their fields, providing insights into changing career paradigms. Discussion includes advice for educators to help prepare students to meet the challenges and opportunities that currently face creative industries, as well as the preparation and training needed to anticipate change. Panelists consider what qualities make for desirable applicants in their respective fields, with insights for individuals who are preparing to enter creative industries, as well as for those considering career transitions. Represented industry segments include animation and VFX, computer graphics and information systems, themed entertainment, and interactive educational technologies. Questions considered include how pedagogy can help prepare and empower students for successful creative careers; what entry-level applicants should have (and should not have) on resumes, portfolios, and demo reels; and what can creative talents do to proactively acquire requisite credentials. Discussion will expose fresh outlooks on the futures of creative fields in animation, computer graphics, and interactive techniques.},
booktitle = {ACM SIGGRAPH 2024 Educator's Forum},
articleno = {1},
numpages = {2},
keywords = {Creative Careers, Demo Reels, Education, Entry-level Employment},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@inproceedings{10.1145/3615522.3615556,
author = {Song, Junrong and Wang, Bingyuan and Wang, Zeyu and Yip, David Kei-Man},
title = {From Expanded Cinema to Extended Reality: How AI Can Expand and Extend Cinematic Experiences},
year = {2023},
isbn = {9798400707513},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615522.3615556},
doi = {10.1145/3615522.3615556},
abstract = {This paper explores the concept of expanded cinema and its relationship to extended reality (XR), focusing on the potential of artificial intelligence (AI) to expand and extend expressive possibilities. Expanded cinema refers to experimental film and multimedia art forms that challenge the conventions of traditional cinema by creating immersive and interactive experiences for audiences. XR, on the other hand, blurs the line between physical and virtual reality, offering immersive storytelling experiences. Both expanded cinema and XR aim to push the boundaries of traditional norms and create immersive experiences through the integration of technology, interactivity, and cross-sensory elements. The paper emphasizes the role of AI in optimizing 3D scene creation for XR and enhancing the overall experience through a case study. It also presents several AI-based techniques, such as generative models and AI-assisted rendering, that facilitate efficient and effective 3D content creation. Additionally, it explores the use of AI plugins in 3D modeling software and the generation of 3D models and textures from 2D images using techniques like GANs and VAEs. The incorporation of AI to extend and expand opens up new possibilities for immersive experiences in the future.},
booktitle = {Proceedings of the 16th International Symposium on Visual Information Communication and Interaction},
articleno = {34},
numpages = {5},
keywords = {expanded cinema, artificial intelligence., Extended reality},
location = {Guangzhou, China},
series = {VINCI '23}
}

@inproceedings{10.1145/3689236.3689257,
author = {Tan, Xiaojun},
title = {AI Digital Anime Style Generation Algorithm Based on Adversarial Generative Network},
year = {2024},
isbn = {9798400718137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689236.3689257},
doi = {10.1145/3689236.3689257},
abstract = {Digital computer technology is changing the development of animation image processing and significantly improving the efficiency of animation creation. Among them, traditional artificial animation faces problems such as low efficiency and poor quality in image drawing. Therefore, a research proposes a digital animation style generation model based on artificial intelligence. First, the animation style generation model is constructed based on the generative adversarial network. Considering the gradient vanishing problem, the residual network is introduced to perfect the model, and the attention mechanism is introduced to correct the image deviation problem. In the two scenarios of contour feature extraction and comic style transfer, the image loss of the research model is 0.012 and 0.038 respectively, which is better than similar models. In addition, in the comparison of animation style conversion quality, the research model handles the details of animation images better, and its peak signal-to-noise ratio is 23.05db, which is better than similar models. It can be seen that the research technology is superior to similar technologies in the field of animation creation and has good application effects. The research content will provide a technical reference for intelligent animation creation.},
booktitle = {Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering},
pages = {407–412},
numpages = {6},
keywords = {Animation, Artificial intelligence, Attention mechanism, Generative adversarial network, Residual network},
location = {
},
series = {ICCSIE '24}
}

@article{10.1145/3716135,
author = {Halperin, Brett A. and Rosner, Daniela K.},
title = {“AI is Soulless”: Hollywood Film Workers’ Strike and Emerging Perceptions of Generative Cinema},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/3716135},
doi = {10.1145/3716135},
abstract = {Why were Hollywood film workers striking or supporting strikes against AI in 2023? To investigate this question, we conduct participant observation on the picket line and interview 15 film workers, including 12 union members from SAG-AFTRA, WGA, and IATSE, as well as three non-unionized workers, across roles. From screenwriting to acting, our interlocutors described how studio use of AI might exacerbate wage squeeze, estrangement from embodied co-creation, rush for results, and inauthentic creativity. We find that film worker resistance to emergent and projected uses of AI echoes earlier technical developments, such as the incorporation of sound, color, HD, DVD, and CGI. These innovations initially sparked anxieties about the demise of cinema, but ultimately created new aesthetic possibilities and professions. We end with a reflection on core concerns for worker engagement, including topics of prophesy and the “soul” of sociotechnical labor.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = apr,
articleno = {19},
numpages = {27},
keywords = {AI Art, Artificial Intelligence, Algorithmic Resistance, Cinema, Cinematography, Computational Creativity, Creative Labor, Generative AI, Film, Filmmaking, Future of Work, IATSE, Labor, Labor Organizing, Resistance, Unions, SAG-AFTRA, Soul, Text-to-Image, Text-to-Video, WGA, Work}
}

@inproceedings{10.1145/3661167.3661192,
author = {Leotta, Maurizio and Yousaf, Hafiz Zeeshan and Ricca, Filippo and Garcia, Boni},
title = {AI-Generated Test Scripts for Web E2E Testing with ChatGPT and Copilot: A Preliminary Study},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661192},
doi = {10.1145/3661167.3661192},
abstract = {Automated testing is vital for ensuring the reliability of web applications. This paper presents a preliminary study on leveraging artificial intelligence (AI) models, specifically ChatGPT and Github Copilot, to generate test scripts for web end-to-end testing. Through experimentation, we evaluated the feasibility and effectiveness of AI language models in generating test scripts based on natural language descriptions of user interactions with web applications. Our preliminary results show that AI-based generation generally provides an advantage over fully manual test scripts development. Starting from test cases clearly defined in Gherkin, a reduction in development time is always observable. In some cases, this reduction is statistically significant (e.g., Manual vs. a particular use of ChatGPT). These results are valid provided that the tester has some skills in manual test script development and is therefore able to modify the code produced by the AI-generation tools. This study contributes to the exploration of AI-driven solutions in web test scripts generation and lays the foundation for future research in this domain.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {339–344},
numpages = {6},
keywords = {ChatGPT, E2E Testing, Empirical Study., GitHub Copilot, LLM, Selenium WebDriver, Test Automation},
location = {Salerno, Italy},
series = {EASE '24}
}

@inproceedings{10.1145/3678717.3695760,
author = {Zhang, Qianheng and Gao, Song},
title = {Automating Geospatial Analysis Workflows Using ChatGPT-4},
year = {2024},
isbn = {9798400711077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678717.3695760},
doi = {10.1145/3678717.3695760},
abstract = {The field of Geospatial Artificial Intelligence (GeoAI) has significantly impacted domain applications such as urban analytics, environmental monitoring, and disaster management. While powerful geoprocessing tools in geographic information systems (GIS) like ArcGIS Pro are available, automating these workflows with Python scripting using AI chatbots remains a challenge, especially for non-expert users. This study investigates whether ChatGPT-4 can automate GIS workflows by generating ArcPy functions based on structured instructions. We tested prompt engineering's ability on helping large language models (LLMs) understand spatial data and GIS workflows. The overall task success rate reaches 80.5%. It is a valid and easy to implement approach for domain scientists who want to use ArcPy to automate their workflows.},
booktitle = {Proceedings of the 32nd ACM International Conference on Advances in Geographic Information Systems},
pages = {715–716},
numpages = {2},
keywords = {GIS, GeoAI, LLM, Prompt engineering, automate workflow},
location = {Atlanta, GA, USA},
series = {SIGSPATIAL '24}
}

@inproceedings{10.1145/3632621.3671422,
author = {Castro, Francisco Enrique Vicente G. and DesPortes, Kayla},
title = {Supporting the Development of AI Literacy Competencies Within Creative Computing Environments},
year = {2024},
isbn = {9798400704765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632621.3671422},
doi = {10.1145/3632621.3671422},
abstract = {Creative computing education can support engagement and learning across artistic and computational disciplines, while providing opportunities for meaningful expression. Prior work has shown that constructing with electronic textiles [6, 12] (alongside restorying practices [13]) and physical computing kits [7] empower learners to highlight personal and community values and stories while challenging and reimagining dominant narratives. The rapid expansion of artificial intelligence (AI) and machine learning (ML) in recent years has created an influx of attention on how to teach AI across a variety of learners. Work on AI literacy often aims to equip learners with interdisciplinary skills to use and produce with AI technologies while also being critical about their designs. These have led to the development of frameworks to describe and outline what constitutes AI literacy [1, 8, 14] and the exploration of creative computing as an avenue for AI/ML education. While research has shown some benefits of creative computing for AI education, such as by facilitating sensemaking of AI through embodied exploration and co-creation and the critical reflection on the limits, tensions, and ethics around AI [2, 15, 16], there is limited work on identifying the specific AI competencies that creative computing tools and environments support. To investigate this, we partnered with educators and community organizations to create co-design [11] spaces for learners and educators to learn and reason about ML while engaging in creative making through dance, visual art, and poetry. Within these spaces, participants used danceON, a web-based creative coding environment that uses pose detection algorithms and enables users to code responsive animations over user-produced videos [3, 9]. One co-design space involved two co-design workshops where instructors from a dance education program at New York University created movement-based performances with danceON to build connections across dance and ML. Another co-design space consisted of 19 remote, two-hour sessions where high school students from Community-Word Project [10], a community organization that engage students in interdisciplinary arts education, used danceON to augment visual art and poetry-reading performances with animations. The following research questions guide our work: RQ1. How do we create learning designs that integrate the learning of art and ML? RQ2. What AI competencies were supported during participants’ engagement with creative computing tools and environments? We use distributed cognition [4, 5] as a lens to focus on the interactions and information processing across the participants and danceON as they worked with danceON’s pose detection algorithm in their creative making. We use the AI literacy competencies conceptual framework by Long and Magerko [8] to identify the specific AI competencies that were supported during the sessions. Early findings show that encountering the limits of pose detection algorithms during artistic creation supported the practice of specific AI literacy competencies such as recognizing the impact of human and sociopolitical factors on AI designs, creating connections across computer sensing and the generation of representations (e.g., of the body), and the selection and curation of data as part of ML. Our work demonstrates how we can meaningfully engage learners and educators within creative computing spaces in ways that draw on their situated knowledge, practices, and experiences to critique and learn about AI/ML, computing, and art.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 2},
pages = {529–530},
numpages = {2},
keywords = {AI education, AI literacy, Creative computing, artificial intelligence, co-design, computing education, dance, dance computing, dance education, danceON, distributed cognition, machine learning, poetry, pose detection, visual art},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@inproceedings{10.1145/3664647.3681516,
author = {Guo, Jiaqi and Gao, Lianli and Zhu, Junchen and Zhang, Jiaxin and Li, Siyang and Song, Jingkuan},
title = {MagicVFX: Visual Effects Synthesis in Just Minutes},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681516},
doi = {10.1145/3664647.3681516},
abstract = {Visual effects synthesis is crucial in the film and television industry, which aims at enhancing raw footage with virtual elements for greater expressiveness. As the demand for detailed and realistic effects escalates in modern production, professionals are compelled to allocate substantial time and resources to this endeavor. Thus, there is an urgent need to explore more convenient and less resource-intensive methods, such as incorporating the burgeoning Artificial Intelligence Generated Content (AIGC) technology. However, research into this potential integration has yet to be conducted. As the first work to establish a connection between visual effects synthesis and AIGC technology, we start by carefully setting up two paradigms according to the need for pre-produced effects or not: synthesis with reference effects and synthesis without reference effects. Following this, we compile a dataset by processing a collection of effects videos and scene videos, which contains a wide variety of effect categories and scenarios, adequately covering the common effects seen in films and television industry. Furthermore, we explore the capabilities of a pre-trained text-to-video model to synthesize visual effects within these two paradigms. The experimental results demonstrate that the pipeline we established can effectively produce impressive visual effects synthesis outcomes, thereby evidencing the significant potential of existing AIGC technology for application in visual effects synthesis tasks. Our dataset can be found in https://github.com/ruffiann/MagicVFX.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {8238–8246},
numpages = {9},
keywords = {diffusion models, video synthesis, visual effects},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/3700794.3700810,
author = {Portela, Carlos and Palomino, Paula and Challco, Geiser and Sobrinho, \'{A}lvaro and Cordeiro, Thiago and Mello, Rafael and Dermeval, Diego and Bittencourt, Ig and Isotani, Seiji},
title = {AI in Education Unplugged Support Equity Between Rural and Urban Areas in Brazil},
year = {2025},
isbn = {9798400710414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3700794.3700810},
doi = {10.1145/3700794.3700810},
abstract = {The AIED Unplugged concept leverages Artificial Intelligence (AI) technologies without requiring constant internet access or substantial infrastructure modifications, making it particularly suitable for underserved regions. This paper explores the implementation and impact of the AIED Unplugged solution in Brazil’s elementary schools, focusing on its role in reducing educational inequalities between rural and urban areas. The implemented AIED solution has key features, such as offline functionality, compatibility with low-cost devices, and integration of handwriting recognition and text production assessment using Computer Vision and Natural Language Processing (NLP). Our impact analysis reveals that 8,238 schools adopted this technology, benefiting 164,545 students through personalized feedback and support to improve their writing skills. Data analysis shows that our AIED solution effectively supports students from rural and urban areas, with no significant differences in writing skill improvements, thus demonstrating its potential for equitable educational support. We noticed that automated pedagogical assessments, printable paper-based dashboards, and scalable AI techniques ensure that students, regardless of location, receive personalized support without overburdening teachers. The findings provide valuable insights for policymakers and educators striving to bridge the educational divide.},
booktitle = {Proceedings of the 13th International Conference on Information &amp; Communication Technologies and Development},
pages = {143–154},
numpages = {12},
keywords = {AI in Education, Equity, Writing Skills},
location = {
},
series = {ICTD '24}
}

@inproceedings{10.1145/3696230.3696231,
author = {Wang, Xiaozhu and Liu, Shengzhuo and Adams, Paul},
title = {Design and Implementation of an AI-Enhanced Heuristic Learning System in a Virtual Realm},
year = {2024},
isbn = {9798400717574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696230.3696231},
doi = {10.1145/3696230.3696231},
abstract = {This paper explores how artificial intelligence (AI) and Virtual Reality (VR) technology can be integrated with heuristic learning methods in a virtual realm. Heuristic learning emphasizes the promotion of deeper understanding and internalization of knowledge through discovery, problem-solving, and practice. This paper outlines AI-enhanced heuristic learning strategies, highlighting how AI integration enables customized learning paths, immersive simulation environments, intelligent feedback and evaluation, auxiliary functions, and data-driven insights. The designed system structure includes modules for data collection, feature engineering, model training and deployment, knowledge base creation, and user interface. Examples of intelligent learning companions demonstrate the practical application of AI-assisted tools in providing real-time heuristic guidance and personalized support in animation design courses. Through experimental comparative analysis, it is concluded that combining heuristic methods with AI-supported environments can enhance the learning experience of students, providing valuable insights for future courses in terms of participation, efficiency, customization, and knowledge internalization.},
booktitle = {Proceedings of the 2024 8th International Conference on Digital Technology in Education (ICDTE)},
pages = {255–263},
numpages = {9},
keywords = {Adaptive learning, Artificial intelligence, Conversational agents, Heuristic learning, Knowledge tracing},
location = {
},
series = {ICDTE '24}
}

@inproceedings{10.1145/3626772.3661350,
author = {Zhao, Kang and Zhao, Xinyu and Jin, Zhipeng and Yang, Yi and Tao, Wen and Han, Cong and Li, Shuanglong and Liu, Lin},
title = {Enhancing Baidu Multimodal Advertisement with Chinese Text-to-Image Generation via Bilingual Alignment and Caption Synthesis},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3661350},
doi = {10.1145/3626772.3661350},
abstract = {Recent advances in generative artificial intelligence have revolutionized information retrieval and content generation, opening up new opportunities for the e-commerce industry. In particular, text-to-image generation models offer a novel approach to guiding the image generation process using natural language input, which is inspiring for multimodal search advertising. Traditional multimodal search ads require advertisers to prepare ad creatives, such as ad images, which is time-consuming and requires uniform image specifications and content quality inspection. To this end, we propose a streamlined generation framework for search ad image creatives. First, we prepare a Chinese image caption model with high-quality image-caption pairs to bootstrap training data refinement. With curated high-quality images and synthesized descriptive captions, we then train a Chinese text-to-image generation model, the largest to date, using SDXL and a 10-billion multimodal text encoder. Specifically, we introduce a two-stage bilingual multimodal representation alignment process to seamlessly integrate the text encoder with the generation model. Extensive experiments validate the effectiveness of our framework, including assessments of image captioning and image generation. The implementation of our framework in Baidu Search Ads shows significant revenue increase, For example, beauty industry ads with generated image creatives achieve a 29% higher click-through rate (CTR).},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2855–2859},
numpages = {5},
keywords = {advertisement image creatives, multimodal sponsored search, text-to-image generation},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3349341.3349478,
author = {Chen, Keqin and Meng, Yixin and Zhang, Gongfan and Zhu, Kun and Luo, Wenbin},
title = {The Transfer of Film Style Based on Meta-Learning},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349478},
doi = {10.1145/3349341.3349478},
abstract = {The film style refers to the combination and configuration of colors in the film, which is often dominated by one color, making the picture show a certain tendency. However, the creation of special effects not only requires special professional skills, but also takes a lot of manual labor. If artificial intelligence technology can be transferred to the picture style of the film industry, production costs will be greatly reduced. In this paper, we propose a technique which combines the style transfer and meta-learning to create a new way of thinking. Compared with traditional image style transfer, the transfer of film style based on meta-learning could save the cost of film production significantly and take much less time to perform the transfer process. Toward the end, extensive experimental results were presented to validate our proposed method, which clearly outperforms the traditional image style transfer.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {616–620},
numpages = {5},
keywords = {Transfer Learning, Neural Network, Meta-learning, Film Style},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@inproceedings{10.1145/3550453.3570125,
author = {Chae, Chuck and Song, Patrick and Song, Dahun},
title = {AI driven Live Interactive Cinematic Experience: Aria Studios' Virtual Movie Pipeline},
year = {2023},
isbn = {9781450394680},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550453.3570125},
doi = {10.1145/3550453.3570125},
abstract = { 'The 'AI-Driven Interactive Cinematic Experience' is a content production system combines live action with real-time graphics and multiple artificial intelligence solutions to create a conversational interactive entertainment. Our target is to make interactive content that viewers can interact with the fictional characters to affect the story plot, in addition to AI-based story engine, audience intention and language analysis, Real-time visualization of virtual set space and virtual characters according to the prompts of the story engine.Viewer response collection unit that receives voice and text responses generates interactive scenarios of virtual content that affects branching story plots in real-time for a natural story interaction while making the viewers fully engaged as if they are talking to a movie. The act of 'Viewers change the plot of the story by communicating with the main character in the drama', suggests new type of media entertainment format.},
booktitle = {Proceedings of the SIGGRAPH Asia 2022 Real-Time Live!},
articleno = {11},
numpages = {1},
location = {Daegu, Republic of Korea},
series = {SA '22}
}

@inproceedings{10.1145/3686397.3686400,
author = {Tran, Tuan and Nguyen, Dong and Nguyen, Ngoc-Anh},
title = {Unveiling Valuable Skills and Credentials in the Information Technology Job Market through Social Media Mining},
year = {2024},
isbn = {9798400717345},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686397.3686400},
doi = {10.1145/3686397.3686400},
abstract = {In the dynamic realm of information technology, aligning educational curricula with market demands is crucial for producing employable graduates and supporting industry growth. This research addresses the significant lag in the integration of rapidly evolving technologies, such as artificial intelligence and machine learning, into academic programs and identifies discrepancies in skill requirements across various sectors, including cybersecurity. Utilizing a novel approach, we developed a software tool to extract data from extensive job postings on a leading job advertisement platform, www.highered.com. Our analysis provides a detailed examination of the IT job market, highlighting geographical distribution of opportunities, required technical skills, experience levels, and necessary credentials across different sectors. The findings reveal skill mismatches and offer critical insights for educators to update course offerings and tailor educational programs to better meet industry needs. This paper also serves as a guide for policymakers in resource allocation and infrastructure development, aiming to enhance graduate employability and address sector-specific demands effectively. The study not only highlights the importance of timely curriculum updates but also demonstrates the benefits of leveraging real-time labor market data to inform educational strategies.},
booktitle = {Proceedings of the 2024 8th International Conference on Information System and Data Mining},
pages = {15–20},
numpages = {6},
keywords = {Social media mining, job credentials, preferred skills},
location = {
},
series = {ICISDM '24}
}

@inproceedings{10.1145/3604321.3604329,
author = {Gregory, Daniel and Monteiro, Diego},
title = {Is this the real life? Investigating the credibility of synthesized faces and voices created by amateurs using artificial intelligence tools.},
year = {2023},
isbn = {9798400708459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604321.3604329},
doi = {10.1145/3604321.3604329},
abstract = {The widespread availability and accessibility of artificial intelligence (AI) tools have enabled experts to create content that fools many and needs deep scrutiny to be discernible from reality; nevertheless, it is unclear whether presented with the same tools amateurs can also create synthesized faces and voices with similar ease. The possibility of creating this kind of content can be life-changing for smaller movie makers. Thus it is important to understand how, can amateurs be supported and guided into creating similar media and how believable are their results. This paper aims to propose a framework that can be used by amateurs to create completely artificial content and investigate the credibility of synthesized faces and voices created by amateurs using AI tools. Specifically, we explore whether an entirely AI-generated piece of media, encompassing both visual and audio components, can be convincingly created by non-experts. To achieve this, we conducted a series of experiments in which participants were asked to evaluate the credibility of synthesized media produced by amateurs. We analyzed the responses and evaluated the extent to which the synthesized media could pass as authentic to the participants. Our findings suggest that, while AI-generated media created by amateurs may appear visually convincing, the audio component is still lacking in terms of naturalness and authenticity. However, we also found that participants’ perceptions of credibility were influenced by their prior knowledge of AI-generated media and their familiarity with the source material. Our findings also suggest that while AI-generated media has the potential to be highly convincing, current AI tools and techniques are still far from achieving perfect emulation of human behavior and speech, when done by amateurs without artistic interference.},
booktitle = {Proceedings of the 2023 ACM International Conference on Interactive Media Experiences Workshops},
pages = {118–122},
numpages = {5},
keywords = {Automatic Generation, Deep Fake, Movies, User Study},
location = {Nantes, France},
series = {IMXw '23}
}

@inproceedings{10.1145/3433996.3434018,
author = {Liang, Fang},
title = {AI-Powered Digital Media Platform and Its Applications},
year = {2020},
isbn = {9781450388641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433996.3434018},
doi = {10.1145/3433996.3434018},
abstract = {Previously, the main method to create digital media such as animation was by humans. With the development of artificial intelligence technology, the work of making media content from raw materials is gradually replaced by computers. In this paper, we conceive a digital media production and interaction platform supported by artificial intelligence, based on some existing artificial intelligence and virtual reality display technologies. We also discuss the feasibility of some possible application scenarios for this platform.Based on our literature review and feasibility analysis, we conclude that current technology development is capable of supporting the AI-powered digital media platform that we propose in this paper.},
booktitle = {Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare},
pages = {121–126},
numpages = {6},
keywords = {Virtual Reality, Biometric Authentication, Artificial Intelligence},
location = {Taiyuan, China},
series = {CAIH2020}
}

@inproceedings{10.1145/3690624.3709401,
author = {Jin, Zhipeng and Tao, Wen and Li, Yafei and Yang, Yi and Han, Cong and Li, Shuanglong and Liu, Lin},
title = {Large Vison-Language Foundation Model in Baidu AIGC Image Advertising},
year = {2025},
isbn = {9798400712456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3690624.3709401},
doi = {10.1145/3690624.3709401},
abstract = {Recent advances in generative artificial intelligence have revolutionized information retrieval and content generation, opening up new opportunities for the e-commerce industry. Alignment learning between small models and parallel corpora cannot meet current needs. The success of ChatGPT demonstrates that large models need to first establish a fundamental understanding, and then utilize high-quality corpora for generation. Having a large model foundation is indispensable. In this paper, we establish a fundamental 10B multimodal model foundation for multimodal generation tasks and propose a scene-based alignment learning approach called conditional sample supervised fine-tuning for downstream generation tasks. Meanwhile, diffusion models are known to be vulnerable to outliers in training data. To address this, we utilize an alternative diffusion loss function that preserves the high quality of generated data like the original squared L2 loss while being robust to outliers.In practical test sets, the multimodal foundation fully demonstrates its alignment and comprehension abilities for graphic and textual content. Additionally, conditional fine-tuning and the design of the loss function significantly enhance the quality of generated content. The quality rate of images has increased by 34.3 percentage points, and prompt control has improved by 19.8 percentage points. The application of our framework in Baidu Search Ads has led to significant revenue growth. For instance, ads with generated image creatives have achieved a 29% higher click-through rate (CTR), resulting in a daily consumption of 3 million yuan.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1},
pages = {2303–2312},
numpages = {10},
keywords = {advertisement image creatives, cross-modal retrieval, multimodal sponsored search, text-to-image generation},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@inproceedings{10.1145/3615522.3615554,
author = {Wang, Bingyuan and Zhu, Pinxi and Li, Hao and Yip, David Kei-Man and Wang, Zeyu},
title = {Simonstown: An AI-facilitated Interactive Story of Love, Life, and Pandemic},
year = {2023},
isbn = {9798400707513},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615522.3615554},
doi = {10.1145/3615522.3615554},
abstract = {We present an interactive story named Simonstown that demonstrates the love and life of ordinary people in the fictional setting of a fatal pandemic. Technically, the artwork integrates different Artificial Intelligence (AI) technologies in the whole production pipeline, including concept formation, creation, and presentation stages; artistically, this interactive film explores the relationship between human and environment in the contemporary context, especially infused with advanced technologies in daily life. The project serves as a demonstration and case study of AI-facilitated interactive storytelling, including better control with AI and how they integrate with live image projects, as well as using the stand-alone camera for real-time synchronization. Our results highlight the significant contribution of AI in visualizing intricate story branching, translation, and adaptation, presenting AI visualization as a distinct, specialized, and well-suited tool for interactive filmmaking.},
booktitle = {Proceedings of the 16th International Symposium on Visual Information Communication and Interaction},
articleno = {32},
numpages = {7},
keywords = {film production, Interactive story, AI art},
location = {Guangzhou, China},
series = {VINCI '23}
}

@inproceedings{10.1145/3589335.3648315,
author = {Yang, Hao and Yuan, Jianxin and Yang, Shuai and Xu, Linhe and Yuan, Shuo and Zeng, Yifan},
title = {A New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3648315},
doi = {10.1145/3589335.3648315},
abstract = {In online advertising scenario, sellers often create multiple creatives to provide comprehensive demonstrations, making it essential to present the most appealing design to maximize the Click-Through Rate (CTR). However, sellers generally struggle to consider users' preferences for creative design, leading to the relatively lower aesthetics and quantities compared to Artificial Intelligence (AI)-based approaches. Traditional AI-based approaches still face the same problem of not considering user information while having limited aesthetic knowledge from designers. In fact that fusing the user information, the generated creatives can be more attractive because different users may have different preferences. To optimize the results, the generated creatives in traditional methods are then ranked by another module named creative ranking model. The ranking model can predict the CTR score for each creative considering user features. However, the two above stages (generating creatives and ranking creatives) are regarded as two different tasks and are optimized separately. Specifically, generating creatives in the first stage without considering the target of improving CTR task may generate several creatives with poor quality, leading to dilute online impressions and directly making bad effectiveness on online results.In this paper, we proposed a new automated C reative G eneration pipeline for Click-Through Rate (CG4CTR).1 The code is at with the goal of improving CTR during the creative generation stage. In this pipeline, a new creative is automatically generated and selected by stable diffusion method with the LoRA model and two novel models: prompt model and reward model. Our contributions have four parts: 1) The inpainting mode in stable diffusion method is firstly applied to creative image generation task in online advertising scene. A self-cyclic generation pipeline is proposed to ensure the convergence of training. 2) Prompt model is designed to generate individualized creative images for different user groups, which can further improve the diversity and quality of the generated creatives. 3) Reward model comprehensively considers the multi-modal features of image and text to improve the effectiveness of creative ranking task, and it is also critical in self-cyclic generation pipeline. 4) The significant benefits obtained in online and offline experiments verify the significance of our proposed method.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {180–189},
numpages = {10},
keywords = {creative generation, prompt and reward models, stable diffusion},
location = {Singapore, Singapore},
series = {WWW '24}
}

@article{10.1145/3588764,
author = {Liu, Chang and Yu, Han},
title = {AI-Empowered Persuasive Video Generation: A Survey},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3588764},
doi = {10.1145/3588764},
abstract = {Promotional videos are rapidly becoming a popular medium for persuading people to change their behaviours in many settings (e.g., online shopping, social enterprise initiatives). Today, such videos are often produced by professionals, which is a time-, labour- and cost-intensive undertaking. In order to produce such contents to support large applications (e.g., e-commerce), the field of artificial intelligence (AI)-empowered persuasive video generation (AIPVG) has gained traction in recent years. This field is interdisciplinary in nature, which makes it challenging for new researchers to grasp. Currently, there is no comprehensive survey of AIPVG available. In this paper, we bridge this gap by reviewing key AI techniques that can be utilized to automatically generate persuasive videos. We offer a first-of-its-kind taxonomy which divides AIPVG into three major steps: (1) visual material understanding, which extracts information from the visual materials (VMs) relevant to the target of promotion; (2) visual storyline generation, which shortlists and arranges high-quality VMs into a sequence in order to compose a storyline with persuasive power; and (3) post-production, which involves background music generation and still image animation to enhance viewing experience. We also introduce the evaluation metrics and datasets commonly adopted in the field of AIPVG. We analyze the advantages and disadvantages of the existing works belonging to the above-mentioned steps, and discuss interesting potential future research directions.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {285},
numpages = {31},
keywords = {storyline generation, video generation, Artificial intelligence}
}

@inproceedings{10.1145/3664476.3664510,
author = {May, Richard and Kr\"{u}ger, Jacob and Leich, Thomas},
title = {SoK: How Artificial-Intelligence Incidents Can Jeopardize Safety and Security},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664476.3664510},
doi = {10.1145/3664476.3664510},
abstract = {In the past years, a growing number of highly-automated systems has build on Artificial-Intelligence (AI) capabilities, for example, self-driving vehicles or predictive health-state diagnoses. As for any software system, there is a risk that misbehavior occurs (e.g., system failure due to bugs) or that malicious actors aim to misuse the system (e.g., generating attack scripts), which can lead to safety and security incidents. While software safety and security incidents have been studied in the past, we are not aware of research focusing on the specifics of AI incidents. With this paper, we aim to shed light on this gap through a case survey of 240 incidents that we elicited from four datasets comprising safety and security incidents involving AI from 2014 to 2023. Using manual data analyses and automated topic modeling, we derived relevant topics as well as the major issues and contexts in which the incidents occurred. We find that the topic of AI incidents is, not surprisingly, becoming more and more relevant, particularly in the contexts of autonomous driving and process-automation robotics. Regarding security and its intersection with safety, most incidents connect to generative AI (i.e., large-language models, deep fakes) and computer-vision systems (i.e., facial recognition). This emphasizes the importance of security to also ensure safety in the context of AI systems, with our results further revealing a high number of serious consequences (system compromise, human injuries) and major violations of confidentiality, integrity, availability, as well as authorization. We hope to support practitioners and researchers in understanding major safety and security issues to support the development of more secure, safe, and trustworthy AI systems.},
booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security},
articleno = {44},
numpages = {12},
keywords = {artificial intelligence, machine learning, safety, safety-critical systems, security, vulnerabilities},
location = {Vienna, Austria},
series = {ARES '24}
}

@inproceedings{10.1145/3678719.3685691,
author = {Garc\'{\i}a, Boni and Leotta, Maurizio and Ricca, Filippo and Whitehead, Jim},
title = {Use of ChatGPT as an Assistant in the End-to-End Test Script Generation for Android Apps},
year = {2024},
isbn = {9798400711091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678719.3685691},
doi = {10.1145/3678719.3685691},
abstract = {Automated testing is crucial in software development to ensure that applications perform as intended. However, generating automated End-to-End (E2E) tests can be time-consuming and challenging, especially for junior developers. This study investigates the use of ChatGPT, a popular Generative Artificial Intelligence (GenAI) model, as an assistant in developing automated E2E test scripts for Android apps. We present an empirical study that compares the effort required to create E2E test scripts and the resulting reliability of these tests using two treatments: manually and assisted by ChatGPT. We used Gherkin, a domain-specific language that allows non-technical practitioners to define test scenarios using a human-readable syntax. Our findings indicate that using ChatGPT significantly reduces the time required to develop automated test scripts without compromising the reliability of the scripts. Statistical analysis shows a notable reduction in development time for the ChatGPT-assisted group compared to the manual group, with a large effect size. While the reliability of the tests did not show a significant difference between the two groups, the results suggest practical benefits in terms of efficiency.},
booktitle = {Proceedings of the 15th ACM International Workshop on Automating Test Case Design, Selection and Evaluation},
pages = {5–11},
numpages = {7},
keywords = {Android, E2E Automated Testing, Empirical Study, GenAI},
location = {Vienna, Austria},
series = {A-TEST 2024}
}

@inproceedings{10.1145/3450615.3464538,
author = {Gagliano, Pietro and Blustein, Casey and Oppenheim, David},
title = {Agence, A Dynamic Film about (and with) Artificial Intelligence},
year = {2021},
isbn = {9781450383684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450615.3464538},
doi = {10.1145/3450615.3464538},
abstract = {Agence is a short “dynamic film” that uses AI to power a real-time story. It was co-produced by Transitional Forms and the National Film Board of Canada (NFB). It is available on VR, PC and mobile, but for the purposes of this paper, we will be talking about the VR version, since it most closely matches the director’s vision. The film is directed by Pietro Gagliano whose work on interactive stories has spanned many years and technologies. A few years ago he started Transitional Forms to combine real-time storytelling with artificial intelligence. The intention behind that process is twofold: First, we believe that entertainment will soon be driven by AI. And secondly, artificial intelligence is poised to be humanity’s greatest tool, and stories might be the best way to make sense of it. To this end, we believe that Agence is an innovative production with bold strides in immersion, interactivity and technology. The approaches taken in this film are novel and unique in their propositions, and may open the door to many new projects that may build upon them.},
booktitle = {ACM SIGGRAPH 2021 Immersive Pavilion},
articleno = {17},
numpages = {2},
keywords = {reinforcement learning, neural networks, immersive, dynamic film, VR},
location = {Virtual Event, USA},
series = {SIGGRAPH '21}
}

@inproceedings{10.1145/3510450.3517319,
author = {Ishtiaq, Faisal and Zinger, Kerry},
title = {VideoAI: realizing the potential of media analytics},
year = {2022},
isbn = {9781450392228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510450.3517319},
doi = {10.1145/3510450.3517319},
abstract = {In this work, we describe a unique system and solution, VideoAI, that allows Comcast to rapidly create Artificial Intelligence (AI) and Machine Learning (ML) based media analytics experiences. The unprecedented growth of Artificial Intelligence, Machine Learning and Deep Learning (DL) has enabled a new level of insight into video content from detecting faces objects to understanding sentiment and emotions. However, many of the solutions today are highly customized to the desired solution and do not easily scale when applied to other solutions.At Comcast we realized that to fully harness the power of latest in Media Analytics techniques - from content creation to consumption - requires a bottoms-up approach. Rather than highly customized solutions, we have developed a growing suite of AI ML capabilities that can be rapidly reused, reproposed, and deployed into a unified solution we call VideoAI. This can be used to quickly develop new Media Analytics powered solution across our media ecosystem.VideoAI analyzes live/linear streams and files to generate temporal metadata that describes moment by moment what is happening in the video. It scans video, audio, and closed captions, along with other signals to generate time indexes enriched tags describing what is happening in the video. Using this approach, we are able to create solutions that change the way we do Dynamic Ad Insertion (DAI), enable binge watching, automatic chaptering, metadata enhancement, and much more. Leveraging the power of this framework, the machine learning algorithms have processed millions of hours of content that have improved the accuracy and robustness of the detectors and ensemble approaches.In the proposed presentation we will describe in greater detail the algorithmic and systematic approach used to harness the power of AI/ML/DL in a reconfigurable and reusable way. The technical benefit of this approach results in minimized training cycles, more efficient use of compute cycles, and algorithmic improvements across use cases. We will also describe a set of applications enabled by VideoAI that includes linear and VOD Segmentation, and metadata enrichment for advertisement.},
booktitle = {Proceedings of the 1st Mile-High Video Conference},
pages = {126},
numpages = {1},
keywords = {video streaming, artificial intelligence},
location = {Denver, Colorado},
series = {MHV '22}
}

@inproceedings{10.1145/3674829.3675081,
author = {Ismail, Yusuf and Till, Sarina},
title = {Developing and Deploying AI and IoT-enabled hydroponic grow tents with subsistence farmers in South Africa},
year = {2024},
isbn = {9798400710483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674829.3675081},
doi = {10.1145/3674829.3675081},
abstract = {South Africa is experiencing extreme weather conditions, often resulting in failed crops. The country has many subsistence farmers who rely on small-scale farming for food security and livelihoods. This paper presents [project name omitted to ensure anonymity], an Artificial Intelligence (AI), and Internet of Things (IoT) hydroponic grow tent implementing the Nutrient Film Technique (NFT) approach for leafy green plant production. The system uses a Random Forest Classifier (RFC) and various sensors for the real-time, accurate management of nutrient levels, pH, light, temperature, and humidity to provide an optimal environment for crop growth, in most cases, regardless of the weather patterns outside of the tent. The system further includes a mobile application that allows farmers to interject and manage all the elements in the tent. This paper reports on the design, development, and first deployment of [project name omitted to ensure anonymity] with rural subsistence farmers in South Africa. We found that South African farmers are keen to explore new agricultural technologies and were able to co-farm using [project name omitted to ensure anonymity] to grow crops that are no longer successfully cultivated due to the erratic weather conditions in South Africa.},
booktitle = {Proceedings of the 7th ACM SIGCAS/SIGCHI Conference on Computing and Sustainable Societies},
pages = {252–255},
numpages = {4},
keywords = {Android Application Integration, Environmental Control Automation, Hydroponic Agriculture, Machine Learning in Agriculture, Nutrient Film Technique (NFT), Random Forest Classifier, Real-Time Data Monitoring, Sustainable Farming Practices},
location = {New Delhi, India},
series = {COMPASS '24}
}

@inproceedings{10.1145/3607720.3608451,
author = {Loukili, Soumaya and Fennan, Abdelhadi and Elaachak, Lotfi},
title = {Applications of Text Generation in Digital Marketing: a review},
year = {2023},
isbn = {9798400700194},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607720.3608451},
doi = {10.1145/3607720.3608451},
abstract = {Billions of dollars are spent each year on different aspects of digital marketing. Automating some of its processes has become crucial to keep up with the volume of offers put online every day. With that being said, textual advertising content plays a major role in attracting converting customers; that is why integrating Text Generation could have high impact benefits on the entire marketing activity. Marketing text could be produced quicker and with a higher quality that is modeled after content that has already demonstrated good results. This paper focuses on Text Generation under Artificial Intelligence, and contains a summarization of the different types of Text Generation, such as Text-to-Text that covers Text Summarization, Dialogue Systems and Machine Translation, Visual-to-Text, and Data-to-Text. This paper also discusses the different techniques used in Text Generation, including Word2vec, GloVe and fastText, RNNs, CNNS, VAEs, and GANs. In addition, it encloses a review on research that's been previously conducted on the matter in Digital Marketing, along with the results, and a comparison of it all.},
booktitle = {Proceedings of the 6th International Conference on Networking, Intelligent Systems &amp; Security},
articleno = {69},
numpages = {8},
keywords = {Text Generation, NLP, Digital Marketing},
location = {Larache, Morocco},
series = {NISS '23}
}

@inproceedings{10.1145/3503181.3503190,
author = {Feng, Yiqiang and Qiu, Leiju and Sun, Baowen},
title = {Internet and Cognition-Based Decision Making: A Survey},
year = {2022},
isbn = {9781450395540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503181.3503190},
doi = {10.1145/3503181.3503190},
abstract = {Digital technologies represented by the Internet have influenced micro-economic agents' cognitive decisions. We review the effects of Internet technologies on the cognitions and decisions of microeconomic agents systematically. For individuals, the Internet has reshaped their cognition with the information shock, the "Google effect," and social networks. Individual decisions depend on personal cognitions; hence we further clarify the impact of the Internet on individual behavior and decisions, such as fertility decisions, house purchasing decisions, and financial investment decisions. For enterprises, we consider Internet Big Data-assisted decision-making. Enterprises can make relevant business decisions such as targeted advertising, product improvement, and price discrimination by mining big data and using artificial intelligence algorithms. We find that internet technology can directly impact human intelligence (cognition) and influence individual decisions. Internet technology such as artificial intelligence algorithms can also assist individual decisions.},
booktitle = {5th International Conference on Crowd Science and Engineering},
pages = {50–54},
numpages = {5},
keywords = {Internet Technology, Decision Making, Crowd Intelligence Network, Cognition, Big Data},
location = {Jinan, China},
series = {ICCSE '21}
}

@inproceedings{10.1145/3423958.3423975,
author = {Isaac, Carlos G},
title = {Web-based 3D HSV color data gathering and visualization for applied cinemetrics: A case study in Hayao Miyazaki’s animated motion pictures},
year = {2020},
isbn = {9781450388856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423958.3423975},
doi = {10.1145/3423958.3423975},
abstract = {The tasks of judgment and criticism in cinema have been based predominantly on interpretive methods to establish their disciplinary mechanisms. Nowadays, the web’s possibilities for supporting artificial intelligence, mathematics, and statistical computing techniques applied to the communication and humanities phenomena, such as the visual narrative arts, allows the development of new tools that aim at exploring the use of quantitative instrumentation for film analysis. Color grouping via K-Means Clustering and JavaScript based graphic visualization and data management, are procedures that web-based services can exploit for a quantitative film analysis. The methods discussed in this paper are examined under Miyazaki’s animated films and shows a measurable relationship between the director’s creative process and the perceptual dominant colors in the films. Furthermore, our proposed solution applies the combination and the exploration of such technologies for a web-based tool that helps researchers, scholars, filmmakers, and designers to gather color data from video media and to visualize them according to a 3D HSV/HSB graphical model.},
booktitle = {Proceedings of the 3rd International Conference on Web Studies},
pages = {22–26},
numpages = {5},
keywords = {web interface, image color quantification, film data},
location = {Hammamet, Tunisia},
series = {WS.3 2020}
}

@article{10.1145/3519306,
author = {Souibgui, Mohamed Ali and Bensalah, Asma and Chen, Jialuo and Forn\'{e}s, Alicia and Waldisp\"{u}hl, Michelle},
title = {A User Perspective on HTR Methods for the Automatic Transcription of Rare Scripts: The Case of Codex Runicus},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3519306},
doi = {10.1145/3519306},
abstract = {Recent breakthroughs in Artificial Intelligence, Deep Learning, and Document Image Analysis and Recognition have significantly eased the creation of digital libraries and the transcription of historical documents. However, for documents in rare scripts with few labelled training data available, current Handwritten Text Recognition (HTR) systems are too constraining. Moreover, research on HTR often focuses on technical aspects only, and rarely puts emphasis on implementing software tools for scholars in Humanities. In this article, we describe, compare, and analyse different transcription methods for rare scripts. We evaluate their performance in a real-use case of a medieval manuscript written in the runic script (Codex Runicus) and discuss advantages and disadvantages of each method from the user perspective. From this exhaustive analysis and comparison with a fully manual transcription, we raise conclusions and provide recommendations to scholars interested in using automatic transcription tools.},
journal = {J. Comput. Cult. Herit.},
month = mar,
articleno = {72},
numpages = {18},
keywords = {codex runicus, human evaluation, handwritten text recognition tools, Historical manuscripts}
}

@inproceedings{10.1145/3386567.3388559,
author = {Polyak, Emil},
title = {Homeostasis},
year = {2020},
isbn = {9781450379526},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386567.3388559},
doi = {10.1145/3386567.3388559},
abstract = {Emerging technologies are impacting human interactions more than ever, while public perception of automation, artificial intelligence (AI) and human enhancement remains critical. In creative fields, innovative ideas are often triggered by intersecting dissimilar phenomena filtered through aesthetic considerations. One can argue that introducing AI in artistic practice destroys spontaneity, intuition and serendipity; consequently, the outcome is deliberate and premeditated. However, art is open to interpretations and through designing of unorthodox digital artifacts, interfaces and experiences in contrast with mainstream processes, we can challenge existing beliefs and provoke new ideas to reach a better understanding of how technology affects our culture. The project Homeostasis is a speculative interactive visual experience. It connects a distinctive interface design with generative art and meaningful data to communicate a significant topic. Shapes, colors, form and timing are manipulated based on a set of design principles, while the pattern of a vapor cloud from an ultrasonic vaporizer is analyzed and processed in a machine learning model in real time. The variations of the vapor pattern enable infinite possibilities between the natural boundaries and provide exciting data through computer vision that then drives the spatial and temporal attributes of the animation. The design is biologically inspired, and it attempts to create an illusion of cellular life-forms in deep waters.},
booktitle = {ACM SIGGRAPH 2020 Art Gallery},
pages = {466},
numpages = {1},
location = {Virtual Event, USA},
series = {SIGGRAPH '20}
}

@inproceedings{10.1145/3524501.3527599,
author = {Ahmed, Md. Arshad and Chatterjee, Madhura and Dadure, Pankaj and Pakray, Partha},
title = {The role of biased data in computerized gender discrimination},
year = {2022},
isbn = {9781450392945},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524501.3527599},
doi = {10.1145/3524501.3527599},
abstract = {Gender bias is prevalent in all walks of life from schools to colleges, corporate as well as government offices. This has led to the under-representation of the female gender in many professions. Most of the Artificial Intelligence-Natural Language Processing (AI-NLP) models learning from these underrepresented real world datasets amplify the bias in many cases, resulting in traditional biases being reinforced. In this paper, we have discussed how gender bias became ingrained in our society and how it results in the underrepresentation of the female gender in several fields such as education, healthcare, STEM, film industry, food industry, and sports. We shed some light on how traditional gender bias is reflected in AI-NLP systems such as automated resume screening, machine translation, text generation, etc. Future prospects of these AI-NLP applications need to include possible solutions to these existing biased AI-NLP applications, such as debiasing the word embeddings and having guidelines for more ethical and transparent standards.},
booktitle = {Proceedings of the Third Workshop on Gender Equality, Diversity, and Inclusion in Software Engineering},
pages = {6–11},
numpages = {6},
keywords = {natural language processing, gender bias, debiasing, artificial intelligence},
location = {Pittsburgh, Pennsylvania},
series = {GE@ICSE '22}
}

@article{10.1145/3555757,
author = {Alvarado, Oscar and Vanden Abeele, Vero and Geerts, David and Verbert, Katrien},
title = {Towards Tangible Algorithms: Exploring the Experiences of Tangible Interactions with Movie Recommender Algorithms},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555757},
doi = {10.1145/3555757},
abstract = {Artificial Intelligence (AI) supports many of our everyday activities and decisions. However, personalized algorithmic recommendations often produce adverse experiences due to a lack of awareness, control, or transparency. While research has directed solutions on graphical user interfaces (GUIs), there are no explorations of Tangible User Interfaces (TUIs) to improve the experience with such systems, despite the valid existing academic arguments in favor of this exploration. Therefore, centering on transparency and control, we analyzed how 18 users of movie recommender systems perceived four different TUIs using individual co-design sessions and post-interview questionnaires. Through thematic analysis, we identified seven design considerations while designing TUIs to interact with algorithmic movie recommender systems: (1) Distinctions between TUIs and GUIs; (2) TUIs replacing predominant interfaces; (3) Preference for single-device TUIs; (4) The relevance of granular control for TUIs; (5) Apparent transparency limitations of TUIs; (6) TUIs and algorithmic social computing; and (7) Overview of specific design choices, including advantages and disadvantages of soft, hard, rounded, cubic, and humanoid interfaces. These findings inspired Recffy: the first functional TUI designed to enhance awareness and control in personalized movie recommendations. Based on this study, we propose the concept of Tangible Algorithms: TUIs dedicated to enhancing the interaction of algorithmic systems and their profiling processes or decisions in a specific context. Furthermore, we describe the relevance of tangible algorithms and design guidelines to promote them in diverse AI contexts. Finally, we invite the HCI and CSCW community to continue exploring tangible algorithms to address the interaction with algorithmic systems, including the collaborative and social computing dynamics they can promote in diverse AI contexts.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {337},
numpages = {30},
keywords = {tangible interfaces, tangible algorithms, social computing, recommender systems, algorithmic experience}
}

@inproceedings{10.1145/3503161.3546970,
author = {Pitas, Ioannis and Mademlis, Ioannis},
title = {Autonomous UAV Cinematography},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3546970},
doi = {10.1145/3503161.3546970},
abstract = {The use of camera-equipped Unmanned Aerial Vehicles (UAVs, or "drones") for professional media production is already an exciting commercial reality. Currently available consumer UAVs for cinematography applications are equipped with high-end cameras and a degree of cognitive autonomy relying on artificial intelligence (AI). Current research promises to further exploit the potential of autonomous functionalities in the immediate future, resulting in portable flying robotic cameras with advanced intelligence concerning autonomous landing, subject detection/tracking, cinematic shot execution, 3D localization and environmental mapping, as well as autonomous obstacle avoidance combined with on-line motion re-planning. Disciplines driving this progress are computer vision, machine/deep learning and aerial robotics. This Tutorial emphasizes the definition and formalization of UAV cinematography aesthetic components, as well as the use of robotic planning/control methods for autonomously capturing them on footage, without the need for manual tele-operation. Additionally, it focuses on state-of-the-art Imitation Learning and Deep Reinforcement Learning approaches for automated UAV/camera control, path planning and cinematography planning, in the general context of "flying &amp; filming".},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {7374–7376},
numpages = {3},
keywords = {uav cinematography, media production, intelligent shooting, autonomous drones, artificial intelligence},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3680528.3687688,
author = {Li, Yunxin and Shi, Haoyuan and Hu, Baotian and Wang, Longyue and Zhu, Jiashun and Xu, Jinyi and Zhao, Zhen and Zhang, Min},
title = {Anim-Director: A Large Multimodal Model Powered Agent for Controllable Animation Video Generation},
year = {2024},
isbn = {9798400711312},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3680528.3687688},
doi = {10.1145/3680528.3687688},
abstract = {Traditional animation generation methods depend on training generative models with human-labelled data, entailing a sophisticated multi-stage pipeline that demands substantial human effort and incurs high training costs. Due to limited prompting plans, these methods typically produce brief, information-poor, and context-incoherent animations. To overcome these limitations and automate the animation process, we pioneer the introduction of large multimodal models (LMMs) as the core processor to build an autonomous animation-making agent, named Anim-Director. This agent mainly harnesses the advanced understanding and reasoning capabilities of LMMs and generative AI tools to create animated videos from concise narratives or simple instructions. Specifically, it operates in three main stages: Firstly, the Anim-Director generates a coherent storyline from user inputs, followed by a detailed director’s script that encompasses settings of character profiles and interior/exterior descriptions, and context-coherent scene descriptions that include appearing characters, interiors or exteriors, and scene events. Secondly, we employ LMMs with the image generation tool to produce visual images of settings and scenes. These images are designed to maintain visual consistency across different scenes using a visual-language prompting method that combines scene descriptions and images of the appearing character and setting. Thirdly, scene images serve as the foundation for producing animated videos, with LMMs generating prompts to guide this process. The whole process is notably autonomous without manual intervention, as the LMMs interact seamlessly with generative tools to generate prompts, evaluate visual quality, and select the best one to optimize the final output. To assess the effectiveness of our framework, we collect varied short narratives and incorporate various Image/video evaluation metrics including visual consistency and video quality. The experimental results and case studies demonstrate the Anim-Director’s versatility and significant potential to streamline animation creation.},
booktitle = {SIGGRAPH Asia 2024 Conference Papers},
articleno = {34},
numpages = {11},
keywords = {Animation Generation, Large Multimodal Models, Autonomous Agent, Video and Image Generation},
location = {Tokyo, Japan},
series = {SA '24}
}

@inproceedings{10.1145/3706598.3713201,
author = {Biswas, Shreyan and Erlei, Alexander and Gadiraju, Ujwal},
title = {Mind the Gap! Choice Independence in Using Multilingual LLMs for Persuasive Co-Writing Tasks in Different Languages},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713201},
doi = {10.1145/3706598.3713201},
abstract = {Recent advances in generative AI have precipitated a proliferation of novel writing assistants. These systems typically rely on multilingual large language models (LLMs), providing globalized workers the ability to revise or create diverse forms of content in different languages. However, there is substantial evidence indicating that the performance of multilingual LLMs varies between languages. Users who employ writing assistance for multiple languages are therefore susceptible to disparate output quality. Importantly, recent research has shown that people tend to generalize algorithmic errors across independent tasks, violating the behavioral axiom of choice independence. In this paper, we analyze whether user utilization of novel writing assistants in a charity advertisement writing task is affected by the AI’s performance in a second language. Furthermore, we quantify the extent to which these patterns translate into the persuasiveness of generated charity advertisements, as well as the role of peoples’ beliefs about LLM utilization for their donation choices. Our results provide evidence that writers who engage with an LLM-based writing assistant violate choice independence, as prior exposure to a Spanish LLM reduces subsequent utilization of an English LLM. While these patterns do not affect the aggregate persuasiveness of the generated advertisements, people’s beliefs about the source of an advertisement (human versus AI) do. In particular, Spanish-speaking female participants who believed that they read an AI-generated advertisement strongly adjusted their donation behavior downwards. Furthermore, people are generally not able to adequately differentiate between human-generated and LLM-generated ads. Our work has important implications on the design, development, integration, and adoption of multilingual LLMs as assistive agents—particularly in writing tasks.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {937},
numpages = {20},
keywords = {Human-AI interaction, Choice Independence, Multilingual LLMs, User Reliance},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3464385.3464763,
author = {Schmidt, Albrecht},
title = {The End of Serendipity: Will Artificial Intelligence Remove Chance and Choice in Everyday Life?},
year = {2021},
isbn = {9781450389778},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3464385.3464763},
doi = {10.1145/3464385.3464763},
abstract = {Software defines our everyday experiences! Communication in families as well as in the workplace is largely software mediated. The choices we make, from the news articles we read to the movies we watch and the people we date, are to a large extent software supported. Personalized news portals, navigation systems, social media platforms, shopping portals, music streaming services, and dating apps are only some examples of systems that affect what we experience, think, and do. Improvements in human computer interaction have led to a wide universal adoption of these systems in many areas. Artificial intelligence, learning about the users and their preferences, and striving for simplification in interaction, reduces the need to make active decisions and thereby removes chance and choice. Will this lead to highly optimized systems – that apparently work great for the user, but at the same time end the element of randomness and serendipity in our lives? Simplified content creating, recommender systems and augmented reality are drivers for this. Can interactive human centered artificial intelligence help to keep the user in control or is this just an illusion?},
booktitle = {Proceedings of the 14th Biannual Conference of the Italian SIGCHI Chapter},
articleno = {1},
numpages = {4},
keywords = {Serendipity, Interactive Human Centered Artificial Intelligence, Human Computer Interaction, Choice., Artificial Intelligence},
location = {Bolzano, Italy},
series = {CHItaly '21}
}

@inproceedings{10.1145/3501709.3544275,
author = {Castro, Francisco Enrique Vicente and DesPortes, Kayla and Payne, William and Bergner, Yoav and McDermott, Kathleen},
title = {AI + Dance: Co-Designing Culturally Sustaining Curricular Resources for AI and Ethics Education Through Artistic Computing},
year = {2022},
isbn = {9781450391955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501709.3544275},
doi = {10.1145/3501709.3544275},
abstract = {Artificial intelligence (AI) and machine learning (ML) systems are ubiquitous across many fields ranging from medicine (e.g., tumor detection), to natural language processing (e.g., digital home assistants, auto-translate tools), and personalization (e.g., social media recommendations). The rise of these systems has also seen a rise in the ethical challenges resulting from how some of these systems are designed and implemented. Most problematic is their propagation and amplification of problems such as racism and sexism [4, 6], among others. Further, these challenges exist within a computing discipline that is already burdened by exclusive, marginalizing cultures and practices that lead to low participation by women and Black, Indigenous, and People of Color (BIPOC) [3]. Our project, AI + Dance, attends to these dimensions of inequity within AI/ML education through developing our understanding of how we can equip learners to recognize and rectify issues of AI and ML systems within an inclusive and culturally sustaining experience. We explore this in collaboration with STEM From Dance1 (SFD), a non-profit organization that supports girls of color in creative production with dance, CS, and STEM. In our prior work with SFD, we developed danceON, an open-access creative coding environment that enables learners to create code to engage authentically with dance and body motion [5]. With danceON, learners can code animations that can bind and respond to body positions and be statically and dynamically positioned in space. The system provides two ways to explore AI and ML. First, it integrates a pose detection machine learning model (PoseNet [8]) that consists of body points (i.e., Left Ear, Nose, etc.) and confidence scores for these key points, which are all accessible within the code. Second, danceON enables learners to import a model trained in Google’s Teachable Machine [2] to access the probabilistic classifier to manipulate and trigger animations. While we have explored the use of danceON to enable learners to build culturally relevant artistic artifacts [5], we have yet to explore how we can teach AI and ML concepts and ethics through the use of the system. We investigate two research questions through the co-design of culturally sustaining AI/ML resources with teachers: RQ1) How can we authentically build on learners’ identities, cultural knowledge, and practices with dance as they build, explore, and critique ML and AI models and systems? RQ2) How can we leverage learners’ creative, embodied experiences with ML and AI to facilitate reflection and critique of CS and AI/ML within their communities and society more broadly? We focus on co-design as a method that facilitates sustainability through centering teachers’ values, ownership, and authentic contexts in the realization, implementation, and evaluation of learning designs [7]. Through co-designing curricular resources, we will develop a set of modules aligned with the AI4K12 guidelines [1] that builds our understanding of creating culturally sustaining educational resources to teach about AI and ethical design in ways that leverage the culturally situated, collaborative, and embodied nature of dance.},
booktitle = {Proceedings of the 2022 ACM Conference on International Computing Education Research - Volume 2},
pages = {26–27},
numpages = {2},
keywords = {danceON, Machine learning, Dance, Creative computing, Artistic computing, Artificial intelligence, AI ethics},
location = {Lugano and Virtual Event, Switzerland},
series = {ICER '22}
}

@inproceedings{10.1145/3706598.3714148,
author = {Wang, Wen-Fan and Lu, Chien-Ting and Ponsa i Campany\`{a}, Nil and Chen, Bing-Yu and Chen, Mike Y.},
title = {AIdeation: Designing a Human-AI Collaborative Ideation System for Concept Designers},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714148},
doi = {10.1145/3706598.3714148},
abstract = {Concept designers in the entertainment industry create highly detailed, often imaginary environments for movies, games, and TV shows. Their early ideation phase requires intensive research, brainstorming, visual exploration, and combination of various design elements to form cohesive designs. However, existing AI tools focus on image generation from user specifications, lacking support for the unique needs and complexity of concept designers’ workflows. Through a formative study with 12 professional designers, we captured their workflows and identified key requirements for AI-assisted ideation tools. Leveraging these insights, we developed AIdeation to support early ideation by brainstorming design concepts with flexible searching and recombination of reference images. A user study with 16 professional designers showed that AIdeation significantly enhanced creativity, ideation efficiency, and satisfaction (all p&lt;.01) compared to current tools and workflows. A field study with 4 studios for 1 week provided insights into AIdeation’s benefits and limitations in real-world projects. After the completion of the field study, two studios, covering films, television, and games, have continued to use AIdeation in their commercial projects to date, further validating AIdeation’s improvement in ideation quality and efficiency.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {21},
numpages = {28},
keywords = {Generative AI, Human-Centered AI, Concept Design, Creativity Support Tool, Visual Exploration},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3605764.3623903,
author = {Coqueret, Beno\^{\i}t and Carbone, Mathieu and Sentieys, Olivier and Zaid, Gabriel},
title = {When Side-Channel Attacks Break the Black-Box Property of Embedded Artificial Intelligence},
year = {2023},
isbn = {9798400702600},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605764.3623903},
doi = {10.1145/3605764.3623903},
abstract = {Artificial intelligence, and specifically deep neural networks (DNNs), has rapidly emerged in the past decade as the standard for several tasks from specific advertising to object detection. The performance offered has led DNN algorithms to become a part of critical embedded systems, requiring both efficiency and reliability. In particular, DNNs are subject to malicious examples designed in a way to fool the network while being undetectable to the human observer: the adversarial examples. While previous studies propose frameworks to implement such attacks in black box settings, those often rely on the hypothesis that the attacker has access to the logits of the neural network, breaking the assumption of the traditional black box. In this paper, we investigate a real black box scenario where the attacker has no access to the logits. In particular, we propose an architecture-agnostic attack which solve this constraint by extracting the logits. Our method combines hardware and software attacks, by performing a side-channel attack that exploits electromagnetic leakages to extract the logits for a given input, allowing an attacker to estimate the gradients and produce state-of-the-art adversarial examples to fool the targeted neural network. Through this example of adversarial attack, we demonstrate the effectiveness of logits extraction using side-channel as a first step for more general attack frameworks requiring either the logits or the confidence scores.},
booktitle = {Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security},
pages = {127–138},
numpages = {12},
keywords = {adversarial examples, black box attack, deep learning, embedded systems, side-channel attack},
location = {Copenhagen, Denmark},
series = {AISec '23}
}

@inproceedings{10.1145/3388761.3407250,
author = {Hanlin, Heath and Holmes, Shaina and Sayed, Raqi and Teich, Sunny},
title = {Bringing VFX to the Table},
year = {2020},
isbn = {9781450379694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388761.3407250},
doi = {10.1145/3388761.3407250},
abstract = {This panel brings together academics and practitioners to discuss the structural problems facing visual effects within the film and television industry today. This discussion is organized around four sub-topics: 1) labor solutions and what unionization would mean, 2) perceptions of VFX artists and their contributions within the industry by filmmakers and artists themselves, as well as the audience, 3) issues of inclusivity along gender and racial lines, and their effects on work/life balances, and 4) VFX as equal creative partners rather than isolated, temporary, highly-skilled specialists.This panel is a continuation of work begun by the faculty and practitioner organizers, Heath Hanlin (Syracuse University, VPA) and Shaina Holmes (Syracuse University, Newhouse) on studying structural challenges and obstacles to entry for students hoping to enter the VFX industry, in particular women, and artists of color. An earlier iteration of this discussion was held at the 2020 Sundance Film Festival. The hope is that the technical focus and high attendance by industry practitioners at Siggraph, adds practitioner perspectives and educator feedback to the sub-topics listed above.Simply put, today's film and television industry rely heavily on visual effects, both to advance the story or as a means to hide imperfections to keep the audience engaged without technical distractions. Traditional conceptions of VFX artistry as a production step that comes after primary cinematography is simply outdated. Unfortunately, the economic and work-flow structure of the industry accepts this new reality while at the same time attempting to compensate, credit, and organize VFX artists within the outdated framework. Often this aspect of the conversation is moored in debates surrounding the potentiality of AI to either positively impact the industry (no more underpaid routinized tasks for artists) or negatively (crafts persons and artists can be replaced by algorithms). Rather, a more fruitful discussion around this point should be centered on labor issues and globalization (sub-topic #1) of VFX within the industry to reflect the VFX Artist's increased importance in most contemporary productions.This new economic bargaining power cannot be established if we first don't engage with, educate, and change the discourse surrounding how other creatives in the industry view VFX artists (sub-topic #2) and their critical roles. This panel discusses the stereotype of today's VFX workers as “easily trainable, easily replaceable, ‘button-pushers’, who can be substituted with intuitive AI” and how we can shift the narrative artisans and craft persons working on the level of a writer, director, or editor, using their talents to equally construct the narrative vision of the piece.The film and television industry relies so heavily on VFX artists for their visions, that similar to the video game industry, long hours are required from those artists who toil away behind a monitor and keyboard. At the same time, similar to the film and television industry as a whole, it is overly represented by majority white male workers who also disproportionately are not responsible for childcare and its accompanying social and emotional labor. As such, VFX professionals, who are women, find themselves working within an unregulated part of the industry working over 100 hours per week on a project to project basis, not knowing what city, let alone country, their next job will be in, and if they will have time to dedicate to supporting, or even starting, a family (see sub-topic #3). This panel discussion hopes to expand these voices and their perception on solutions to inclusivity within the industry.Issues of equality are revealed in a broader sense when we realize VFX artists need to be recognized more as equal creative partners (see sub-topic #2) due to their increased reliance upon and importance in the contemporary digital workflow of content creation and visual storytelling. More than merely surveying other creators’ perceptions of VFX artists’ contributions, there needs to be new contractual precedents and workflow procedures established in order to accomplish structural changes. For example, VFX artists need to be at the beginning of production choices, not merely as pre-visualization workers, but as co-creators and organized division heads who can ready their departments for the production troubleshooting invariably ahead. VFX need not be an afterthought – not secondary “fixers” who through a combination of creativity and technical know-how, solve problems quickly with little to no additional compensation. Rather, they need to be co-leaders beginning right from pre-production until post as co-creatives.This discussion amongst the panelists is expected to explore the topics above, but also expand and evolve into new conversations and connections to the artist, technology, industry, globalization, COVID-19 impact, and education spaces. Towards the end of the dialogue, we will invite the audience to participate by asking questions to the panel for open discussion and sharing their own knowledge and experiences.},
booktitle = {ACM SIGGRAPH 2020 Panels},
articleno = {2},
numpages = {2},
location = {Virtual Event, USA},
series = {SIGGRAPH '20}
}

@inproceedings{10.1145/3706598.3714155,
author = {Liu, Vivian and Kazi, Rubaiat Habib and Wei, Li-Yi and Fisher, Matthew and Langlois, Timothy and Walker, Seth and Chilton, Lydia},
title = {LogoMotion: Visually-Grounded Code Synthesis for Creating and Editing Animation},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714155},
doi = {10.1145/3706598.3714155},
abstract = {Creating animation takes time, effort, and technical expertise. To help novices with animation, we present LogoMotion, an AI code generation approach that helps users create semantically meaningful animation for logos. LogoMotion automatically generates animation code with a method called visually-grounded code synthesis and program repair. This method performs visual analysis, instantiates a design concept, and conducts visual checking to generate animation code. LogoMotion provides novices with code-connected AI editing widgets that help them edit the motion, grouping, and timing of their animation. In a comparison study on 276 animations, LogoMotion was found to produce more content-aware animation than an industry-leading tool. In a user evaluation (n=16) comparing against a prompt-only baseline, these code-connected widgets helped users edit animations with control, iteration, and creative expression.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {157},
numpages = {16},
keywords = {animation, large language models, motion design, program synthesis, code generation, GPT, logos},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3706598.3713342,
author = {Halperin, Brett A. and Ru\'{\i}z, Diana Flores and Rosner, Daniela K.},
title = {Underground AI? Critical Approaches to Generative Cinema through Amateur Filmmaking},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713342},
doi = {10.1145/3706598.3713342},
abstract = {Amateurism (e.g., hobbyist and do-it-yourself making) has long helped human-computer interaction (HCI) scholars map alternatives to status quo technology developments, cultures, and practices. Following the 2023 Hollywood film worker strikes, many scholars, artists, and activists alike have called for alternative approaches to AI that reclaim the apparatus for co-creative and resistant means. Towards this end, we conduct an 11-week diary study with 20 amateur filmmakers of 15 AI-infused films, investigating the emerging space of generative cinema as a critical technical practice. Our close reading of the films and filmmakers’ reflections on their processes reveal four critical approaches to negotiating AI use in filmmaking: minimization, maximization, compartmentalization, and revitalization. We discuss how these approaches suggest the potential for underground filmmaking cultures to form around AI with critical amateurs reclaiming social control over the creative possibilities.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1141},
numpages = {18},
keywords = {AI, AI Art, Amateurism, Bias, Creativity, Critical Humanistic Inquiry, Cinema, Cinematography, Critical Technical Practice, Film, Filmmaking, Generative AI, Non-Use, Storytelling, Underground Film, Video, Visual Storytelling},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3493700.3493769,
author = {Paliwal, Dr Mukta and Rao, Dattaraj and Tarcar, Amogh},
title = {Responsible AI Tutorial},
year = {2022},
isbn = {9781450385824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493700.3493769},
doi = {10.1145/3493700.3493769},
abstract = {There is rapid technical progress and widespread adoption of Artificial Intelligence (AI) based products and workflows influencing many aspects of human and business activities like banking, healthcare, advertising and many more.Although accuracy of AI models is undoubtedly the most important factor considered while deploying AI based products, there is urgent need to understand how AI can be designed to operate responsibly.Responsible AI is a framework that each software developing organization needs to adapt to build customer trust in the transparency, accountability, fairness, and security of deployed AI solutions. At the same time a key aspect to making AI responsible is to have a development pipeline that can promote reproducibility of results and manage the lineage of data and ML models.This tutorial will throw light on these aspects of Responsible AI with a working example demonstrating the concept. The intent of the tutorial will be to equip the audience with enough knowledge of the concepts along with code to gain appreciation for the importance of building Responsible AI.},
booktitle = {Proceedings of the 5th Joint International Conference on Data Science &amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)},
pages = {339–341},
numpages = {3},
keywords = {Responsible AI, Machine Learning Operations, Explainable AI},
location = {Bangalore, India},
series = {CODS-COMAD '22}
}

@article{10.1145/3632173,
author = {Manimaran, A. and Syed, Mohammad Haider and Kumar, M. Siva and Selvanayaki, S. and Sunitha, Gurram and Manna, Asmita},
title = {Enhancing Asian Indigenous Language Processing through Deep Learning-based Handwriting Recognition and Optimization Techniques},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {8},
issn = {2375-4699},
url = {https://doi.org/10.1145/3632173},
doi = {10.1145/3632173},
abstract = {Asian&nbsp;indigenous language or autochthonous language is a language which is native to a region and spoken by indigenous people in Asia. This language is a linguistically different community created in the region. Recently, researchers in handwriting detection studies comparing with indigenous languages have attained important internet amongst the research community. A new development of artificial intelligence (AI), natural language processing (NLP), cognitive analytics, and computational linguistics (CL) find it helpful in the analysis of regional low-resource languages. It can be obvious in the obtainability of effectual machine detection methods and open access handwritten databases. Tamil is the most ancient Indian language that is mostly exploited in the Southern part of India, Sri Lanka, and Malaysia. Tamil handwritten Character Recognition (HCR) is a critical procedure in optical character detection. Therefore, this study designs a Henry Gas Solubility Optimization with Deep Learning-based Handwriting Recognition Model (HGSODL-HRM) for Asian Indigenous Language Processing. The proposed HGSODL-HRM technique relies on computer vision and DL concepts for automated handwriting recognition in the Tamil language, which is one of the popular indigenous languages in Asia. To accomplish this, the HGSODL-HRM technique employs a capsule network (CapsNet) model for feature vector generation with the HGSO algorithm as a hyperparameter optimizer. For the recognition of handwritten characters, wavelet neural network (WNN) model is exploited. Finally, the WNN parameters can be optimally chosen by sail fish optimizer (SFO) algorithm. To demonstrate the promising results of the HGSODL-HRM system, an extensive range of simulations can be implemented. The simulation outcomes stated the betterment of the HGSODL-HRM system compared to recent DL models.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {119},
numpages = {20},
keywords = {Natural language processing, Cognitive Analytics, Asian&nbsp;indigenous language, Handwriting recognition, Deep Learning}
}

@inproceedings{10.1145/3531146.3533080,
author = {Engelmann, Severin and Ullstein, Chiara and Papakyriakopoulos, Orestis and Grossklags, Jens},
title = {What People Think AI Should Infer From Faces},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533080},
doi = {10.1145/3531146.3533080},
abstract = {Faces play an indispensable role in human social life. At present, computer vision artificial intelligence (AI) captures and interprets human faces for a variety of digital applications and services. The ambiguity of facial information has recently led to a debate among scholars in different fields about the types of inferences AI should make about people based on their facial looks. AI research often justifies facial AI inference-making by referring to how people form impressions in first-encounter scenarios. Critics raise concerns about bias and discrimination and warn that facial analysis AI resembles an automated version of physiognomy. What has been missing from this debate, however, is an understanding of how “non-experts” in AI ethically evaluate facial AI inference-making. In a two-scenario vignette study with 24 treatment groups, we show that non-experts (N = 3745) reject facial AI inferences such as trustworthiness and likability from portrait images in a low-stake advertising and a high-stake hiring context. In contrast, non-experts agree with facial AI inferences such as skin color or gender in the advertising but not the hiring decision context. For each AI inference, we ask non-experts to justify their evaluation in a written response. Analyzing 29,760 written justifications, we find that non-experts are either “evidentialists” or “pragmatists”: they assess the ethical status of a facial AI inference based on whether they think faces warrant sufficient or insufficient evidence for an inference (evidentialist justification) or whether making the inference results in beneficial or detrimental outcomes (pragmatist justification). Non-experts’ justifications underscore the normative complexity behind facial AI inference-making. AI inferences with insufficient evidence can be rationalized by considerations of relevance while irrelevant inferences can be justified by reference to sufficient evidence. We argue that participatory approaches contribute valuable insights for the development of ethical AI in an increasingly visual data culture.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {128–141},
numpages = {14},
keywords = {artificial intelligence, computer vision, human faces, participatory AI ethics},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{10.1145/3706598.3713840,
author = {Halperin, Brett A. and Lukin, Stephanie M.},
title = {From Camera-Eye to AI: Exploring the Interplay of Cinematography and Computational Visual Storytelling},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713840},
doi = {10.1145/3706598.3713840},
abstract = {While much prior work on computational visual storytelling analyzes image content, it largely overlooks formal elements. This raises the question: how might particular cinematographic techniques shape a system’s interpretation and narration of imagery? To investigate this question, we generate 60 responses from a Vision Language Model using a multi-faceted prompt paired with different still frames from Man with a Movie Camera (1929), a silent documentary film renowned for its innovative cinematography. We present three themes that highlight roles of cinematography in computational visual storytelling: (1) how AI discerns drama and power from camera shots and angles that portray social reality; (2) how AI (mis)interprets lighting and focus techniques that compose ambiguous reality; and (3) how AI navigates visual effects that render surreality. In turn, we look toward cinematic controls to reimagine users as directors of visual storytelling systems and discuss how expressive AI can support speculating about the past.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {327},
numpages = {18},
keywords = {AI, Automatic Story Generation, Cinema, Cinematography, Computational Storytelling, Film, Generative AI, Vision Language Models, Narrative, Narrative Intelligence, Narrative System, Natural Language Generation, Visual Storytelling, Storytelling},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3437802.3437809,
author = {Zhang, Manyu},
title = {Application of Artificial Intelligence Interactive storytelling in Animated},
year = {2021},
isbn = {9781450388054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437802.3437809},
doi = {10.1145/3437802.3437809},
abstract = {The research significance of this article is to realize the scene storytelling of animation based on the visualization of UnrealTM game engine. In the scene preview, the characters’ moving speed and path are simulated and controlled to realize the real-time interaction of the virtual character to know the effect of the whole story development in advance. We illustrate a method for the core role of artificial actors in interactive storytelling and how to participate in the creation of dynamic storylines. User autonomous behavior the artificial characters and the interactive storytelling of artificial intelligence of the virtual actors allow interaction between the virtual characters and the characters from users. Autonomous virtual actors generate dynamic plots based on the dynamic interaction between the characters and according to the storytelling plot to increase user entertainment.},
booktitle = {Proceedings of the 2020 1st International Conference on Control, Robotics and Intelligent System},
pages = {37–41},
numpages = {5},
keywords = {virtual engine, storytelling scenario design, autonomous characters, artificial intelligence, Digital interactive storytelling},
location = {Xiamen, China},
series = {CCRIS '20}
}

@inproceedings{10.1145/3534678.3542617,
author = {Kenthapadi, Krishnaram and Lakkaraju, Himabindu and Natarajan, Pradeep and Sameki, Mehrnoosh},
title = {Model Monitoring in Practice: Lessons Learned and Open Challenges},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3542617},
doi = {10.1145/3534678.3542617},
abstract = {Artificial Intelligence (AI) is increasingly playing an integral role in determining our day-to-day experiences. Increasingly, the applications of AI are no longer limited to search and recommendation systems, such as web search and movie and product recommendations, but AI is also being used in decisions and processes that are critical for individuals, businesses, and society. With AI based solutions in high-stakes domains such as hiring, lending, criminal justice, healthcare, and education, the resulting personal and professional implications of AI are far-reaching. Consequently, it becomes critical to ensure that these models are making accurate predictions, are robust to shifts in the data, are not relying on spurious features, and are not unduly discriminating against minority groups. To this end, several approaches spanning various areas such as explainability, fairness, and robustness have been proposed in recent literature, and many papers and tutorials on these topics have been presented in recent computer science conferences. However, there is relatively less attention on the need for monitoring machine learning (ML) models once they are deployed and the associated research challenges.In this tutorial, we first motivate the need for ML model monitoring[14], as part of a broader AI model governance[9] and responsible AI framework, from societal, legal, customer/end-user, and model developer perspectives, and provide a roadmap for thinking about model monitoring in practice. We then present findings and insights on model monitoring desiderata based on interviews with various ML practitioners spanning domains such as financial services, healthcare, hiring, online retail, computational advertising, and conversational assistants[15]. We then describe the technical considerations and challenges associated with realizing the above desiderata in practice. We provide an overview of techniques/tools for model monitoring (e.g., see [1, 1, 2, 5, 6, 8, 10-13, 18-21]. Then, we focus on the real-world application of model monitoring methods and tools [3, 4, 7, 11, 13, 16, 17], present practical challenges/guidelines for using such techniques effectively, and lessons learned from deploying model monitoring tools for several web-scale AI/ML applications. We present case studies across different companies, spanning application domains such as financial services, healthcare, hiring, conversational assistants, online retail, computational advertising, search and recommendation systems, and fraud detection. We hope that our tutorial will inform both researchers and practitioners, stimulate further research on model monitoring, and pave the way for building more reliable ML models and monitoring tools in the future.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4800–4801},
numpages = {2},
keywords = {case studies from industry, ethics in ai, model monitoring and model risk management, responsible ai},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3706599.3720264,
author = {Jung, Yongnam and Hua, Peixin and Bao, Jiaqi (Agnes) and Sundar, S. Shyam},
title = {AI-Generated or AI-Modified? User Reactions to Labeling AI Use in Social Media Posts},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3720264},
doi = {10.1145/3706599.3720264},
abstract = {Although social media platforms are beginning to implement policies for labeling AI usage in posts, we do not know how this labeling affects user perceptions and which types of labeling source and language would be most effective. What does AI-generated content mean to users? What are users’ perceptions of the content, content creator, and platform that has AI-labeled content? Do the effects differ depending on whether the label is attributed to the user or the platform? A focus group study (N=14) revealed that users appreciate how AI helps to create better content. However, their perceptions of AI-labeled content are shaped by their mental models of how social media algorithms work. Some participants viewed AI-labeled content as more trendy, while others saw it as direct advertisements. Some believed the content to be automatically fake, but their reactions varied depending on the type of content or account. Regarding labeling source, users preferred self-labeling over platform labeling. We discuss theoretical and practical implications for the design of social media interfaces for disclosing AI usage.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {57},
numpages = {7},
keywords = {AI Labeling, UGC, User-Generated Content},
location = {
},
series = {CHI EA '25}
}

@inbook{10.5555/3716662.3716713,
author = {Hice-Fromille, Theresa and Papazoglakis, Sarah},
title = {Afrofuturist Values for the Metaverse (Extended Abstract)},
year = {2025},
publisher = {AAAI Press},
abstract = {This article proposes diverse speculative texts as generative sources of inspiration for the metaverse. Fiction writing and storytelling provide well-known sources of inspiration for people building the metaverse. However, the familiar cadre of texts routinely credited with inspiring the creation of virtual reality lacks diversity. By contrast, Black and Indigenous People of Color (BIPOC) creatives have cultivated outlets of expression through speculative works of art, literature, film, television, and games, that serve as meditations on the experiences of marginalized communities and the kind of world these communities are collaborating to build. The fictional future is where community members, cultural workers, academics, policymakers and technologists can, quite literally, get on the same page. By analyzing critical, even dystopian, stories of how future technologies can negatively impact society, these disparate groups can collaborate to co-produce more ethical and sustainable technologies. The authors assembled an archive of thirty-nine speculative fiction texts including twenty Afrofuturist texts, two Latinx-futurist texts, seven Asian-futurist texts, six Indigenous-futurist, and four mainstream science fiction TV shows. We used inductive textual and visual analysis to, first, identify technologies present in each text and the conditions and contexts within which each technology was employed, and then to map these technologies to recurring themes across the archive. In these methods of analysis, the researcher reads/views texts and identifies themes first individually and then within and against other texts. Our analysis revealed three overarching themes that serve as recommendations for the creation and maintenance of a diverse and inclusive metaverse: Collective Power, Inclusive Engagement, and Cultural Specificity. We outline each recommendation through analysis of three Afrofuturist texts -Washington Black (Edugyan 2019), Traveling While Black (Williams 2019), and Black Panther (Coogler 2018) - and specify the undercurrents of collectivity and co-production that bind them together. BIPOC creatives utilize speculative fiction as a technology whose function is to communicate critical visions of the future entangled with complex histories and diverse contemporary perspectives. Engaging BIPOC communities to co-create the metaverse offers an opportunity to imagine how these immersive spaces can potentially bridge the digital divide, lead to more accessible internet access and virtual tools, and ensure equitable representation and participation in the 3D internet. However, before engaging diverse communities, reading diverse speculative fiction can prepare industry professionals for the process of co-production. In the discussion section, the authors outline a suggested process for critically reading diverse speculative fiction. With these recommendations, creators will examine the many moving components of a diverse assortment of speculative fiction texts and learn what histories are embedded, concerns are shared, and communal goals are articulated before building technologies of their own. By engaging these creative works together, disparate groups, each holding a unique stake in our technological future, can collaboratively guide the public in imaging alternative frameworks for a more equitable future. We conclude by considering the power of cultural analysis to complement processes that are often structured as purely legal imperatives. These collaborative cultural interventions may help to shape democratic processes governing the future of AI.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {609},
numpages = {1}
}

@inproceedings{10.1145/3630106.3658995,
author = {Stark, Luke},
title = {Animation and Artificial Intelligence},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3658995},
doi = {10.1145/3630106.3658995},
abstract = {Animation as genre is broadly used across many forms of digital media. In this paper, I argue ChatGPT and similar chatbots powered by Large Language Models (LLMs) can be best understood as animated characters. More than just cartooning, puppetry, or CGI, animation is a paradigm involving the projection of qualities perceived as human such as power, agency, will, and personality outside of the self and onto objects in the environment. Characteristics of animation—including reliance on stereotypes, obfuscation of human labor, and manipulation of an audience's emotions—can help us both analyze and respond appropriately to interactive AI technologies and the hyperbolic claims of their promoters.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1663–1671},
numpages = {9},
keywords = {AI ethics, ChatGPT, Turing test, animation, artificial intelligence, emotion, grammar of action, human-computer interaction (HCI), human-machine interaction, inference, interactive AI, labor},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@inproceedings{10.1145/3597512.3597528,
author = {Piskopani, Anna Maria and Chamberlain, Alan and Ten Holter, Carolyn},
title = {Responsible AI and the Arts: The Ethical and Legal Implications of AI in the Arts and Creative Industries},
year = {2023},
isbn = {9798400707346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597512.3597528},
doi = {10.1145/3597512.3597528},
abstract = {This position piece starts to examine the ways in which AI-based autonomous technologies have begun to influence a range of human activities in the arts and creative industries.The rise of AI-generated art could potentially transform the act of creation and impact our understandings of creativity – from painting, writing, and music composition, to video animation. At the same time, there is increasing debate about the social, ethical, and legal implications of using these tools (eg copyright, biased data sets, devaluing artistic processes). Responsible Innovation (RI) could have a crucial role to play in understanding and responding to the complexity of debates. We will explore and unpack how artists, AI developers and associated audiences/consumers of art have started to approach some of these issues. We will use these ideas as a starting point to explicate and further develop discourses surrounding the challenges associated with these technologies in the context of the creative industries. Finally, we will investigate how and if these challenges might be addressed.},
booktitle = {Proceedings of the First International Symposium on Trustworthy Autonomous Systems},
articleno = {48},
numpages = {5},
keywords = {neural networks, datasets, creative industries},
location = {Edinburgh, United Kingdom},
series = {TAS '23}
}

@inproceedings{10.1145/3641520.3665309,
author = {He, Kevin and Lapham, Annette and Li, Zenan},
title = {Enhancing Narratives with SayMotion's text-to-3D animation and LLMs},
year = {2024},
isbn = {9798400705267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641520.3665309},
doi = {10.1145/3641520.3665309},
abstract = {SayMotion, a generative AI text-to-3D animation platform, utilizes deep generative learning and advanced physics simulation to transform text descriptions into realistic 3D human motions for applications in gaming, extended reality (XR), film production, education and interactive media. SayMotion addresses challenges due to the complexities of animation creation by employing a Large Language Model (LLM) fine-tuned to human motion with further AI-based animation editing components including spatial-temporal Inpainting via a proprietary Large Motion Model (LMM). SayMotion is a pioneer in the animation market by offering a comprehensive set of AI generation and AI editing functions for creating 3D animations efficiently and intuitively. With an LMM at its core, SayMotion aims to democratize 3D animations for everyone through language and generative motion.},
booktitle = {ACM SIGGRAPH 2024 Real-Time Live!},
articleno = {2},
numpages = {2},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@inproceedings{10.1145/3314111.3322874,
author = {Yang, Zhizhuo and Bailey, Reynold},
title = {Towards a data-driven framework for realistic self-organized virtual humans: coordinated head and eye movements},
year = {2019},
isbn = {9781450367097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314111.3322874},
doi = {10.1145/3314111.3322874},
abstract = {Driven by significant investments from the gaming, film, advertising, and customer service industries among others, efforts across many different fields are converging to create realistic representations of humans that look like (computer graphics), sound like (natural language generation), move like (motion capture), and reason like (artificial intelligence) real humans. The ultimate goal of this work is to push the boundaries even further by exploring the development of realistic self-organized virtual humans that are capable of demonstrating coordinated behaviors across different modalities. Eye movements, for example, may be accompanied by changes in facial expression, head orientation, posture, gait properties, or speech. Traditionally however, these modalities are captured and modeled separately and this disconnect contributes to the well-known uncanny valley phenomenon. We focus initially on facial modalities, in particular, coordinated eye and head movements (and eventually facial expressions), but our proposed data-driven framework will be able to accommodate other modalities as well. transfer [Laine et al. 2017]. Despite these advances, the resulting renderings or animations are often still distinguishable from a real human, sometimes in unsettling ways - the so called uncanny valley phenomenon [Mori et al. 2012]. We argue that the traditional approach of capturing and modeling various human modalities separately contributes this effect. In this work, we focus on capturing, transferring, and generating realistic coordinated facial modalities (eye movements, head movements, and eventually facial expressions). We envision a flexible framework that can be extended to accommodate other modalities as well.},
booktitle = {Proceedings of the 11th ACM Symposium on Eye Tracking Research &amp; Applications},
articleno = {55},
numpages = {3},
keywords = {machine learning, eye-head coordination, data driven animation},
location = {Denver, Colorado},
series = {ETRA '19}
}

@article{10.5555/3381613.3381625,
author = {Gestwicki, Paul},
title = {Unreal engine 4 for computer scientists},
year = {2019},
issue_date = {October 2019},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {35},
number = {5},
issn = {1937-4771},
abstract = {This tutorial will provide an introduction to Unreal Engine 4 (UE4), focusing on properties that would be of interest to Computer Science students and faculty. UE4 is a popular game engine, the fourth generation of an engine technology whose development started in the mid 1990s. As a game engine, it provides common functionality that is used in entertainment software, including a 3D graphics rendering system, real-time audio, input management, networking infrastructure, and artificial intelligence support. It supports development for numerous platforms, including desktop and mobile operating systems, game consoles, and virtual reality devices. The engine is also becoming more popular for use in film as well as architecture.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {109–110},
numpages = {2}
}

@proceedings{10.1145/3401956,
title = {MOCO '20: Proceedings of the 7th International Conference on Movement and Computing},
year = {2020},
isbn = {9781450375054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This volume presents the Proceedings of the 7th International Conference on Movement and Computing (MOCO'20). MOCO is an interdisciplinary conference that explores the use of computational technology to support and understand human movement practice (e.g. computational analysis) as well as movement as a means of interacting with computers (e.g. movement interfaces). This requires not only a wide range of computational tasks, including modeling, representation, segmentation, recognition, classification, and generation of movement information, but also an interdisciplinary understanding of movement that ranges from biomechanics to embodied cognition and the phenomenology of bodily experience. We therefore invite submissions from a wide range of disciplines including (but not limited to): Human-Computer Interaction, Psychology, Dance, Artificial Intelligence, Neuroscience, Sports Science, Machine Learning, Cognitive Science, Visual Arts, Robotics, Philosophy, Anthropology, Music, Affective Computing, Games, Healthcare and Animation.},
location = {Jersey City/Virtual, NJ, USA}
}

@inproceedings{10.1145/3678698.3687197,
author = {Yip, David and song, Junrong},
title = {Star Pilgrim: Blending UE Cinematics with AIGC for an Elevated Fantasy and Surrealism in Visuals},
year = {2024},
isbn = {9798400709678},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678698.3687197},
doi = {10.1145/3678698.3687197},
abstract = {Star Pilgrim is an experimental short video artwork displayed on a 16:9 screen. It combines Unreal Engine filmmaking with AIGC techniques. The project used Midjourney to generate a customized dataset of stylized images, which was then used as LoRA training data for style transfer applied to the video footage. Additionally, the creative process involved human-AI collaborative generation of original music and poetry. By blending cinematic visuals with AI-powered generative content, this work aims to explore the unique artistic potential of human and machine creativity. Through its experimental approach, Star Pilgrim seeks to push the boundaries of conventional filmmaking, offering a dreamlike and hyper-real interpretation of the concept.},
booktitle = {Proceedings of the 17th International Symposium on Visual Information Communication and Interaction},
articleno = {42},
numpages = {3},
keywords = {AIGC, Unreal Engine Filmmaking, AI-Generated Content, Style Transfer, Human-AI Co-Creativity in Music, Experimental Video},
location = {
},
series = {VINCI '24}
}

@inproceedings{10.1145/3630590.3630601,
author = {Lahesoo, Laura and Do, Uyen and Carnier, Rodrigo and Fukuda, Kensuke},
title = {SIURU: A Framework for Machine Learning Based Anomaly Detection in IoT Network Traffic},
year = {2023},
isbn = {9798400709395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630590.3630601},
doi = {10.1145/3630590.3630601},
abstract = {With the increasing adoption of Internet of Things (IoT), research into anomaly detection (AD) in IoT network traffic is gaining importance. Malicious disturbances (e.g. malware and cyber attacks) and operational issues (e.g. software/sensor failures and physical damage) can seriously disrupt network operations. These problems can be detected with traffic monitoring and responded to with mitigation systems. In recent years, machine learning (ML) models have been successfully applied for AD, with state-of-the-art solutions reaching detection rates of . However, the heterogeneity of devices and anomalies in IoT networks poses a challenge for existing solutions. High AD accuracy is usually limited to the types of anomalies present in the training data, falling off significantly in the face of new anomalies. Many IoT-AD solutions are additionally limited to certain types of devices or require additional hardware setup. To tackle these challenges, we present SIURU, a flexible framework for AD research and development capable of applying various feature extraction and ML algorithms. SIURU’s intended deployment environment is broker-based publish-subscribe IoT systems, posing no additional constraints on the IoT device capabilities or network setup. SIURU supports the creation of custom pipelines with data loading, preprocessing, and encoding components, followed by training and testing of ML-based AD models and reporting of prediction results. To validate our proposed framework, we implement the architecture and evaluate its runtime performance. Finally, we demonstrate the effectiveness of the proposed architecture by a case example that tests the performance of eXplainable Artificial Intelligence (XAI) algorithms with multiple ADs and datasets.},
booktitle = {Proceedings of the 18th Asian Internet Engineering Conference},
pages = {87–95},
numpages = {9},
keywords = {IoT network traffic, anomaly detection, machine learning},
location = {Hanoi, Vietnam},
series = {AINTEC '23}
}

@inproceedings{10.1145/3680532.3689579,
author = {Caldwell, Craig B},
title = {Breaking the Story Formula},
year = {2024},
isbn = {9798400711350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3680532.3689579},
doi = {10.1145/3680532.3689579},
abstract = {Audiences want stories that meet their expectations… just not in the way they expect it. That is the fine line a storyteller must walk, and it is one of the biggest challenges in media today. To achieve that goal, directors and producers rely on animators, VFX artists, and interactive designers. The Story has evolved over thousands of years, and the expected structures (formulas) have been codified in the Hero's Journey, Save the Cat, and Story Circle (Embryo).In this session, we will look at how filmmakers have been subverting story 'formulas" while using recognized story `forms' to break from expectations. Such approaches includes `forms' such as Time (Christopher Nolan), Setting (Cohen Brothers), and Endings (Michael Arndt). We will also explore breaking Genre structures (i.e., Comedy, Action, Hybrid Genres) and the impact of their Tropes (i.e., Anti-Hero as Hero/Villain). What impact do cultural differences have on story expectations (i.e., Eastern (Kishotenketsu) versus Western (MonoMyth) storytelling [Do we read images differently? Do all stories have conflict?]) When breaking from norms, is it a Story's Structure or its Storytelling (narrative) that provides the most significant latitude? Where do Unreliable Narrators, Subtext, Cause-and-Effect, Choices, Arcs, and AI… come into play?This session reveals an audience's expectations in storytelling, which, if you are going to break from expectations, it is advantageous to know ahead of time. This session is for those who want to understand how story norms have been broken in creating content for animated films, VFX, video games, and interactive media. This presentation contains many visuals to illustrate the concepts.},
booktitle = {SIGGRAPH Asia 2024 Courses},
articleno = {2},
numpages = {11},
location = {Tokyo, Japan},
series = {SA Courses '24}
}

@inproceedings{10.1145/3680530.3695443,
author = {Huang, Xuanyang and Zhong, Xiaoyun and Zhong, Zhihua and Papatheodorou, Theodoros and Yip, Kei-Man},
title = {Cinema Meowdiso: Films Co-Created by Human, Large Language Models and a Cat},
year = {2024},
isbn = {9798400711336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3680530.3695443},
doi = {10.1145/3680530.3695443},
abstract = {Cinema Meowdiso is a system that utilizes generative AI to transform daily footage captured by pets into short films across various genres. This project explores the potential of non-human entities as narrative agents and examines how AI can autonomously create stories with human guidance. An automated visual story generation system based on pets’ perspective is proposed, aiming to provide a novel fictional narrative experience that resonates with real-life footage.},
booktitle = {SIGGRAPH Asia 2024 Art Papers},
articleno = {5},
numpages = {7},
keywords = {AI Art, Generative AI, LLMs, Narrative, Cinema},
location = {
},
series = {SA '24}
}

@inbook{10.1145/3477322.3477323,
author = {Cassell, Justine},
title = {Foreword},
year = {2021},
isbn = {9781450387200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3477322.3477323},
abstract = {In preparation for writing this foreword, I looked through old emails (really old emails) dating back to early 1998, when we were planning the “First Workshop on Embodied Conversational Characters.” In and amongst detailed menu planning for the workshop (I haven’t changed a bit since those days) are emails floating the idea of publishing a book with the best papers from the workshop. We were already starting to see a shift in the literature, away from “lifelike computer characters” (Microsoft’s Clippy was presented at a workshop with that name) and “believable computer characters” (characters whose behavior was believable, but that did not do anything for people), and we wanted the book to reflect that shift.We particularly wanted to highlight the fact that embodied conversational characters did not only talk but also listened. They were capable of understanding as well as generating language and non-verbal behavior, and they did so in the service of humans—they were agents, like travel agents or real estate agents. To that end, I sent an email to the chapter authors with the following tidbits. I wrote:It’s amusing to read this today when we take for granted the agentive nature of our conversational systems. At this point, we assume that embodied conversation agents (ECAs) are designed primarily to accomplish work for people. We also take for granted that ECAs must both understand and talk. However, when the Embodied Conversational Agents book was conceived, both of those features were only newly possible. In turn, the title of the current volume highlights the most recent technological innovation, which is the ability of the systems not just to do work for humans but to interact socially with them in the process, in many cases using social interaction as a way to bootstrap task performance.It’s illuminating to look at two other debates that took place during this same period. The first concerns what kinds of data are used to create the most natural behaviors for an ECA. The second concerns whether it is ethical to build natural-acting ECAs.While there was beginning to be consensus in the late 1990s on the idea that conversational characters could do more than just look pretty, there were three schools of thought about the proper inspiration for the conversational behaviors of ECAs (as they were called). Some of the authors in the original volume worked with actors to understand what kinds of language and non-verbal behaviors were most evocative of normal human conversation. These researchers hewed to the belief that ECAs should behave in a somewhat exaggerated fashion, like actors on a stage, in order to seem natural to their human interlocutors. Other authors believed that, being native speakers of their own language, and acculturated to the customs of their own society, the simple intuitions of the researcher were sufficient to design human-like conversational behaviors. A third group believed that psychological and linguistic studies of human conversation were the only proper inspiration for the behaviors of ECAs. Today, while a few researchers still work with actors or rely on their own intuitions, the community of researchers in ECAs (and in today’s socially interactive agents) mostly rely on empirical psychological and linguistic studies of human behavior as their inspiration. Some of these researchers carry out their own studies, and some rely on extant literature, but in both cases they rely on normal everyday humans for inspiration rather than actors or computer scientists. The debate is interesting in the face of today’s focus on big data. In fact, the increasing reliance in the field of artificial intelligence (AI) on machine learning techniques to analyze human behavior has led to a parallel increase in ECA systems that rely on deep learning techniques applied to large corpora, often of naturally produced human conversational behavior, to generate appropriate verbal and non-verbal conversational behavior. In other words, AI has brought us closer to the human-inspired ECAs of the past by bringing a focus on corpora of natural behaviors. At the same time, however, it has taken us further away from those human-inspired ECAs of the past because the corpora are too large to be examined by the human eye. Another debate that evoked heated interchanges in the late 1990s, and that is useful to contemplate today, was whether we should even contemplate deploying ECAs as interfaces to computational systems in the first place. Many if not most of the authors in the 1998 volume believed that ECAs represented a more natural way of interacting with computational systems than a keyboard and a mouse. Their work was predicated on the assumption that interacting with a human-like agent was a more intuitive manner of accessing technical systems. To other computer scientists of the era, however, ECAs were downright evil. Perhaps most famously, human–computer interaction researcher Ben Schneiderman saved his strongest invectives for human-like agents and their designers. In 1995, he wroteToday, fears about whether robots will steal jobs, and whether machine learning will make it hard to tell who is human and who is an AI, have once again launched debates on whether human-like agents are a good or bad influence on society. These debates have led to a stronger focus on transparency in AI, a concern with bias in data, and a much-needed conversation on the ethics of where ECAs should and should not be used. These contemporary debates, however, and contra Shneiderman’s predictions, show that anthropomorphic agents have stood the test of time. The topic has inspired passion and dedication in a whole new generation of researchers. To that end, here 20 years later is a two-volume follow-up from our 1998 Embodied Conversational Agents book, with more than 25 chapters, showing the depth, breadth, innovation, creativity, and, yes, effectiveness, of human-inspired agents.Justine Cassell},
booktitle = {The Handbook on Socially Interactive Agents: 20 Years of Research on Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics Volume 1: Methods, Behavior, Cognition},
pages = {xvii–xx}
}

@article{10.5555/3580523.3580568,
author = {Shahid, Abdur R.},
title = {Facilitating Internet of Things (IoT) Experience in Computer and Information Systems Education},
year = {2022},
issue_date = {November 2022},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {38},
number = {3},
issn = {1937-4771},
abstract = {Over the past few years, with the advancement of sensing technologies, wireless communication, cloud computing, data analytics, machine learning, conversational artificial intelligence, and embedded systems, we have observed a dramatic increase in the research effort to turn physical objects into smart devices and enable them to talk to each other through the internet. The network formed by these different smart devices is the Internet of Things (IoT). We are starting to see its applications in various industries, including healthcare, home automation, automobile, manufacturing, retail, and education. It is a multi-billion-dollar industry projected to grow into the trillion-dollar range. However, there is a significant gap between industry demand and the supply of professionals from the universities. Therefore, it is highly expected that graduates from different Computing-related fields will fill the void. Motivated by the problem of introducing IoT education in an undergraduate Computer and Information Systems (CIS) program, we developed a proof-of-concept IoT curriculum. We incorporated it into an existing Advance Java programming language course. This approach has several benefits. First, it avoids creating an entirely new course on IoT. Second, it allows the students to be engaged in developing a solution for a real world-centric problem under physical constraints. Third, it helps us to attract a wider audience. Three groups of students created java programs to solve problems for raspberry pi-based intelligent cars. All projects followed the Bridge21 Pedagogy Model, emphasizing teamwork, learning by doing, and technology-mediated project work. In addition, the projects introduced several critical concepts to the students, including Linux systems, system setup, scripting, and working with multiple programming languages (Java and Python).},
journal = {J. Comput. Sci. Coll.},
month = nov,
pages = {207},
numpages = {1}
}

@inproceedings{10.1145/3589335.3641295,
author = {He, Fengxiang and Du, Mengnan and Filos-Ratsikas, Aris and Cheng, Lu and Song, Qingquan and Lin, Min and Vines, John},
title = {AI Driven Online Advertising: Market Design, Generative AI, and Ethics},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3641295},
doi = {10.1145/3589335.3641295},
abstract = {Online advertising contributes a considerable part of the tech sector's revenue, and has been remarkably influencing the public agenda. With evolving developments, AI is playing an increasingly significant role in online advertising. We propose to create a forum for researchers, developers, users, ventures, policymakers, and other stakeholders to exchange ideas, research, innovations, etc. with emphasis on (1) AI driven mechanism design for distributing advertisements, (2) generative AI for creating content in advertisements, such as the promotion images/videos, and (3) ethics issues, especially in political advertisements, such as user privacy, fairness, hating speech, misinformation, etc. Relevant but not mentioned areas are also much encouraged. We plan to organize a half-day workshop.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {1407–1409},
numpages = {3},
keywords = {algorithmic game theory, auction, e-commerce, economics, ethics, generative ai, mechanism design, online advertising},
location = {Singapore, Singapore},
series = {WWW '24}
}

@article{10.1145/3633287,
author = {Richards, Mike and Waugh, Kevin and Slaymaker, Mark and Petre, Marian and Woodthorpe, John and Gooch, Daniel},
title = {Bob or Bot: Exploring ChatGPT's Answers to University Computer Science Assessment},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
url = {https://doi.org/10.1145/3633287},
doi = {10.1145/3633287},
abstract = {Cheating has been a long-standing issue in university assessments. However, the release of ChatGPT and other free-to-use generative AI tools has provided a new and distinct method for cheating. Students can run many assessment questions through the tool and generate a superficially compelling answer, which may or may not be accurate.&nbsp;We ran a dual-anonymous “quality assurance” marking exercise across four end-of-module assessments across a distance university computer science (CS) curriculum. Each marker received five ChatGPT-generated scripts alongside 10 student scripts. A total of 90 scripts were marked; every ChatGPT-generated script for the undergraduate modules received at least a passing grade (&gt;40%), with all of the introductory module CS1 scripts receiving a distinction (&gt;85%). None of the ChatGPT-taught postgraduate scripts received a passing grade (&gt;50%). We also present the results of interviewing the markers and of running our sample scripts through a GPT-2 detector and the TurnItIn AI detector, which both identified every ChatGPT-generated script but differed in the number of false positives. As such, we contribute a baseline understanding of how the public release of generative AI is likely to significantly impact quality assurance processes. Our analysis demonstrates that in most cases, across a range of question formats, topics, and study levels, ChatGPT is at least capable of producing adequate answers for undergraduate assessment.},
journal = {ACM Trans. Comput. Educ.},
month = jan,
articleno = {5},
numpages = {32},
keywords = {ChatGPT, generative AI, cheating, quality assurance, university assessment’}
}

@inproceedings{10.1145/3706598.3714119,
author = {Siddiqui, Momin N and Pea, Roy D and Subramonyam, Hari},
title = {Script&amp;Shift: A Layered Interface Paradigm for Integrating Content Development and Rhetorical Strategy with LLM Writing Assistants},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714119},
doi = {10.1145/3706598.3714119},
abstract = {Good writing is a dynamic process of knowledge transformation, where writers refine and evolve ideas through planning, translating, and reviewing. Generative AI-powered writing tools can enhance this process but may also disrupt the natural flow of writing, such as when using LLMs for complex tasks like restructuring content across different sections or creating smooth transitions. We introduce Script&amp;Shift, a layered interface paradigm designed to minimize these disruptions by aligning writing intents with LLM capabilities to support diverse content development and rhetorical strategies. By bridging envisioning, semantic, and articulatory distances, Script&amp;Shift’s interactions allow writers to leverage LLMs for various content development tasks (scripting) and experiment with diverse organization strategies while tailoring their writing for different audiences (shifting). This approach preserves creative control while encouraging divergent and iterative writing. Our evaluation shows that Script&amp;Shift enables writers to creatively and efficiently incorporate LLMs while preserving a natural flow of composition.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {532},
numpages = {19},
keywords = {Human-AI collaborative writing, large language models, writing assistants, creativity support},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3581783.3613432,
author = {Elhagry, Ahmed},
title = {Text-to-Metaverse: Towards a Digital Twin-Enabled Multimodal Conditional Generative Metaverse},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3613432},
doi = {10.1145/3581783.3613432},
abstract = {Developing realistic and interactive virtual environments is a major hurdle in the progress of Metaverse. At present, majority of Metaverse applications necessitate the manual construction of 3D models which is both time-consuming and costly. Additionally, it is challenging to design environments that can promptly react to users' actions. To address this challenge, this paper proposes a novel approach to generate virtual worlds using digital twin (DT) technology and AI through a Text-to-Metaverse pipeline. This pipeline converts natural language input into a scene JSON, which is used to generate a 3D virtual world using two engines: Generative Script Engine (GSE) and Generative Metaverse Engine (GME). GME generates a design script from the JSON file, and then uses it to generate 3D objects in an environment. It aims to use multimodal AI and DT technology to produce realistic and highly detailed virtual environments. The proposed pipeline has potential applications including education, training, architecture, healthcare and entertainment, and could change the way designers and developers create virtual worlds. While this short paper covers an abstract as per the Doctorial Symposium's guidelines, it contributes to the research on generative models for multimodal data and provides a new direction for creating immersive virtual experiences.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {9336–9339},
numpages = {4},
keywords = {computer vision, digital twin, generative models, metaverse, multimodal ai, nlp},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@article{10.5555/3512489.3512509,
author = {Liang, Lily and Wellman, Briana and Amir, Uzma and Elhamdani, Abdeladim and Enamorado, Jeffrey and Watson, Jermel},
title = {Hybrid student service learning through creative community partnership: poster abstract},
year = {2021},
issue_date = {October 2021},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {37},
number = {3},
issn = {1937-4771},
abstract = {Service learning connects students with their communities and provides meaning and affirmation to their career aspirations. It also helps them to develop their professional identities as well as leadership. However, the pandemic forced service learning to be reinvented. We share our experience of establishing a student service-learning mechanism that integrates both virtual and in-person activities, implemented via a creative partnership between the University of the District of Columbia (UDC) and Rockville Science Center (RSC). With this mechanism, we were able to leverage the resources of both partnering organizations and continue providing service-learning opportunities to our students, despite the constraints caused by the pandemic. Given the resources of each partner as well as the skill sets and personal interests of student volunteers, we created the following two distinctive projects: 1) a virtual workshop series on Artificial Intelligence for K-12 students; 2) a computer refurbish program that repairs, and upgrades used computers to be distributed to families in need. In the first project, RSC provides virtual workshops, hosting and advertising platforms to reach the community. UDC trains student volunteers to become workshop instructors. For the second project, RSC provides donated computers/parts, a venue for computer donation drop-off and pick-up, and serves as a distribution center for refurbished computers. During social distancing, UDC student volunteers refurbished the donated computers at home after picking them up from RSC and returned them for distribution. The volunteers attended RSC workshops for free as a benefit. These workshops provided them extra-curriculum enrichment. We not only successfully transformed our service-learning program to a hybrid model, but also expanded both our team and the scope of our services. This hybrid service learning model, though designed under social distancing, can be utilized post-pandemic and will continue to benefit our students and the community.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {159},
numpages = {1}
}

@inproceedings{10.1145/3614419.3644000,
author = {Torricelli, Maddalena and Martino, Mauro and Baronchelli, Andrea and Aiello, Luca Maria},
title = {The Role of Interface Design on Prompt-mediated Creativity in Generative AI},
year = {2024},
isbn = {9798400703348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3614419.3644000},
doi = {10.1145/3614419.3644000},
abstract = {Generative AI for the creation of images is becoming a staple in the toolkit of digital artists and visual designers. The interaction with these systems is mediated by prompting, a process in which users write a short text to describe the desired image’s content and style. The study of prompts offers an unprecedented opportunity to gain insight into the process of human creativity. Yet, our understanding of how people use them remains limited. We analyze more than 145,000 prompts from the logs of two Generative AI platforms (Stable Diffusion and Pick-a-Pic) to shed light on how people explore new concepts over time, and how their exploration might be influenced by different design choices in human-computer interfaces to Generative AI. We find that users exhibit a tendency towards exploration of new topics over exploitation of concepts visited previously. However, a comparative analysis of the two platforms, which differ both in scope and functionalities, reveals some stark differences. Features diverting user focus from prompting and providing instead shortcuts for quickly generating image variants are associated with a considerable reduction in both exploration of novel concepts and detail in the submitted prompts. These results carry direct implications for the design of human interfaces to Generative AI and raise new questions regarding how the process of prompting should be aided in ways that best support creativity.},
booktitle = {Proceedings of the 16th ACM Web Science Conference},
pages = {235–240},
numpages = {6},
keywords = {Pick-a-Pic, Prompting, Stable Diffusion, creativity, explore-exploit},
location = {Stuttgart, Germany},
series = {WEBSCI '24}
}

@inproceedings{10.5555/3408352.3408621,
author = {Datta, Kamalika and Dutt, Arko and Zaky, Ahmed and Chand, Umesh and Singh, Devendra and Li, Yida and Huang, Jackson Chun-Yang and Thean, Aaron and Aly, Mohamed M Sabry},
title = {Fledge: flexible edge platforms enabled by in-memory computing},
year = {2020},
isbn = {9783981926347},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {The proliferation of advanced analytics and artificial intelligence has been driven by huge volumes of data that are mostly generated at the edge. Simultaneously, there is a rising demand to perform analytics on edge platforms (i.e., near-sensor data analytics). However, conventional architectures of such platforms may not execute the targeted applications in an energy-efficient manner. Emerging near and in-memory computing paradigms can increase the energy efficiency of edge platforms by relying on emerging logic and memory devices. More importantly, these paradigms enable the possibility of performing computations on unconventional platforms, namely flexible computing systems. In this paper, we explore the benefits of in-memory computing at the edge on a flexible substrate enabled by thin-film transistors (TFTs) and resistive RAM (RRAM). As a case study, we consider bio-signal processing application workloads, i.e., compressive sensing and anomaly detection. We model the device, circuit, and architecture of our targeted platform and evaluate the corresponding system-level performance. Preliminary results indicate that in-memory computing enabled by flexible electronic devices enables a new class of edge platforms with lower power consumption, compared to that of rigid TFT devices.},
booktitle = {Proceedings of the 23rd Conference on Design, Automation and Test in Europe},
pages = {1181–1186},
numpages = {6},
keywords = {RRAM, edge computing, flexible electronics, thin-film transistor},
location = {Grenoble, France},
series = {DATE '20}
}

@inproceedings{10.1145/3656650.3656688,
author = {Grigis, Paolo and De Angeli, Antonella},
title = {Playwriting with Large Language Models: Perceived Features, Interaction Strategies and Outcomes},
year = {2024},
isbn = {9798400717642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3656650.3656688},
doi = {10.1145/3656650.3656688},
abstract = {Large Language Models (LLMs) are sparking debates about creativity, intellectual property, and artistic integrity. This paper focuses on creativity, defined as consensual agreement among domain experts. It presents an inductive analysis of seven semi-structured interviews with professional playwrights who engaged in a longitudinal project with the aim of writing a theatre script using commercial systems. Overall, participants regarded LLMs as unsuitable for playwrighting. However, they enjoyed the experience and identified utility for editorial tasks and brainstorming. A significant obstacle was associated with the politics embedded in LLMs. Not only did these systems avoid a language that could offend sensibilities, but they also refused to engage in taboos and conflicts, which are the core of dramaturgy. Other system features (speed, exploitation, and unpredictability) were sometimes considered conducive and sometimes detrimental to creativity. Participants experienced difficulties and tried to build common ground by trial and error. Often, this strategy evolved into role play: the playwright instructed the LLM to enact characters. The interaction provided hints of inspiration and fostered suspension of disbelief and ontological reflection. However, it often led to technology rejection. Comparing and contrasting our insights with related work, we conclude by opening new directions for research at the boundaries of HCI and AI.},
booktitle = {Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
articleno = {38},
numpages = {9},
keywords = {Creative AI, Creativity, Roleplay, Suspension of Disbelief, Theatre, Unpredictability},
location = {Arenzano, Genoa, Italy},
series = {AVI '24}
}

@inproceedings{10.1145/3446368.3452126,
author = {Cutler, Larry and Darnell, Eric and Dirksen, Nathaniel and Tucker, Amy and Stafford, Scot and Oh, Erick and Nagpal, Anika and Lee, Eusong and Ladd, Nick},
title = {From quest to quill: pushing the boundaries of VR storytelling in baobab's Baba Yaga and Namoo},
year = {2022},
isbn = {9781450383226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446368.3452126},
doi = {10.1145/3446368.3452126},
abstract = {In this session, the team behind the Immersive Best-in-Show winner (Bonfire) at Siggraph 2019 reveals creative and technical insights from their two recent award winning VR projects: Baba Yaga and Namoo.Baba Yaga: Inspired by one of the most distinctive and well-known characters from Eastern European folklore, Baobab Studio's interactive VR story Baba Yaga re-imagines this ancient fairytale with themes of environmental conservation and female empowerment (the project features an all-female, diverse cast with Kate Winslet, Daisy Ridley, Jennifer Hudson, and Glenn Close). Baba Yaga is Baobab's most ambitious interactive experience to date, the culmination of all of its previous narrative experiments with AI intelligent characters, real-time responsive environments, emergent branching storytelling, all while pushing the boundaries of what it means to tell stories in immersive animation. The creative team will explore the following areas of innovation (and more) on this project: How do they make you, the audience, a main character where your choices really matter and have meaningful consequences. How did they create a fairytale universe that is fully interactive with real-time AI-driven characters and environments. How did they employ a theatrical art style for VR that combines theatrical lighting, stage-craft design elements, and a hand-crafted feel all running in real-time on a mobile headset. How did they layer spatialized sound and music into our process to recreate the mythical world of Baba Yaga?The Baba Yaga speakers are Eric Darnell, writer/director and co-founder of Baobab Studios, Nathaniel Dirksen, visual effects supervisor, Amy Tucker, lighting supervisor, Larry Cutler, executive producer, and Scot Stafford, Sound Supervisor.Namoo: Namoo (meaning "tree" in Korean), is a narrative poem come to life as an animated VR experience entirely created with Oculus's VR animation tool "Quill." The project is led by esteemed Korean director Erick Oh (who won Annecy's Cristal Award for TV for The Dam Keeper Poems with Tonko House) in partnership with Baobab Studios. The entire piece takes place on a grassy knoll next to a seed that grows into a sapling and eventually a fully mature tree. This namoo might be interpreted as a kind of metaphor for the man's life, as it collects his meaningful memories in its branches - from pacifiers and stuffed animals to books, typewriters, and favorite scarves - to broken glasses and objects from times he'd rather soon forget. Namoo is a deeply personal yet surprisingly universal piece that will undoubtedly resonate with each viewer differently. The Namoo team will dive into all aspects of VR filmmaking to bring this visually rich film to life using Quill, from storyboarding to visual development to camera and staging to animation to optimizations for rendering on the Oculus Quest mobile headset.The Namoo speakers are Erick Oh, writer/director, Anika Nagpal, production manager, Eusong Lee, art director, and Nick Ladd, lead quill artist.},
booktitle = {ACM SIGGRAPH 2021 Production Sessions},
articleno = {6},
numpages = {1},
location = {Virtual Event, USA},
series = {SIGGRAPH '21}
}

@inproceedings{10.1145/3641520.3665311,
author = {Petrenko, Oleksandr and Puchka, Oleksandr and Klimenko, Alex},
title = {Revolutionizing VFX Production with Real-Time Volumetric Effects},
year = {2024},
isbn = {9798400705267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641520.3665311},
doi = {10.1145/3641520.3665311},
abstract = {Zibra AI introduces a unified approach, integrating both the creation and rendering of VFX directly within game engines, aiming to streamline and enhance the production process. The new workflow is empowered by two major innovations: ZibraVDB — an OpenVDB compression tool that enables real-time rendering of volumetric VFX, and Zibra Effects — a highly performant real-time VFX simulation technology. Both tools support in-engine workflows for Unreal Engine, Unity, or any other custom engine. ZibraVDB achieves up to a 150x compression rate while maintaining the high visual fidelity required for advanced production workflows such as virtual production and AAA game development. Combined with a highly efficient custom render pass, it achieves significant rendering performance. Zibra Effects enables the physics-based VFX simulation of liquids, smoke, and fire directly inside a game engine. It leverages proprietary AI-based SDF compression technology that reduces collider memory footprint by 40x on average. In combination with ZibraVDB, it covers the VFX creation workflow end-to-end: users can choose between real-time interactive effects or cache them into ZibraVDB format for efficiency},
booktitle = {ACM SIGGRAPH 2024 Real-Time Live!},
articleno = {10},
numpages = {2},
keywords = {Real-time visual effects simulation, SDF compression, VDB compression, Volumetric visual effects},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@inproceedings{10.1145/3423603.3424052,
author = {Hafaiedh, Khaled and Rhouma, Mouhib Ben and Chargui, Fahd and Haouas, Yassine and Kerkeni, Ahmed},
title = {AdRobot: a smart segmentation application for automated &amp; personalized marketing campaigns},
year = {2020},
isbn = {9781405377539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423603.3424052},
doi = {10.1145/3423603.3424052},
abstract = {Digital Advertising and promotional e-campaigns have been a basic pillar of marketing. One of the main challenges marketers face nowadays is about associating the right promotion to the right customer. Making the product-customer assignment accurate is crucial to satisfy customer needs. However, manually analyzing qualitative data for the purpose of defining the right target audience is exhausting and time consuming, especially when the number of costumers is high. In this paper, our aim is to automatically assign personalized campaigns that match specific customer desire, therefore making promotional campaigns consistent with their interests. Automating the process of assigning the right promotion to the right customer according to its specific needs is appealing as customers often show little to no interest in random ads. Our solution, referred to as "AdRobot", aims at overcoming these challenges by gathering complex data and insights into the target audience using data collected from conversations via the designed chatbot. Our strategy consists of performing fine-grained audience classification by segmenting profiles based on some profiling and conversational constraints, so that the audience is matched with the right promotional campaign. In order to achieve this goal, we propose an algorithm that investigates profiling and conversational data collected along with the customers' intents using artificial intelligence heuristics. Results show that "AdRobot" accurately matches promotional campaigns with the right customers according to their needs.},
booktitle = {Proceedings of the 2nd International Conference on Digital Tools &amp; Uses Congress},
articleno = {15},
numpages = {6},
keywords = {personalized campaigns, marketing, automatic segmentation, artificial intelligence},
location = {Virtual Event, Tunisia},
series = {DTUC '20}
}

@proceedings{10.1145/3680531,
title = {SA Computer Animation Festival '24: SIGGRAPH Asia 2024 Computer Animation Festival},
year = {2024},
isbn = {9798400711343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Computer Animation Festival celebrates the world of computer animation and boasts an extensive program that encompasses a wide array of creative content, including short films, scientific visualizations, and state-of-the-art AI-enhanced animations.},
location = {Tokyo, Japan}
}

@inproceedings{10.1145/3681757.3697052,
author = {Hwang, Sienna and Jia, Muqing and Cao, Yan-Pei and Guo, Yuan-Chen and Li, Yangguang and Liang, Ding},
title = {Tripo Doodle: The Next-Gen AI 3D Creative Tool},
year = {2024},
isbn = {9798400711398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3681757.3697052},
doi = {10.1145/3681757.3697052},
abstract = {Creating 3D digital content has been a tough challenge, especially when dealing with scenes packed with objects or characters performing complex motions. With Tripo Doodle, we can now rapidly prototype entire scenes and fully animatable characters, with nothing more than simple doodles and text prompts. The AI-generated assets are high- quality and ready to be used across a range of applications---from gaming and animation to immersive virtual experiences and beyond. This technology opens the door to new creative possibilities, making 3D content creation faster and more accessible than ever before.},
booktitle = {SIGGRAPH Asia 2024 Real-Time Live!},
articleno = {10},
numpages = {1},
location = {Tokyo, Japan},
series = {SA Real-Time Live! '24}
}

@inproceedings{10.1145/3677846.3677855,
author = {Shen, Ming and Huang, Gang and Wu, Yuxuan and Song, Shuyi and Zhou, Sheng and Li, Liangcheng and Yu, Zhi and Wang, Wei and Bu, Jiajun},
title = {Making Accessible Movies Easily: An Intelligent Tool for Authoring and Integrating Audio Descriptions to Movies},
year = {2024},
isbn = {9798400710308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677846.3677855},
doi = {10.1145/3677846.3677855},
abstract = {Blind and visually impaired (BVI) individuals encounter significant challenges in perceiving the visual content of movies. Audio descriptions (AD) are inserted into speech gaps to describe visual content and storyline for BVI individuals. However, the processes of authoring and integrating AD are laborious, involving tasks such as identifying speech gaps, authoring AD scripts, dubbing, and integrating them into the movie. To streamline these processes, we introduce EasyAD, an intelligent tool to automate these processes. EasyAD utilizes character recognition technology to identify speech gaps and utilizes speech synthesis technology for AD dubbing. EasyAD addresses the misidentification of the background music of existing methods, and for the first time applies a multimodal large language model in the tool to generate AD. EasyAD is currently operational at the China Braille Library and we invite 6 AD authors for a user study. The results demonstrate that with the use of EasyAD, the processing time for a medium-difficulty movie is reduced by nearly 50%, reducing the workload of AD authors and accelerating accessible movie production in China. EasyAD leverages the advantages of AI technologies, especially multimodal large language models, for accessible movie production and benefits BVI individuals.},
booktitle = {Proceedings of the 21st International Web for All Conference},
pages = {160–164},
numpages = {5},
keywords = {accessible movie, accessibility, blind and visually impaired, audio descriptions, multimodal large language model},
location = {Singapore, Singapore},
series = {W4A '24}
}

@inproceedings{10.1145/3613905.3651049,
author = {Sun, Qirui and Luo, Qiaoyang and Ni, Yunyi and Mi, Haipeng},
title = {Text2AC: A Framework for Game-Ready 2D Agent Character(AC) Generation from Natural Language},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3651049},
doi = {10.1145/3613905.3651049},
abstract = {The visual appearance and personality of game characters are crucial to the player experience. Assembling characters in 2D games is a common practice with a significant demand for creativity, yet the creation process remains challenging, confined to professional graphic designers and animators. Traditional automated character creation in games usually involves manual parameter adjustment or the use of photos as references, coupled with the selection of pre-set backgrounds or professions within the game, leading to limitations in terms of freedom, usability, and consistency. However, with advancements in natural language processing, large language models (LLMs), and generative AI, we have the opportunity to reimagine the process of user-interactive character creation. This paper introduces a novel text-to-character methodology, integrating LLM technology with generative image AI, eliminating the need for reference photos or manual parameter editing while providing extensive options for character creation. Through natural language processing and automated skeletal animation, the generated 2D characters are capable of being driven across various web platforms and game engines. This exploration aims to establish a framework for driven agents that harmonizes the visual layer with linguistic personality, thereby providing a more enriched user experience in games and other applications as agents.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {312},
numpages = {7},
keywords = {2D Character Animation, Game Character Generation, Generative AI, Personalized Agents},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{10.1145/3613905.3651123,
author = {Sawada, Shoko and Suzuki, Tomoyuki and Yamaguchi, Kota and Toyoda, Masashi},
title = {Visual Explanation for Advertising Creative Workflow},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3651123},
doi = {10.1145/3613905.3651123},
abstract = {Explainable AI (XAI) attempts to produce interpretable results from highly complex AI systems, but its form and effectiveness vary depending on the application domain. In this paper, we explore how XAI techniques can help graphic designers work on advertising materials. A creative domain such as graphic design is often characterized by a weak connection between the individual work and the business goal; e.g., a small change in the design of a banner can result in a huge difference in the audience’s reaction. We develop an XAI system for designers that provides visual feedback explaining which component of the design is likely to affect the business metric. Our user study shows that with our system, designers complete the project in fewer iterations and in less time to achieve the desired quality of work compared to naive score-based feedback. These findings highlight the benefits of leveraging XAI in creative domains.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {368},
numpages = {8},
keywords = {Graphic design, Visual explanation, XAI},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{10.1145/3681758.3698021,
author = {Nam, Seoyong and Chung, Minho and Kim, Haerim and Kim, Eunchae and Kim, Taehyeon and Yoo, Yongjae},
title = {Automatic Generation of Multimodal 4D Effects for Immersive Video Watching Experiences},
year = {2024},
isbn = {9798400711404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3681758.3698021},
doi = {10.1145/3681758.3698021},
abstract = {The recent trend of watching content using over-the-top (OTT) services pushes the 4D movie industry to seek a way of transformation. To address the issue, this paper suggests an AI-driven automatic 4D effects generation algorithm applied to a low-cost comfort chair. The system extracts multiple features using psychoacoustic analysis, saliency detection, optical flow, and an LLM-based thermal effect synthesis, and maps them into various sensory displays such as vibration, heat, wind, and poking automatically. To evaluate the system, a user study with 21 participants across seven film genres was conducted. The results showed that 1) there was a general improvement with 4D effects in terms of immersion, concentration, and expressiveness, and 2) multisensory effects were particularly useful in action and fantasy movie scenes. The suggested system could be directly used in current general video-on-demand services.},
booktitle = {SIGGRAPH Asia 2024 Technical Communications},
articleno = {25},
numpages = {4},
keywords = {4D films, haptics, multimodal, AI-driven haptic effects generation},
location = {
},
series = {SA '24}
}

@inproceedings{10.1145/3626253.3633418,
author = {Gunawardena, Ananda and Chaturvedi, Naina},
title = {AI Enhanced Learning: Powering Curated Videos with Generative Intelligence},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3633418},
doi = {10.1145/3626253.3633418},
abstract = {Instructional videos are becoming increasingly popular among computer science students. Over 78% of students frequently visit YouTube to find videos as supplement to their textbook or classroom instruction[1]. Recent surveys show that on average, 73% of students prefer having their instructors curate a supplemental video library to aid in their learning. Now, the emergence of generative AI is revolutionizing supplemental video instruction, enabling instructors to generate slides, recording scripts, and produce high-quality videos with deep search and embedded interactive activities.Generative AI also takes the student video learning to a new level by providing AI-generated video summaries, on-demand questions, and exploration of topics in greater depth. Integrating AI into standard videos greatly expands the possibilities of video-based learning. This workshop demonstrates how educators can enhance their existing video playlists by incorporating AI to increase student engagement and establish safety measures for AI use in education. By using dynamic dashboards, scheduled content, and gamified questions, instructors can maintain student focus.Drawing on insights from computer science courses taught at Princeton and Rutgers Universities, we will highlight the transformative potential of AI-enhanced videos in promoting active learning, particularly in large classes. We will discuss engagement strategies and real-time data visualizations applicable to any video platform. We will utilize the cubits.ai[2] platform, a Princeton University initiative that enhances the impact of computer science courses. The platform is free, and participants are encouraged to bring their own video playlists to curate them into AI-enabled collections by enhancing the student experience through integrated generative AI.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1898},
numpages = {1},
keywords = {ai generated content, contextualized generative ai, cost-effective videos, customized videos, data-driven insights, instructional videos, video summarization},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3478432.3499157,
author = {Walsh, Benjamin and Ali, Safinah and Castro, Francisco and Desportes, Kayla and DiPaola, Daniella and Lee, Irene and Payne, William and Sieke, Scott and Zhang, Helen},
title = {Making Art with and about Artificial Intelligence: Three Approaches to Teaching AI and AI Ethics to Middle and High School Students},
year = {2022},
isbn = {9781450390712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478432.3499157},
doi = {10.1145/3478432.3499157},
abstract = {In this hands-on workshop participants will experience the curricula from three NSF funded projects, which engage youth in creating art with and about AI technologies while exploring related ethical concerns.danceON is a web-based creative coding environment that engages learners in creating multimedia dance performances. Besides writing reactive code that generates animations to augment dance performances, students also use the system to explore the boundaries and biases of AI.DAILy is a curriculum focused on developing AI literacy among middle school students through the integration of technical concepts and processes, ethical and societal implications, and career futures in AI. Participants will be focusing on the AI + art modules of DAILy.Imagine AI develops project-based curricula to support youth in exploring critical ethical issues related to AI. Students read short stories featuring youth at the center of AI ethical dilemmas, build and manipulate AI systems, and create digital media to express ethical stances.Participants will leave the workshop with multiple AI teaching strategies that blend technical learning with social purpose and creative expression.},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education V. 2},
pages = {1203},
numpages = {1},
keywords = {artificial intelligence education, art, ai ethics education},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}

@inproceedings{10.1145/3681756.3697919,
author = {Wang, Zhiwei and Xia, Yuzhe and Nie, Kexin and Guo, Mengyao},
title = {Alive Yi: Interactive Preservation of Yi Minority Embroidery Patterns through Digital Innovation},
year = {2024},
isbn = {9798400711381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3681756.3697919},
doi = {10.1145/3681756.3697919},
abstract = {By integrating heritage, design, and technology, “Alive Yi" innovatively empowers intangible cultural heritage through interactive generative design. Using our extensive pattern database, we visualize traditional Yi minority embroidery by applying particle and 3D effects in TouchDesigner with Leap Motion, emphasizing their symbolic meanings. This interactivity allows users to transform heritage visuals in real-time, fostering deeper engagement and appreciation. Future plans include using AI tools to create designs appealing to youth. Despite technical challenges, this approach promises cultural resilience in the digital age, offering a blueprint for leveraging computational creativity to preserve and sustain intangible culture.},
booktitle = {SIGGRAPH Asia 2024 Posters},
articleno = {106},
numpages = {2},
keywords = {Interactive Visualization, Cultural Preservation, Computational Heritage, Generative Patterns Design},
location = {
},
series = {SA '24}
}

@inproceedings{10.1145/3706599.3720140,
author = {Ni, Yunyi and Sun, Qirui and Chai, Yuan and Mi, Haipeng},
title = {VoiceForge: A Text-Driven Character Voice Generation System for Narrative Content Creation},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3720140},
doi = {10.1145/3706599.3720140},
abstract = {In narrative content creation, generating distinctive and appropriate character voices remains challenging. While professional voice acting achieves ideal results, its high costs and complex production process limit widespread adoption. We present VoiceForge, an interactive system that enables users to intuitively generate character voices through natural language descriptions. The system converts textual character descriptions into unique voice configurations and intelligently identifies dialogue in scripts to automatically assign corresponding voices to different characters. Unlike existing platforms with preset voice libraries, VoiceForge offers flexible voice customization through text-driven generation and audio mixing capabilities. Our user evaluation comparing VoiceForge with professional voice acting and existing AI solutions shows that our system-generated voices approach professional quality in terms of character matching and speech fluency, significantly outperforming existing AI platforms. This research demonstrates an effective method for bridging the gap between character description and voice generation in narrative content creation.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {584},
numpages = {6},
keywords = {Text-to-Speech, Character Voice Generation, Voice Customization, Narrative Content Creation},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3539597.3575791,
author = {Farseev, Aleksandr},
title = {Under the Hood of Social Media Advertising: How Do We use AI Responsibly for Advertising Targeting and Creative Evaluation},
year = {2023},
isbn = {9781450394079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539597.3575791},
doi = {10.1145/3539597.3575791},
abstract = {Digital Advertising is historically one of the most developed areas where Machine Learning and AI have been applied since its origination. From smart bidding to creative content generation and DCO, AI is well-demanded in the modern digital marketing industry and partially serves as a backbone of most of the state-of-the-art computational advertising systems, making them impossible for the AI tech and the programmatic systems to exist apart from one another. At the same time, given the drastic growth of the available AI technology nowadays, the issue of responsible AI utilization as well as the balance between the opportunity of deploying AI systems and the possible borderline etic and privacy-related consequences are still yet to be discussed comprehensively in both business and research communities. Particularly, an important issue of automatic User Profiling use in modern Programmatic systems like Meta Ads as well as the need for responsible application of the creative assessment models to fit into the business etic guidelines is yet to be described well. Therefore, in this talk, we are going to discuss the technology behind modern programmatic bidding and content scoring systems and the responsible application of AI by SoMin.ai to manage the Advertising targeting and Creative Validation process.},
booktitle = {Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
pages = {1281–1282},
numpages = {2},
keywords = {ads performance prediction, digital advertising},
location = {Singapore, Singapore},
series = {WSDM '23}
}

@inproceedings{10.1145/3674912.3674922,
author = {Weerakoon, Oshani and Lepp\"{a}nen, Ville and M\"{a}kil\"{a}, Tuomas},
title = {Enhancing Pedagogy with Generative AI: Video Production from Course Descriptions},
year = {2024},
isbn = {9798400716843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674912.3674922},
doi = {10.1145/3674912.3674922},
abstract = {This paper explores a novel workflow that integrates Generative AI tools, ChatGPT and DALL·E, into educational use, aiming to improve the traditional teaching methods in university education. Our workflow is focused on creating short introductory videos for university courses, using primary course descriptions available in the university’s study guide with the idea of introducing courses visually. This approach was deliberately selected for experimentation, and we believe that it could be further enhanced to generate course videos on specific course topics. This will minimize the efforts of teachers who are required to produce detailed course videos as a part of their teaching. As the first part of our workflow, we present a tool that utilizes ChatGPT-4 and DALL·E 2 to autonomously generate a script and background graphics for videos, using primary course descriptions extracted through a given course web URL. As the second part of the workflow, we combine those generated artefacts into videos using Narakeet, a Text-to-Speech software service that is available online. To analyze the feasibility of this workflow, we then conducted a field survey where university teachers participated in reviewing introductory course videos of their courses generated through our workflow. We employed only engineering courses that are English-taught in this field survey. The results demonstrate the potential of AI-generated content to increase the efficiency of teachers when creating video materials. However, challenges such as the uncanny valley effect in text-to-speech narration and the propensity for AI-generated misinformation highlight the need for careful review by humans on such content before setting it for wider use. This paper argues for the strategic integration of AI in university education, focusing on the benefits, while acknowledging the limitations owned by generative AI tools.},
booktitle = {Proceedings of the International Conference on Computer Systems and Technologies 2024},
pages = {249–255},
numpages = {7},
keywords = {AI in Education, ChatGPT, DALL·E, Generative AI, Pedagogical Tools},
location = {Ruse, Bulgaria},
series = {CompSysTech '24}
}

@inproceedings{10.1145/3630106.3659009,
author = {Grill, Gabriel},
title = {Constructing Capabilities: The Politics of Testing Infrastructures for Generative AI},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3659009},
doi = {10.1145/3630106.3659009},
abstract = {The advertised and perceived capabilities of generative AI products like ChatGPT have recently stimulated considerable investments and discourse surrounding their potential to aid and replace work. The prominence of these systems, and their promise to be general-purpose, has resulted in an avalanche of tests to discover and certify their capabilities. This new testing regime is concerned with creating ever-more tasks for generative AI products instead of testing a model for one specialized task. Beyond efforts to understand products’ capabilities, the construction of tasks and corresponding tests are also performative enactments meant to convince others and thus to gain attention, scientific legitimacy, and investment. The current market concentration of a few big AI companies points to a concerning conflict of interest: those with a vested interest in the success of the technology also have control over globalized testing infrastructures and thereby the exclusive means to create extensive knowledge claims about these systems. In this paper, I theorize capabilities as contested constructions and situated accomplishments shaped by power imbalances. I further unpack the globalized testing infrastructures involved in the construction and stabilization of generative AI products’ capabilities. Furthermore, I discuss how the testing of these AI models and products is externalized, extracting value from the unpaid or under-paid labor of researcher and developer communities, content creators, subcontractors, and users. Lastly, I discuss a reflexive and critical approach to testing that challenges depoliticization and seeks to produce lasting critiques that serve more emancipatory goals.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1838–1849},
numpages = {12},
keywords = {ML benchmarks, affordances, capabilities, generative AI, infrastructure studies, testing},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@inproceedings{10.1145/3627043.3659574,
author = {Manzoor, Ahtsham and Ziegler, Samuel C. and Garcia, Klaus Maria. Pirker and Jannach, Dietmar},
title = {ChatGPT as a Conversational Recommender System: A User-Centric Analysis},
year = {2024},
isbn = {9798400704338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627043.3659574},
doi = {10.1145/3627043.3659574},
abstract = {With the rapid advances in deep learning, we have witnessed a strongly increased interest in conversational recommender systems (CRS). Until recently, however, even the latest generative models exhibited major limitations and they frequently return non-meaningful responses according to previous studies. However, with the latest Generative AI-based dialog systems implemented with Generative Pre-Trained Transformer (GPT) models, a new era has arrived for CRS research. In this work, we study the use of ChatGPT as a movie recommender system. To this purpose, we conducted an online user study involving N=190 participants, who were tasked to evaluate ChatGPT’s responses in a multitude of dialog situations. As a reference point for the analysis, we included a retrieval-based conversational method in the experiment, which was found to be a robust approach in previous research. Our study results indicate that the responses by ChatGPT were perceived to be significantly better than those by the previous system in terms of their meaningfulness. A detailed inspection of the results showed that ChatGPT excelled when providing recommendations, but sometimes missed the context when asked questions about a movie within a longer dialog. A statistical analysis revealed that information adequacy and recommendation accuracy of the responses had the strongest influence on the perceived meaningfulness of the responses. Finally, an additional analysis showed that the human perceptions of meaningfulness correlated only very weakly with computational metrics such as BLEU or ROUGE, emphasizing the importance of involving humans in the evaluation of a CRS.},
booktitle = {Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {267–272},
numpages = {6},
keywords = {Conversational Recommendation, Large Language Models, User Study},
location = {Cagliari, Italy},
series = {UMAP '24}
}

@inproceedings{10.1145/3422839.3423059,
author = {Rouxel, Alexandre},
title = {AI in the Media Spotlight},
year = {2020},
isbn = {9781450381468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422839.3423059},
doi = {10.1145/3422839.3423059},
abstract = {The use of AI technology offers many new opportunities for the media sector; in particular, it leads to an increase in productivity and efficiency to convey relevant information to appropriate viewers quickly and accurately.  In this keynote, I will show how AI is gradually transforming the content production and distribution chain for broadcasters and media in general. We will start with an overview of AI applications in the media field and the underlying technologies. Then we will go through some projects led or developed by the EBU for the Public Media Services, PSM. To conclude, I will sketch the trend, the limitations and potential evolutions of the uptake of AI in media. The range of AI applications in the mediums of written press, cinema, radio, television and advertising is widespread. To start with the content production and post-production AI is used in video creation and editing, in the written press, for automatic or assisted writing, information analysis and verification. Without being exhaustive, in the broad field of audience analytics, AI can identify the optimal audience for a given content, personalise and recommend the content for a targeted audience or specific user depending on the granularity. From the perspective of accessibility and inclusion, AI plays a predominant role in improving access to content through transcription, translation, vocal synthesis and recommendation.&nbsp; In this context of the raising of AI in the media sphere, the PSM are facing the need to be innovative and transform their value chain to reach the audience better. This can't be performed without keeping the PSM remit which combines a full range of distinctive quality content to fulfil its central mission: inform, educate, entertain.  As such, the EBU is leading projects and developing technologies to leverage AI capabilities for media while meeting PSM remit [1]. Firstly, the EBU is leading a project to benchmark AI tools on the market in the context of PSM. As a first step, we are focusing on Automatic Speech Recognition. Therefore I will describe the objective, the metrics and the evolution of the tool. Among other activities related to machine learning and metadata, the EBU is developing a tool to generate high-level tags on written content. Since NLP recently achieved a breakthrough for many applications, we are working on leveraging this technology to produce high-level explainable tags on written contents. As they are called high level, these tags identify properties correlated with several groups of linguistic features like vocabulary, grammar, semantic or formality. Originally designed to detect fake news they can as well feed recommender systems or classifiers. I will detail the machine learning algorithms behind the tool and pave the way of future works.&nbsp;},
booktitle = {Proceedings of the 2nd International Workshop on AI for Smart TV Content Production, Access and Delivery},
pages = {1–2},
numpages = {2},
keywords = {public service media, media, machine learning, content tagging, broadcasters, benchmarking, ai},
location = {Seattle, WA, USA},
series = {AI4TV '20}
}

@article{10.1145/3725885,
author = {Yang, Qi and Farseev, Aleksandr and Ongpin, Marlo and Huang, Alfred and Chu-Farseeva, Yu-Yi and You, Da-Min and Lepikhin, Kirill and Nikolenko, Sergey},
title = {Fusing Predictive and Large Language Models for Actionable Recommendations In Creative Marketing},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1046-8188},
url = {https://doi.org/10.1145/3725885},
doi = {10.1145/3725885},
abstract = {The opaqueness of modern digital advertising, exemplified by large platforms such as Meta Ads, raises concerns regarding their control over audience targeting, pricing structures, and ad relevancy assessments. Locked in place by network effects, these natural monopolies attract countless advertisers who rely on subjective intuition, with billions of dollars lost on ineffective social media advertisements. The platforms’ algorithms rely on huge amounts of data unavailable to advertisers, and the algorithms themselves are opaque too, so advertisers often cannot make informed decisions. To promote transparency and help individual advertisers, we first propose novel ways to optimize advertising strategies, predicting click-through rates of novel advertising content based on the content itself. However, advertisers face both opaqueness and a vast abundance of data: a large platform has so many competitor ads that it is hard to derive meaningful insights. Drawing inspiration from the success of large language models (LLM), we propose a system that merges multimodal LLMs and pretrained AI models with an emphasis on digital marketing and advertising data analysis. Leveraging the capabilities of LLMs and incorporating explainability features, including modern text-image models, we aim to improve efficiency and produce synergy between human marketers and AI systems.},
note = {Just Accepted},
journal = {ACM Trans. Inf. Syst.},
month = apr,
keywords = {Digital Advertising, Ads Performance Prediction, Deep Learning, Large Language Model, Explainable AI}
}

@inproceedings{10.1145/3681757.3697054,
author = {He, Chengan and Amador Herrera, Jorge Alejandro and Zhou, Yi and Shu, Zhixin and Sun, Xin and Feng, Yao and Pirk, S\"{o}ren and Michels, Dominik L. and Zhang, Meng and Wang, Yangtuanfeng and Rushmeier, Holly},
title = {Digital Salon: An AI and Physics-Driven Tool for 3D Hair Grooming and Simulation},
year = {2024},
isbn = {9798400711398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3681757.3697054},
doi = {10.1145/3681757.3697054},
abstract = {We introduce Digital Salon, a novel approach to 3D hair grooming and simulation by integrating advanced AI and physics-based algorithms. This tool enables users to create detailed hairstyles through natural language descriptions, seamlessly blending text-driven hair generation, interactive editing, and high-fidelity rendering within a cohesive workflow. With its innovative real-time simulation capabilities, Digital Salon supports dynamic hair interactions, accommodating 10,000 to 80,000 strands, thus making sophisticated hair design accessible to a wide range of users. This tool significantly enhances the creative process in digital media by providing an intuitive, versatile, and efficient solution for hair modeling and animation.},
booktitle = {SIGGRAPH Asia 2024 Real-Time Live!},
articleno = {4},
numpages = {1},
location = {Tokyo, Japan},
series = {SA Real-Time Live! '24}
}

@article{10.1145/3716848,
author = {Fu, Yujia and Liang, Peng and Tahir, Amjed and Li, Zengyang and Shahin, Mojtaba and Yu, Jiaxin and Chen, Jinfu},
title = {Security Weaknesses of Copilot-Generated Code in GitHub Projects: An Empirical Study},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3716848},
doi = {10.1145/3716848},
abstract = {Modern code generation tools utilizing AI models like Large Language Models (LLMs) have gained increased popularity due to their ability to produce functional code. However, their usage presents security challenges, often resulting in insecure code merging into the code base. Thus, evaluating the quality of generated code, especially its security, is crucial. While prior research explored various aspects of code generation, the focus on security has been limited, mostly examining code produced in controlled environments rather than open source development scenarios. To address this gap, we conducted an empirical study, analyzing code snippets generated by GitHub Copilot and two other AI code generation tools (i.e., CodeWhisperer and Codeium) from GitHub projects. Our analysis identified 733 snippets, revealing a high likelihood of security weaknesses, with 29.5% of Python and 24.2% of JavaScript snippets affected. These issues span 43 Common Weakness Enumeration (CWE) categories, including significant ones like CWE-330: Use of Insufficiently Random Values, CWE-94: Improper Control of Generation of Code, and CWE-79: Cross-site Scripting. Notably, eight of those CWEs are among the 2023 CWE Top-25, highlighting their severity. We further examined using Copilot Chat to fix security issues in Copilot-generated code by providing Copilot Chat with warning messages from the static analysis tools, and up to 55.5% of the security issues can be fixed. We finally provide the suggestions for mitigating security issues in generated code.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
keywords = {Code Generation, Security Weakness, CWE, GitHub Copilot, GitHub Project}
}

@inproceedings{10.1145/3702163.3702165,
author = {Gong, Rushi and Jiang, Rui and Guo, Chuanlei and Hu, Wanqing and Li, Yanyan},
title = {Roles emerging during the knowledge construction process in collaborative learning: Does a generative AI-support chatbot matter?},
year = {2025},
isbn = {9798400717819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702163.3702165},
doi = {10.1145/3702163.3702165},
abstract = {Students’ emerging roles in computer supported collaborative learning (CSCL) are crucial in revealing what learning characteristics and states students present during their collaborative knowledge construction. Previous researchers have unveiled the fact that pedagogical scaffoldings such as AI chatbots play a pivotal role in students’ role emerging, but with the prevalence of generative AI (GAI), there is also an urgent need to investigate whether GAI chatbots influence students’ emerging roles during the knowledge construction process in collaborative learning. Therefore, this study conducted a quasi-experiment, using an integration of cluster analysis, chi-square test, case analysis, and content analysis to investigate whether and how a GAI chatbot affected students’ emerging roles in their online collaborative knowledge construction. Results demonstrated statistical significance that the GAI chatbot and the traditional static scripts did not have a distinct difference in students’ emerging roles. However, qualitative data showed that the GAI chatbot had an impact on the allocation of roles and that there were perceptual differences in how students with the same roles experienced the writing process and collaborative atmosphere under different support conditions. The study will provide insights into how GAI chatbots can be adapted for future development and application in a collaborative learning context with consideration of students’ roles.},
booktitle = {Proceedings of the 2024 16th International Conference on Education Technology and Computers},
pages = {8–16},
numpages = {9},
keywords = {Computer Supported Collaborative Learning, Generative AI Chatbot, Knowledge Construction, Students’ Roles},
location = {
},
series = {ICETC '24}
}

@article{10.1145/3618333,
author = {Li, Jiaman and Wu, Jiajun and Liu, C. Karen},
title = {Object Motion Guided Human Motion Synthesis},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3618333},
doi = {10.1145/3618333},
abstract = {Modeling human behaviors in contextual environments has a wide range of applications in character animation, embodied AI, VR/AR, and robotics. In real-world scenarios, humans frequently interact with the environment and manipulate various objects to complete daily tasks. In this work, we study the problem of full-body human motion synthesis for the manipulation of large-sized objects. We propose Object MOtion guided human MOtion synthesis (OMOMO), a conditional diffusion framework that can generate full-body manipulation behaviors from only the object motion. Since naively applying diffusion models fails to precisely enforce contact constraints between the hands and the object, OMOMO learns two separate denoising processes to first predict hand positions from object motion and subsequently synthesize full-body poses based on the predicted hand positions. By employing the hand positions as an intermediate representation between the two denoising processes, we can explicitly enforce contact constraints, resulting in more physically plausible manipulation motions. With the learned model, we develop a novel system that captures full-body human manipulation motions by simply attaching a smartphone to the object being manipulated. Through extensive experiments, we demonstrate the effectiveness of our proposed pipeline and its ability to generalize to unseen objects. Additionally, as high-quality human-object interaction datasets are scarce, we collect a large-scale dataset consisting of 3D object geometry, object motion, and human motion. Our dataset contains human-object interaction motion for 15 objects, with a total duration of approximately 10 hours.},
journal = {ACM Trans. Graph.},
month = dec,
articleno = {197},
numpages = {11},
keywords = {animation, conditional diffusion model, contact, human-object interaction}
}

@article{10.1145/3351270,
author = {Xiao, Ning and Yang, Panlong and Li, Xiang-Yang and Zhang, Yanyong and Yan, Yubo and Zhou, Hao},
title = {MilliBack: Real-Time Plug-n-Play Millimeter Level Tracking Using Wireless Backscattering},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3351270},
doi = {10.1145/3351270},
abstract = {Real-time handwriting tracking is important for many emerging applications such as Artificial Intelligence assisted education and healthcare. Existing movement tracking systems, including those based on vision, ultrasound or wireless technologies, fail to offer high tracking accuracy, no learning/training/calibration process, low tracking latency, low cost and easy to deploy at the same time. In this work, we design and evaluate a wireless backscattering based handwriting tracking system, called MilliBack, that satisfies all these requirements. At the heart of MilliBack are two Phase Differential Iterative (PDI) schemes that can infer the position of the backscatter tag (which is attached to a writing tool) from the change in the signal phase. By adopting carefully-designed differential techniques in an iterative manner, we can take the diversity of devices out of the equation. The resulting position calculation has a linear complexity with the number of samples, ensuring fast and accurate tracking.We have put together a MilliBack prototype and conducted comprehensive experiments. We show that our system can track various handwriting traces accurately, in some testings it achieve a median error of 4.9 mm. We can accurately track and reconstruct arbitrary writing/drawing trajectories such as equations, Chinese characters or just random shapes. We also show that MilliBack can support relatively high writing speed and smoothly adapt to the changes of working environment.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {112},
numpages = {23},
keywords = {Wireless Tracking, Wireless Backscattering, Battery Free}
}

@inproceedings{10.1145/3539618.3591876,
author = {Qu, Xinghua and Liu, Hongyang and Sun, Zhu and Yin, Xiang and Ong, Yew Soon and Lu, Lu and Ma, Zejun},
title = {Towards Building Voice-based Conversational Recommender Systems: Datasets, Potential Solutions and Prospects},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591876},
doi = {10.1145/3539618.3591876},
abstract = {Conversational recommender systems (CRSs) have become crucial emerging research topics in the field of RSs, thanks to their natural advantages of explicitly acquiring user preferences via interactive conversations and revealing the reasons behind recommendations. However, the majority of current CRSs are text-based, which is less user-friendly and may pose challenges for certain users, such as those with visual impairments or limited writing and reading abilities. Therefore,for the first time, this paper investigates the potential of voice-based CRS (VCRSs) to revolutionize the way users interact with RSs in a natural, intuitive, convenient, and accessible fashion. To support such studies, we create two VCRSs benchmark datasets in the e-commerce and movie domains, after realizing the lack of such datasets through an exhaustive literature review. Specifically, we first empirically verify the benefits and necessity of creating such datasets. Thereafter, we convert the user-item interactions to text-based conversations through the ChatGPT-driven prompts for generating diverse and natural templates, and then synthesize the corresponding audios via the text-to-speech model. Meanwhile, a number of strategies are delicately designed to ensure the naturalness and high quality of voice conversations. On this basis, we further explore the potential solutions and point out possible directions to build end-to-end VCRSs by seamlessly extracting and integrating voice-based inputs, thus delivering performance-enhanced, self-explainable, and user-friendly VCRSs. Our study aims to establish the foundation and motivate further pioneering research in the emerging field of VCRSs. This aligns with the principles of explainable AI and AI for social good, viz., utilizing technology's potential to create a fair, sustainable, and just world. Our codes and datasets are available on GitHub (https://github.com/hyllll/VCRS ).},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2701–2711},
numpages = {11},
keywords = {ai for social good, conversational recommender systems, explainable ai, voice-based recommendation},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3700297.3700367,
author = {Wu, Weimei and Hu, Jianhua and Huang, Yingjie and Zeng, Wenying and Shao, Hui},
title = {Intelligent Teaching Platform Based on Large Models},
year = {2024},
isbn = {9798400707100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3700297.3700367},
doi = {10.1145/3700297.3700367},
abstract = {Currently, most educational platforms suffer from limited resources, delayed updates, lack of personalized recommendations, monotonous teaching modes, poor interactivity, inadequate adaptability, and incomplete assessment methods. Focused on the innovative needs of the education sector, this article develops an intelligent teaching platform based on large language models. The platform aims to provide a comprehensive and multi-level educational solution for students and teachers, offering not only diverse and up-to-date teaching resources but also genuinely personalized instruction to create a highly interactive teaching environment. With core features like AI-powered real-time tutoring assistants, digital human video creation for lectures, and big data analysis of learning conditions, the platform significantly enhances teaching quality and learning experiences. It can timely and accurately answer various student inquiries, offering precise real-time solutions and personalized advice to help students better plan and execute their learning strategies. Teachers can easily upload background images and lecture scripts to effortlessly generate digital human teaching videos. Moreover, by closely monitoring student learning data, teachers can adjust their teaching strategies appropriately and continuously optimize teaching outcomes. As the digital transformation in education progresses, this platform is poised to become a key force in driving educational innovation and improving teaching quality, effectively promoting the efficient use and rational allocation of educational resources.},
booktitle = {Proceedings of the 2024 International Symposium on Artificial Intelligence for Education},
pages = {409–415},
numpages = {7},
keywords = {AI real-time tutoring, Large language model, digital human teaching video, intelligent teaching platform, personalized learning},
location = {
},
series = {ISAIE '24}
}

@inproceedings{10.1145/3613904.3642040,
author = {Kamath, Purnima and Morreale, Fabio and Bagaskara, Priambudi Lintang and Wei, Yize and Nanayakkara, Suranga},
title = {Sound Designer-Generative AI Interactions: Towards Designing Creative Support Tools for Professional Sound Designers},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642040},
doi = {10.1145/3613904.3642040},
abstract = {The practice of sound design involves creating and manipulating environmental sounds for music, films, or games. Recently, an increasing number of studies have adopted generative AI to assist in sound design co-creation. Most of these studies focus on the needs of novices, and less on the pragmatic needs of sound design practitioners. In this paper, we aim to understand how generative AI models might support sound designers in their practice. We designed two interactive generative AI models as Creative Support Tools (CSTs) and invited nine professional sound design practitioners to apply the CSTs in their practice. We conducted semi-structured interviews and reflected on the challenges and opportunities of using generative AI in mixed-initiative interfaces for sound design. We provide insights into sound designers’ expectations of generative AI and highlight opportunities to situate generative AI-based tools within the design process. Finally, we discuss design considerations for human-AI interaction researchers working with audio.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {730},
numpages = {17},
keywords = {Audio, Creative Support Tools, Generative AI, Mixed-Initiative Creative Interfaces, Sound design},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3615522.3615553,
author = {Lin, Rem RunGu and Ke, Yongen and Zhang, Kang},
title = {Urban Symphony: An AI and Data-Driven Approach to Real-Time Animation for Public Digital Art},
year = {2023},
isbn = {9798400707513},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615522.3615553},
doi = {10.1145/3615522.3615553},
abstract = {Projection mapping is a form of urban public art that uses light and video to transform buildings and structures into dynamic canvases. However, producing high-quality projection mapping content with compelling storytelling requires extensive time and resources, as it involves integrating local culture, urban spatial understanding, and animation production. To address this challenge, this paper proposes a method that combines artistic co-creation with AI, audio-visualization, and data-visualization techniques. The authors present a case study: “Urban Symphony,” an immersive public art installation that showcases our method and leverages AI and data-driven storytelling. This method fosters interdisciplinary research collaboration and explores the potential of projection mapping as a bridge between art, technology, and society. The paper describe the motivation, design, and production of the artwork, the outcomes of the performance, and the challenges and limitations of our method. The authors also suggest future directions for further improvement and exploration in this domain.},
booktitle = {Proceedings of the 16th International Symposium on Visual Information Communication and Interaction},
articleno = {31},
numpages = {7},
keywords = {urban public digital art, projection mapping, data-visualization, co-creative AI, animation production, Audio-visualization},
location = {Guangzhou, China},
series = {VINCI '23}
}

@inproceedings{10.1145/3708359.3712152,
author = {Hardy, Amelia and Reuel, Anka and Jafari Meimandi, Kiana and Soder, Lisa and Griffith, Allie and Asmar, Dylan M and Koyejo, Sanmi and Bernstein, Michael S. and Kochenderfer, Mykel John},
title = {More than Marketing? On the Information Value of AI Benchmarks for Practitioners},
year = {2025},
isbn = {9798400713064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708359.3712152},
doi = {10.1145/3708359.3712152},
abstract = {Public AI benchmark results are widely broadcast by model developers as indicators of model quality within a growing and competitive market. However, these advertised scores do not necessarily reflect the traits of interest to those who will ultimately apply AI models. In this paper, we seek to understand if and how AI benchmarks are used to inform decision-making. Based on the analyses of interviews with 19 individuals who have used, or decided against using, benchmarks in their day-to-day work, we find that across these settings, participants use benchmarks as a signal of relative performance difference between models. However, whether this signal was considered a definitive sign of model superiority, sufficient for downstream decisions, varied. In academia, public benchmarks were generally viewed as suitable measures for capturing research progress. By contrast, in both product and policy, benchmarks – even those developed internally for specific tasks – were often found to be inadequate for informing substantive decisions. Of the benchmarks deemed unsatisfactory, respondents reported that their goals were neither well-defined nor reflective of real-world use. Based on the study results, we conclude that effective benchmarks should provide meaningful, real-world evaluations, incorporate domain expertise, and maintain transparency in scope and goals. They must capture diverse, task-relevant capabilities, be challenging enough to avoid quick saturation, and account for trade-offs in model performance rather than relying on a single score. Additionally, proprietary data collection and contamination prevention are critical for producing reliable and actionable results. By adhering to these criteria, benchmarks can move beyond mere marketing tricks into robust evaluative frameworks that accurately reflect AI progress and guide informed decision-making in both research and practical domains.},
booktitle = {Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {1032–1047},
numpages = {16},
keywords = {AI Benchmarks, User Study, Performance Metrics, Model Evaluation, Real-world Applicability, Qualitative Research},
location = {
},
series = {IUI '25}
}

@inproceedings{10.1145/3512749.3520237,
author = {Seymour, Mike and Li, Hao and Li, Zimo},
title = {The Champion : Neural Render Case Study},
year = {2024},
isbn = {9781450392440},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512749.3520237},
doi = {10.1145/3512749.3520237},
abstract = {The Champion of Auchwitz is the first full-length feature film that was neural rendered in order to allow the actors to be converted from performing in German to performing in English. In this Polish film, set in WW2, the entire production was filmed and finished in German. Using advances in AI and machine learning, the actors' faces have been replaced by inferred versions, visually built from English re-performances of the original dialogue.The film tells the story of pre-war boxing champion Tadeusz "Teddy" Pietrzykowski, who in 1940 arrives with the first transport of prisoners to the newly created Auschwitz concentration camp. The story was filmed without any consideration of later dialogue replacement.This production session discusses the important issues involved in doing professional neural rendering on a large scale - hundreds of shots, often with multiple characters in the same frame.While smaller projects have featured neural rendering in the past, these previous applications involved massive amounts of manual intervention per shot. Based on new technology, The Champion used only the footage already edited for the final german version of the film, combined with a robust and non-intrusive recording of the actors delivering the lines in English.The panel will discuss innovations in technology and provide insights into further advances that the team is working on. Including: • How the team found a general solution to actor re-capturing that was quick to set up and non-invasive to the actor's process. Our solution involved only 5 cameras and no tracking markers, per shot or per scene lighting, and no special camera calibration. • The workflow that was developed to remove training data ML sorting or categorizing. • Our professional pipeline allows for any film to be processed, with no requirement of access to special calibration clips, filming, or even outtakes from the main unit. The process relies solely on the finished film and the additional audio session recordings. • The film was adapted instead of dubbed with the input and cooperation of the Director, the actors, and the creative team. However, our pipeline could also be adapted to replacing dubbing on actors who are no longer available.Our objective was to produce a robust pipeline that could perform visual dubbing in a scalable fashion, more than a demo that would only work with extensive and time-consuming manual intervention on a few scenes.Our production solution needed to allow for • vastly varying camera angles, (not just the common face-swapping approach where the talent is facing the camera), • dramatic changes in lighting, contrast and camera artifacts, and • no access to lengthy or specially shot training data of the final scenes • actors with varying facial appearance over the film, in our case, a boxer who at times is severely bruised and beat up.With the extensive use of visuals and clips, the panel will discuss both the lessons learned and the advances in ML that allowed the wide-scale adoption of this technology in place of traditional dubbing or subtitles.},
booktitle = {ACM SIGGRAPH 2022 Production Sessions},
articleno = {7},
numpages = {2},
location = {Vancouver, BC, Canada},
series = {SIGGRAPH '22}
}

@inproceedings{10.1145/3544548.3581498,
author = {Sharma, Tanusree and Kaushik, Smirity and Yu, Yaman and Ahmed, Syed Ishtiaque and Wang, Yang},
title = {User Perceptions and Experiences of Targeted Ads on Social Media Platforms: Learning from Bangladesh and India},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581498},
doi = {10.1145/3544548.3581498},
abstract = {While people’s perceptions of targeted ads have been studied extensively from a Western perspective&nbsp;(e.g., North America, Europe), we know little about users’ perceptions in the South Asian region. We interviewed 40 participants from two South Asian countries, Bangladesh and India, to explore their perceptions and practices regarding targeted ads on social media platforms. Participants identified emerging ad types, such as influencer-based ads and soft ads, through articles. In addition, participants often outweighed discounts over product quality when viewing ads. We also observed novel user mental models of targeted ads based on mobile app permissions and excessive AI usage. Participants often preferred ad control over transparency. While most participants rarely used ad settings, some controlled ads by changing mobile app permissions or muting ads on social media platforms. Participants also raised concerns about fraudulent targeted ads and privacy violations due to device sharing. We present potential design ideas to mitigate these concerns.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {663},
numpages = {15},
keywords = {Privacy, South Asia, Targeted Advertisement},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3658852.3661317,
author = {Pataranutaporn, Pat and Mano, Phoomparin and Bhongse-Tong, Piyaporn and Chongchadklang, Tas and Archiwaranguprok, Chayapatr and Hantrakul, Lamtharn and Eaimsa-ard, Jirach and Maes, Pattie and Klunchun, Pichet},
title = {Human-AI Co-Dancing: Evolving Cultural Heritage through Collaborative Choreography with Generative Virtual Characters},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658852.3661317},
doi = {10.1145/3658852.3661317},
abstract = {This research introduces an approach for translating traditional dance knowledge into interactive computational models extending beyond static dance performance recordings. Specifically, this paper presents the concept of "Human-AI co-dancing," which involves integrating human dancers with virtual dance partners powered by models derived from dance principles. To demonstrate this concept, the research focuses on the choreographic principles deconstructed from the knowledge of traditional Thai dance. The principles are analyzed and translated into computational procedures that dynamically manipulate the movements of a virtual character by altering animation keyframes and the motions of individual joints in real-time. We developed an interactive system that enables dancers to improvise alongside the virtual agent. The system incorporates voice control functionality, allowing the dancer, choreographer, and even the audience to participate in altering the choreography of the virtual agents by adjusting parameters that represent traditional Thai dance elements. Human-AI rehearsals yielded intriguing artistic results, with hybrid movement aesthetics emerging from the synergy and friction between humans and machines. The resulting dance production, "Cyber Subin," demonstrates the potential of combining intangible cultural heritage, intelligent technology, and posthuman choreography to expand artistic expression and preserve traditional wisdom in a contemporary context.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {14},
numpages = {10},
keywords = {AI-generated Character, Computer-generated choreography, Cultural Computing, Human-AI Interaction, Virtual Agent},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.5555/3721488.3721663,
author = {Chojnowski, Oliver and Eberhard, Alexander and Schiffmann, Michael and M\"{u}ller, Ana and Richert, Anja},
title = {Human-like Nonverbal Behavior with MetaHumans in Real-World Interaction Studies: An Architecture Using Generative Methods and Motion Capture},
year = {2025},
publisher = {IEEE Press},
abstract = {Socially interactive agents are gaining prominence in domains like healthcare, education, and service contexts, particularly virtual agents due to their inherent scalability. To facilitate authentic interactions, these systems require verbal and nonverbal communication through e.g., facial expressions and gestures. While natural language processing technologies have rapidly advanced, incorporating human-like nonverbal behavior into real-world interaction contexts is crucial for enhancing the success of communication, yet this area remains underexplored. One barrier is creating autonomous systems with sophisticated conversational abilities that integrate human-like nonverbal behavior. This paper presents a distributed architecture using Epic Games' MetaHuman, combined with advanced conversational AI and camera-based user management, that supports methods like motion capture, handcrafted animation, and generative approaches for nonverbal behavior. We share insights into a system architecture designed to investigate nonverbal behavior in socially interactive agents, deployed in a three-week field study in the Deutsches Museum Bonn, showcasing its potential in realistic nonverbal behavior research.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1279–1283},
numpages = {5},
keywords = {generative ai, motion capture, nonverbal behavior metahuman, socially interactive agents},
location = {Melbourne, Australia},
series = {HRI '25}
}

@inproceedings{10.1145/3706598.3714242,
author = {Leong, Joanne and Ledo, David and Driscoll, Thomas and Grossman, Tovi and Fitzmaurice, George and Anderson, Fraser},
title = {Paratrouper: Exploratory Creation of Character Cast Visuals Using Generative AI},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714242},
doi = {10.1145/3706598.3714242},
abstract = {Great characters are critical to the success of many forms of media, such as comics, games, and films. Designing visually compelling casts of characters requires significant skill and consideration, and there is a lack of specialized tools to support this endeavor. We investigate how AI-driven image-generation techniques can empower creatives to explore a variety of visual design possibilities for individual and groups of characters. Informed by interviews with character designers, Paratrouper is a multi-modal system that enables creating and experimenting with multiple permutations for character casts and visualizing them in various contexts as part of a holistic approach to design. We demonstrate how Paratrouper supports different aspects of the character design process, and share insights from its use by eight creators. Our work highlights the interplay between creative agency and serendipity, as well as the visual interrelationships among character aesthetics.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {189},
numpages = {20},
keywords = {generative artificial intelligence, image generation, character design, character concept art, AI-assisted creativity},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3613904.3642868,
author = {Wang, Sitong and Menon, Samia and Long, Tao and Henderson, Keren and Li, Dingzeyu and Crowston, Kevin and Hansen, Mark and Nickerson, Jeffrey V and Chilton, Lydia B},
title = {ReelFramer: Human-AI Co-Creation for News-to-Video Translation},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642868},
doi = {10.1145/3613904.3642868},
abstract = {Short videos on social media are the dominant way young people consume content. News outlets aim to reach audiences through news reels—short videos conveying news—but struggle to translate traditional journalistic formats into short, entertaining videos. To translate news into social media reels, we support journalists in reframing the narrative. In literature, narrative framing is a high-level structure that shapes the overall presentation of a story. We identified three narrative framings for reels that adapt social media norms but preserve news value, each with a different balance of information and entertainment. We introduce ReelFramer, a human-AI co-creative system that helps journalists translate print articles into scripts and storyboards. ReelFramer supports exploring multiple narrative framings to find one appropriate to the story. AI suggests foundational narrative details, including characters, plot, setting, and key information. ReelFramer also supports visual framing; AI suggests character and visual detail designs before generating a full storyboard. Our studies show that narrative framing introduces the necessary diversity to translate various articles into reels, and establishing foundational details helps generate scripts that are more relevant and coherent. We also discuss the benefits of using narrative framing and foundational details in content retargeting.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {169},
numpages = {20},
keywords = {creativity support tools, generative AI, narratives, scriptwriting, short videos, storyboarding},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3665463.3680556,
author = {Perry, Jane},
title = {Deconstructing Performance: The Use of Actors in Games},
year = {2024},
isbn = {9798400706929},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665463.3680556},
doi = {10.1145/3665463.3680556},
abstract = {Performed storytelling is a human impulse, one we can trace to the beginning of recorded time, starting with theatre and moving more recently to celluloid, television and the streaming platforms we enjoy in our own homes. Video Games are a spritely addition to this entertainment landscape, and they come with added value: active involvement in play. Like storytelling, games are embedded in the human experience. We reach for structured activities that reward us with challenge, pleasure and fun. It was only a matter of time before storytelling and gameplay found each other. Twenty years after the arrival of the first computer game, the joy of play was augmented by the introduction of narrative. Engaging characters escorted the player from one plot-point to the next, adding value to their gaming experience. Built upon my own personal and professional experiences as both an actor and a voice artist for multiple high profile video games in the past decade, this keynote will explore how, with the arrival of video games, the landscape of storytelling has changed forever. How does the art of performance, steeped in nearly 2000 years of tradition, sit with the demands of this new upstart? What do actors and developers alike need to consider when the formally passive witness to performance, the audience, is now also the player? And what awaits in the future? As we head directly into an era full of technological possibilities and challenges for the creative sector such as game production, how will AI generated performances impact the players of games, as well as the creative input of those who make them?},
booktitle = {Companion Proceedings of the 2024 Annual Symposium on Computer-Human Interaction in Play},
pages = {1},
numpages = {1},
keywords = {AI-generated, game characters, games, performance in games, perofrming arts, sound, voice acting},
location = {Tampere, Finland},
series = {CHI PLAY Companion '24}
}

@inproceedings{10.1145/3507657.3528541,
author = {Mandal, Anuradha and Saxena, Nitesh},
title = {SoK: Your Mind Tells a Lot About You: On the Privacy Leakage via Brainwave Devices},
year = {2022},
isbn = {9781450392167},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3507657.3528541},
doi = {10.1145/3507657.3528541},
abstract = {Head-worn wearables, such as consumer-grade EEG headsets deployed in Brain Computer Interfaces (BCI), are getting popularity in the gaming and entertainment industry, and for people with certain disabilities. However, the increasing popularity of these wearables creates a significant privacy risk. For instance, tech companies are intending to use brainwave devices to detect workers' emotional state and mental condition. There are AI techniques that can learn what people are looking at in real-time. Silently conversing with the computing system is now possible using neuromuscular signals, for instance, untold digit recognition with higher accuracy is possible, which can retrieve untold PIN or password. These applications can reveal more private information than designated benign purpose, such as, while detecting performance of worker, sensitive information like Parkinson's disease, substance abuse disorder, heart disease, can be revealed from brainwave. The consequences of these privacy leakages may be potentially devastating, such as tracking users for targeted advertisements and launching targeted attacks against users. In this paper, we analyze current devices, explore previously studied attacks, research efforts to extract information from brainwave and analyze and synthesize potential future attacks from the current deployment. This systematization will provide right direction towards ensuring privacy risk of BCI devices, which is a pre-requisite to building future defense mechanisms against the attacks.},
booktitle = {Proceedings of the 15th ACM Conference on Security and Privacy in Wireless and Mobile Networks},
pages = {175–187},
numpages = {13},
keywords = {wireless or mobile security for cyber-physical systems, sok, side-channel attacks, brainwave security &amp; privacy},
location = {San Antonio, TX, USA},
series = {WiSec '22}
}

@inproceedings{10.1145/3706599.3721349,
author = {Lee, Chaeyeon and Lee, Chungnyeong and Kim, Sangyong and Choi, Yongsoon and Kim, Jusub},
title = {StorageChat Timeline: A Generative AI-Based Art Appreciation System for Enhancing Immersion and Exploratory Experience},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3721349},
doi = {10.1145/3706599.3721349},
abstract = {This video showcases StorageChat Timeline, an AI-powered system utilizing a Large Language Model (LLM) and generative AI technologies (e.g., style transfer, image-to-video) for art appreciation education. By offering real-time interactive question-and-answer experiences, the system enables users to construct the meaning of artworks based on their knowledge and experiences. It also provides immersive generative animations reflecting the artworks’ styles and multimodal features, including text-to-speech and dynamic visuals, to enhance emotional engagement. Through this design, the system aims to enhance immersion, learning motivation, and visual literacy, fostering active participation in art appreciation. This innovative approach enhances accessibility to art and proposes a generative AI-driven methodology for art education. The video demonstrates how the system’s key features—AI conversational interface, immersive animations, and multimodal integration—create an engaging and visually interactive experience, showcasing its potential to transform art appreciation.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {921},
numpages = {2},
keywords = {Generative AI, Large Language Models (LLM), Museum Education, Immersive Learning},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3688868.3689206,
author = {Laga, Hamid},
title = {Statistical 3D and 4D Shape Analysis: Theory and Applications in the Era of Generative AI},
year = {2024},
isbn = {9798400711954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3688868.3689206},
doi = {10.1145/3688868.3689206},
abstract = {The need for 3D and 4D (i.e., 3D + time) shape analysis arises in many branches of science ranging from anatomy, bioinformatics, medicine, and biology to computer graphics, multimedia, and virtual and augmented reality. In fact, shape is an essential property of natural and man-made 3D objects. It deforms over time as a result of many internal and external factors. For instance, anatomical organs such as bones, kidneys, and subcortical structures in the brain deform due to natural growth or disease progression; human faces deform as a consequence of talking, executing facial expressions, and aging. Similarly, human body actions and motions such as walking, jumping, and grasping are the result of the deformation, over time, of the human body shape. The ability to understand and model (1) the typical shape and deformation patterns of a class of 3D objects, and (2) the variability of these shapes and deformations within and across object classes has many applications. For example, in medical diagnosis and biological growth modeling, one is interested in measuring the intensity of pain from facial deformations, and in distinguishing between normal growth and disease progression using the shape of the body and its deformation over time. In computer vision and graphics, the ability to statistically model such spatiotemporal variability can be used to summarize collections of 3D objects and their animation, and simulate animations and motions. Similar to 3D morphable models, these tools can also be used in a generative model for synthesizing large corpora of labeled longitudinal 3D shape data, e.g., 4D faces, virtual humans, and various objects. In this talk, I will share the research undertaken by my group and collaborators in the area of statistical analysis and modelling of static (i.e., 3D) and dynamic (i.e., 4D) shapes. I will first highlight the importance of this topic for various applications ranging from biology and medicine to computer graphics and virtual/augmented reality. I will then structure my talk into three parts. The first one focuses on 3D shapes that bend, stretch, and change in topology. I will introduce our mathematical framework, termed Square Normal Fields (SRNF) [6, 10-12, 15], which provides (1) an efficient representation of 3D shapes, (2) an elastic metric for quantifying shape differences between objects, (3) mechanisms for computing correspondences and geodesics between such shapes, and (4) methods for characterizing populations of 3D shapes using generative models. I will consider both shapes that bend and stretch [6, 10-12, 15] but also those that change their structure and topology [18-22]. The second part of the talk will focus on 4D shapes, i.e., 3D shapes that move and deform as the result of normal growth or disease progression [9, 14]. I will summarize the latest solutions we developed for the statistical analysis of the spatio-temporal variability in such 4D shape data and highlight their applications in various fields. The third part of this talk will focus on the role statistical 3D and 4D shape models played and have to play in the era of Deep Learning and Generative AI. I will particularly highlight their importance and the role they played in advancing the field of 3D and 4D reconstruction and generation from images, videos, and text [1-5, 7, 8, 13, 16, 17]. I will conclude the talk by sharing insights into potential future developments in and applications of statistical 3D and 4D shape models.},
booktitle = {Proceedings of the 1st International Workshop on Multimedia Computing for Health and Medicine},
pages = {5–6},
numpages = {2},
keywords = {differential geometry, elastic shape analysis, generative ai, geometry, spatio-temporal shape analysis, topology, tree-shaped objects},
location = {Melbourne VIC, Australia},
series = {MCHM'24}
}

@article{10.1145/3549486,
author = {Evin, Inan and H\"{a}m\"{a}l\"{a}inen, Perttu and Guckelsberger, Christian},
title = {Cine-AI: Generating Video Game Cutscenes in the Style of Human Directors},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3549486},
doi = {10.1145/3549486},
abstract = {Cutscenes form an integral part of many video games, but their creation is costly, time-consuming, and requires skills that many game developers lack. While AI has been leveraged to semi-automate cutscene production, the results typically lack the internal consistency and uniformity in style that is characteristic of professional human directors. We overcome this shortcoming with Cine-AI, an open-source procedural cinematography toolset capable of generating in-game cutscenes in the style of eminent human directors. Implemented in the popular game engine Unity, Cine-AI features a novel timeline and storyboard interface for design-time manipulation, combined with runtime cinematography automation. Via two user studies, each employing quantitative and qualitative measures, we demonstrate that Cine-AI generates cutscenes that people correctly associate with a target director, while providing above-average usability. Our director imitation dataset is publicly available, and can be extended by users and film enthusiasts.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {223},
numpages = {23},
keywords = {video games, storyboard, procedural cinematography, imitation, cutscene}
}

@inproceedings{10.1145/3347122.3347127,
author = {Liu, Lucas and Long, Duri and Gujrania, Swar and Magerko, Brian},
title = {Learning Movement through Human-Computer Co-Creative Improvisation},
year = {2019},
isbn = {9781450376549},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3347122.3347127},
doi = {10.1145/3347122.3347127},
abstract = {Computers that are able to collaboratively improvise movement with humans could have an impact on a variety of application domains, ranging from improving procedural animation in game environments to fostering human-computer co-creativity. Enabling real-time movement improvisation requires equipping computers with strategies for learning and understanding movement. Most existing research focuses on gesture classification, which does not facilitate the learning of new gestures, thereby limiting the creative capacity of computers. In this paper, we explore how to develop a gesture clustering pipeline that facilitates reasoning about arbitrary novel movements in real-time. We describe the implementation of this pipeline within the context of LuminAI, a system in which humans can collaboratively improvise movements together with an AI agent. A preliminary evaluation indicates that our pipeline is capable of efficiently clustering similar gestures together, but further work is necessary to fully assess the pipeline's ability to meaningfully cluster complex movements.},
booktitle = {Proceedings of the 6th International Conference on Movement and Computing},
articleno = {5},
numpages = {8},
keywords = {pre-processing, movement, motion capture, machine learning, lifelong machine learning, dynamic programming, dimensionality reduction, dance, co-creative, clustering, Kinect},
location = {Tempe, AZ, USA},
series = {MOCO '19}
}

@article{10.1145/3635705,
author = {Menapace, Willi and Siarohin, Aliaksandr and Lathuili\`{e}re, St\'{e}phane and Achlioptas, Panos and Golyanik, Vladislav and Tulyakov, Sergey and Ricci, Elisa},
title = {Promptable Game Models: Text-guided Game Simulation via Masked Diffusion Models},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {2},
issn = {0730-0301},
url = {https://doi.org/10.1145/3635705},
doi = {10.1145/3635705},
abstract = {Neural video game simulators emerged as powerful tools to generate and edit videos. Their idea is to represent games as the evolution of an environment’s state driven by the actions of its agents. While such a paradigm enables users to play a game action-by-action, its rigidity precludes more semantic forms of control. To overcome this limitation, we augment game models with prompts specified as a set of natural language actions and desired states. The result—a Promptable Game Model (PGM)—makes it possible for a user to play the game by prompting it with high- and low-level action sequences. Most captivatingly, our PGM unlocks the director’s mode, where the game is played by specifying goals for the agents in the form of a prompt. This requires learning “game AI,” encapsulated by our animation model, to navigate the scene using high-level constraints, play against an adversary, and devise a strategy to win a point. To render the resulting state, we use a compositional NeRF representation encapsulated in our synthesis model. To foster future research, we present newly collected, annotated and calibrated Tennis and Minecraft datasets. Our method significantly outperforms existing neural video game simulators in terms of rendering quality and unlocks applications beyond the capabilities of the current state-of-the-art. Our framework, data, and models are available at snap-research.github.io/promptable-game-models.},
journal = {ACM Trans. Graph.},
month = jan,
articleno = {17},
numpages = {16},
keywords = {Neural radiance fields, diffusion models, human motion generation, language modeling}
}

@inproceedings{10.1145/3625468.3652176,
author = {Xu, Yutong and Du, Junhao and Wang, Jiahe and Ning, Yuwei and Zhou, Sihan and Cao, Yang},
title = {Panonut360: A Head and Eye Tracking Dataset for Panoramic Video},
year = {2024},
isbn = {9798400704123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625468.3652176},
doi = {10.1145/3625468.3652176},
abstract = {With the rapid development and widespread application of VR/AR technology, maximizing the quality of immersive panoramic video services that match users' personal preferences and habits has become a long-standing challenge. Understanding the saliency region where users focus, based on data collected with HMDs (Head-mounted Displays), can promote multimedia encoding, transmission, and quality assessment. At the same time, large-scale datasets are essential for researchers and developers to explore short/long-term user behavior patterns and train AI models related to panoramic videos. However, existing panoramic video datasets often include low-frequency user head or eye movement data through short-term videos only, lacking sufficient data for analyzing users' Field of View (FoV) and generating video saliency regions.Driven by these practical factors, in this paper, we present a head and eye tracking dataset involving 50 users (25 males and 25 females) watching 15 panoramic videos (mostly in 4K). The dataset provides details on the viewport and gaze attention locations of users. Besides, we present some statistics samples extracted from the dataset. For example, the deviation between head and eye movements challenges the widely held assumption that gaze attention decreases from the center of the FoV following a Gaussian distribution. Our analysis reveals a consistent downward offset in gaze fixations relative to the FoV in experimental settings involving multiple users and videos. That's why we name the dataset Panonut, a saliency weighting shaped like a donut. Finally, we also provide a script that generates saliency distributions based on given head or eye coordinates and pre-generated saliency distribution map sets of each video from the collected eye tracking data.The dataset and related code are publicly available on our website: https://dianvrlab.github.io/Panonut360/.},
booktitle = {Proceedings of the 15th ACM Multimedia Systems Conference},
pages = {319–325},
numpages = {7},
keywords = {Dataset, Panoramic videos, Saliency, User Behavior Analysis},
location = {Bari, Italy},
series = {MMSys '24}
}

@inproceedings{10.1145/3651671.3651711,
author = {Usami, Yoshiyuki and Moro, Shota},
title = {Integrated Artificial Intelligence for Making Digital Human II},
year = {2024},
isbn = {9798400709234},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3651671.3651711},
doi = {10.1145/3651671.3651711},
abstract = {This paper presents the second report on making an integrated AI. Regarding visual processing, YOLOv8 and image2-text are employed to recognize surrounding visional scenes, which output meaningful text to the AI’s text processing unit. In text processing, our previous work employed a question-answering system for questions from humans to the AI. However, in this study we use GPT2 for the convesation between humans and AI. This is because GPT2 has been found to be an excellent natural language processor that can provide appropriate responses to human questions and casual conversations. AI’s response is shown in the form of a 3D avatar animation by using the MakeItTalk routine. In this work we devised a system in which AI can autonomously learn by itself when it does not respond to humans. Namely, about eight news items are obtained from web news. From there, AI generates sentences using GPT2. The repeating process of text generation is accomplished by cutting out several leading words from sentences. As a result, AI’s attention theme is changing to a different topic. In addition, the AI creates a picture of the content by text2image routine. It makes AI’s association in a form that is easy to understand for humans. In this work, the system accomplishes fine-tuning so that these associations could be stored in the AI circuits as distributed weights. In such a way, we have created an integrated system of AI, that behaves like a human being. As described in the text, we may say that our system is superior to human beings in two points. The first is computation speed. We recognize that the association process of AI is far faster than that of humans. The second point is that AI avatars created using diffusion model may be more attractive than real humans. Of course, the present system has problems that need to be improved. However, we can surely say that those problems will be solved with the current technology. In creating an integrated AI, this study shows that it is possible to create an AI that is superior to humans in several ways even in 2024.},
booktitle = {Proceedings of the 2024 16th International Conference on Machine Learning and Computing},
pages = {677–683},
numpages = {7},
keywords = {image2text, question and answering, text generation, visual object detection, visual object recognition},
location = {Shenzhen, China},
series = {ICMLC '24}
}

@inproceedings{10.1145/3719330.3721230,
author = {Ren, Zebin and Doekemeijer, Krijn and De Matteis, Tiziano and Pinto, Christian and Stoica, Radu and Trivedi, Animesh},
title = {An I/O Characterizing Study of Offloading LLM Models and KV Caches to NVMe SSD},
year = {2025},
isbn = {9798400715297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719330.3721230},
doi = {10.1145/3719330.3721230},
abstract = {With the popularity of generative AI, LLM inference has become one of the most popular cloud workloads. Modern popular LLMs have hundreds of billions of parameters and support very large input/output prompt token sizes (100K-1M). As a result, their computational state during LLM inference can exceed the memory available on GPUs. One solution to this GPU memory problem is to offload the model weights and KV cache to the host memory. As the size of the models and prompts continue to increase, researchers have started to explore the use of secondary storage, such as SSDs, to store the model weights and KV cache. However, there is a lack of study on the I/O characteristics and performance requirements of these offloading operations. In order to have a better understanding of the performance characteristics of these offloading operations, in this work, we collect, study, and characterize the block layer I/O traces from two LLM inference frameworks, DeepSpeed and FlexGen, that support model and KV cache offloading to SSDs. Through our analysis of these I/O traces, we report that: (i) libaio-based tensor offloading delivers higher I/O bandwidth for both writing and reading tensors to/from the SSDs than POSIX; (ii) the I/O workload of model offloading is dominated by 128 KiB reads for both DeepSpeed and FlexGen in the block layer; (iii) model offloading does not saturate NVMe SSDs; and (iv) the I/O workload of KV cache offloading contains both read and write workloads dominated by 128 KiB requests, but the average bandwidth of read is much higher than write (2.0 GiB/s vs. 11.0 MiB/s). We open-source the scripts and the I/O traces of this work at https://github.com/stonet-research/cheops25-IO-characterization-of-LLM-model-kv-cache-offloading-nvme},
booktitle = {Proceedings of the 5th Workshop on Challenges and Opportunities of Efficient and Performant Storage Systems},
pages = {23–33},
numpages = {11},
keywords = {KV cache offloading, Large language model, Model offloading, SSDs},
location = {Rotterdam, Netherlands},
series = {CHEOPS '25}
}

@article{10.5555/3722479.3722506,
author = {Liao, Weidong and Guzide, Osman},
title = {Enhancing Undergraduate Computing Education with LMMs and ChatGPT-4o},
year = {2024},
issue_date = {October 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {3},
issn = {1937-4771},
abstract = {Large Language Models (LLMs) and ChatGPT have significantly impacted programming practices and computer science education. The rapid advancements in natural language processing, recurrent neural networks, and Transformer architectures have captured the attention of students and educators alike. These tools aid students in brainstorming, coding, analyzing code, and writing reports. Although concerns about cheating and plagiarism persist, these tools also provide educators with novel ways to create and assess assignments. Despite some hesitancy among educators to integrate these AI tools into the classroom, the advert and development of Large MultiModal Models (LMMs), the enhancement of LLMs that can deal with multimedia inputs and outputs, illustrates a significant evolution in generative AI capabilities.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {62},
numpages = {1}
}

@inproceedings{10.1145/3613904.3642125,
author = {Liu, Zhihao and Li, Yu and Tu, Fangyuan and Zhang, Ruiyuan and Cheng, Zhanglin and Yokoya, Naoto},
title = {DeepTreeSketch: Neural Graph Prediction for Faithful 3D Tree Modeling from Sketches},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642125},
doi = {10.1145/3613904.3642125},
abstract = {We present DeepTreeSketch, a novel AI-assisted sketching system that enables users to create realistic 3D tree models from 2D freehand sketches. Our system leverages a tree graph prediction network, TGP-Net, to learn the underlying structural patterns of trees from a large collection of 3D tree models. The TGP-Net simulates the iterative growth of botanical trees and progressively constructs the 3D tree structures in a bottom-up manner. Furthermore, our system supports a flexible sketching mode for both precise and coarse control of the tree shapes by drawing branch strokes and foliage strokes, respectively. Combined with a procedural generation strategy, users can freely control the foliage propagation with diverse and fine details. We demonstrate the expressiveness, efficiency, and usability of our system through various experiments and user studies. Our system offers a practical tool for 3D tree creation, especially for natural scenes in games, movies, and landscape applications.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {471},
numpages = {19},
keywords = {3D modeling interface, ideation, neural networks, sketching system},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1109/ASE56229.2023.00023,
author = {Xu, Bowen and Nguyen, Thanh-Dat and Le-Cong, Thanh and Hoang, Thong and Liu, Jiakun and Kim, Kisub and Gong, Chen and Niu, Changan and Wang, Chenyu and Le, Bach and Lo, David},
title = {Are We Ready to Embrace Generative AI for Software Q&amp;A?},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00023},
doi = {10.1109/ASE56229.2023.00023},
abstract = {Stack Overflow, the world's largest software Q&amp;A (SQA) website, is facing a significant traffic drop due to the emergence of generative AI techniques. ChatGPT is banned by Stack Overflow after only 6 days from its release. The main reason provided by the official Stack Overflow is that the answers generated by ChatGPT are of low quality. To verify this, we conduct a comparative evaluation of human-written and ChatGPT-generated answers. Our methodology employs both automatic comparison and a manual study. Our results suggest that human-written and ChatGPT-generated answers are semantically similar, however, human-written answers outperform ChatGPT-generated ones consistently across multiple aspects, specifically by 10% on the overall score. We release the data, analysis scripts, and detailed results at https://github.com/maxxbw54/GAI4SQA.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1713–1717},
numpages = {5},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3322905.3322928,
author = {Milo, Thomas and Mart\'{\i}nez, Alicia Gonz\'{a}lez},
title = {A New Strategy for Arabic OCR: Archigraphemes, Letter Blocks, Script Grammar, and shape synthesis},
year = {2019},
isbn = {9781450371940},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322905.3322928},
doi = {10.1145/3322905.3322928},
abstract = {Current OCR has limited capability for Arabic because of script models lacking scientific basis. We propose a new OCR strategy for Arabic, based on 1. Islamic script grammar including extended shaping and 2. treating Arabic script as a multi-layered writing system.We analyse Arabic script as an allographic rendering of graphemic abstractions. Grapheme is a term adapted from phonology; it is analogous to the term phoneme. In phonology, the smallest functional unit of sound is the phoneme. This is not heard, but perceived. What one hears are contextually conditioned allophones. In Arabic orthography, the smallest functional unit of spelling is the grapheme. This is not seen, but perceived. What one sees are contextually conditioned allographs.In our analysis, the letter block is the minimum unit of Arabic script formation and therefore of script grammar. A letter block is a single allograph or of a group of fused allographs surrounded by graphic space. The analogy with phonology can be pushed further: the archiphoneme is a bundle of shared features between two or more phonemes, minus their distinctive features. The archigrapheme is the bundle of shared features between two or more graphemes, minus their distinctive features. An archigraphemic letter block consists of one or more reduced allographs between spaces. The letter block follows the base line. There can be ligatures between letter blocks. In our strategy the archigraphemic letter block also forms the minimum unit of OCR. We have(1) implemented an algorithm that reduces any Unicode text in Arabic script to archigraphemes and we used it to create a list in Unicode format of all attested unique archigraphemic letter blocks on the internet.(2) With this list, and applying extended Islamic script grammar, we can synthesize realistic images of all possible archigraphemic fusions in a given style.These two developments make it possible to create an OCR system for recognizing synthetic Arabic under controlled conditions for both basic and extended shaping in a given style. These two steps result in competence, after which the OCR system should be trained to apply tolerance for the variation of performance in real documents. To interpret the identified letter blocks linguistically, a technique for the parsing of archigraphemes must be developed. For example, the single sequence of the three archigraphemic letter blocks EBD A LLH can be interpreted as several different surface texts such as abda-n li llaahi, abdu l-laahi and inda l-laahi. To facilitate the linguistic phase of the process, the same list of unique archigraphemic letter blocks is designed to identify the language of the text under scrutiny. In this phase we can present• Islamic script synthesis• Unicode conversion from plene orthography to archigraphemic transliteration• the archigraphemic search algorithm• the list of unique archigraphemic letter blocks• samples of authentic shape generationThese are the first steps towards static OCR technology. The next step is to create or find matching AI software to teach OCR to recognize any unmapped letter blocks in order to make the OCR dynamic.},
booktitle = {Proceedings of the 3rd International Conference on Digital Access to Textual Cultural Heritage},
pages = {93–96},
numpages = {4},
keywords = {OCR, Arabic},
location = {Brussels, Belgium},
series = {DATeCH2019}
}

@inproceedings{10.14236/ewic/BCSHCI2024.1,
author = {Puthukulangara, Sujisha and Bond, Raymond R. and Mulvenna, Maurice D. and McTear, Michael F.},
title = {Intelligent user experience tool to help evaluate and quality assure handcrafted chatbot dialogue designs},
year = {2025},
publisher = {BCS Learning &amp; Development Ltd},
address = {Swindon, GBR},
url = {https://doi.org/10.14236/ewic/BCSHCI2024.1},
doi = {10.14236/ewic/BCSHCI2024.1},
abstract = {Introduction: Chatbots have revolutionized customer support, offering round-the-clock assistance. This research addresses the need for improving chatbot dialogues, considering the challenges faced in the quality assurance of chatbot content (utterances). Many chatbots may use pre-scripted utterances (conversational dialogue designs) that have been handcrafted by domain experts. For example, a healthcare chatbot may comprise of utterances that have been pre-scripted (pre-designed) by healthcare professionals. This deterministic approach avoids providing an AI-generated response that could include potentially harmful advice. This paper presents an intelligent tool that can be used to assess the quality of handcrafted dialogue designs before they are included into the knowledge base of a chatbot. Methods and results: An interactive tool was developed to allow a user to import a handcrafted dialogue dataset and to then define compliance rules (e.g. utterances should have a certain sentiment score, readability score, utterance length and perhaps emoji use). These rules are then used to discover utterances that violate these rules which allow conversational designers to review and perhaps edit these utterances to provide a better user experience whilst quality assuring the dialogue design. Our tool effectively analyzes and visualizes dialogue characteristics, allowing users to identify flagged issues. Users can edit flagged utterances for improvement, promoting better dialogue quality. Conclusion: This research contributes to conversational design and chatbot development, offering an innovative tool to enhance the quality of the chatbot ensuring consistent and engaging customer experiences.},
booktitle = {Proceedings of the 37th International BCS Human-Computer Interaction Conference},
pages = {1–6},
numpages = {6},
keywords = {Chatbot development, UX, User experience, Conversational design, Natural language processing, Sentiment analysis, Complexity assessment, Emoji validation, Utterance length},
location = {University of Central Lancashire (UCLan)},
series = {BCS HCI '24}
}

@inproceedings{10.1145/3510450.3517308,
author = {Prins, Martin},
title = {Improving content discovery and viewer engagement with AI},
year = {2022},
isbn = {9781450392228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510450.3517308},
doi = {10.1145/3510450.3517308},
abstract = {We are quickly heading towards the golden age of video. For consumers there is a wealth of video services to choose from, ranging from broad video offerings to content tailored to a specific audience or genre. Many services come with premium content and competitive pricing. This however brings a dilemma: people will only subscribe to a few services, and are more likely to replace a service for another, in case the service they use does not meet their needs. Churn rates of video services are already increasing, and with new services being launched in 2022, the fight for viewer attention will further intensify.The battle for eyeballs will not be won by the services that provide the most content, but by the ones that offer the most engaging experience and help consumers quickly find the most relevant content, and who are able to do that in a cost effective manner.In this presentation the author will discuss three applications where AI can help viewer engagement and ensure viewers can find, discover and play the content they are interested in as quickly as possible:• Automatic content chaptering for news, talk shows and sports programs, such that viewers can quickly jump to the items they are interested in. This allows video services to make use of long-form content but address the viewer needs of short form playback. The concept has been popularized by YouTube which offers a tool to assist content creators to chapter their content, but still requires manual curation [2]. We explore what can be achieved in a fully automated way.• Identifying meaningful topics in non-scripted live content, such that viewers can discover and follow content based on their topics of interest, and video service providers. These programs typically lack traditional metadata about the contents, due to the live nature and the costs involved to create metadata by hand.• Generating high quality, appealing and personalized episode-specific thumbnails out of live (broadcast) video, to help the viewer pick the right content quicker. This allows video services to replace meaningless stock images and engage viewers better with (near)-live programming. Personalized imagery for Video on Demand has earlier been popularized by Netflix [1]. But generating images from broadcast video has additional challenges that need to be resolved to be able to produce appealing images.The cases are from real implementations that are being tested with different video services and their viewers. As such the talk also dives into the practical challenges, how these were resolved and present results and takeaways from both technical and user perspectives.},
booktitle = {Proceedings of the 1st Mile-High Video Conference},
pages = {132},
numpages = {1},
keywords = {user experience, content discovery, content analysis, artificial intelligence},
location = {Denver, Colorado},
series = {MHV '22}
}

@inproceedings{10.1145/3593013.3594133,
author = {Andri\'{c}, Katja and Kasirzadeh, Atoosa},
title = {Reconciling Governmental Use of Online Targeting With Democracy},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593013.3594133},
doi = {10.1145/3593013.3594133},
abstract = {The societal and epistemological implications of online targeted advertising have been scrutinized by AI ethicists, legal scholars, and policymakers alike. However, the government’s use of online targeting and its consequential socio-political ramifications remain under-explored from a critical socio-technical standpoint. This paper investigates the socio-political implications of governmental online targeting, using a case study of the UK government’s application of such techniques for public policy objectives. We argue that this practice undermines democratic ideals, as it engenders three primary concerns — Transparency, Privacy, and Equality — that clash with fundamental democratic doctrines and values. To address these concerns, the paper introduces a preliminary blueprint for an AI governance framework that harmonizes governmental use of online targeting with certain democratic principles. Furthermore, we advocate for the creation of an independent, non-governmental regulatory body responsible for overseeing the process and monitoring the government’s use of online targeting, a critical measure for preserving democratic values.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1871–1881},
numpages = {11},
keywords = {AI governance, democracy, democracy and AI, governmental online targeting, public policy, targeted advertising},
location = {Chicago, IL, USA},
series = {FAccT '23}
}

@inproceedings{10.1145/3505284.3532976,
author = {van der Boon, Matthijs and Fermoselle, Leonor and ter Haar, Frank and Dijkstra-Soudarissanane, Sylvie and Niamut, Omar},
title = {Deep Learning Augmented Realistic Avatars for Social VR Human Representation},
year = {2022},
isbn = {9781450392129},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3505284.3532976},
doi = {10.1145/3505284.3532976},
abstract = {Virtual reality (VR) has created a new and rich medium for people to meet each other digitally. In VR, people can choose from a broad range of representations. In several cases, it is important to provide users with avatars that are a lifelike representation of themselves, to increase the user experience and effectiveness of communication. In this work, we propose a pipeline for generating a realistic and expressive avatar from a single reference image. The pipeline consists of a blendshape-based avatar combined with two deep learning improvements. The first improvement module runs offline and improves the texture map of the base avatar. The second module runs inference in real-time at the rendering stage and performs a style transfer to the avatar’s eyes. The deep learning modules effectively improve the visual representation of the avatar and show how AI techniques can be integrated with traditional animation methods to generate realistic human avatars for social VR.},
booktitle = {Proceedings of the 2022 ACM International Conference on Interactive Media Experiences},
pages = {311–318},
numpages = {8},
keywords = {virtual reality, real-time deep learning, human avatars, generative adversarial networks},
location = {Aveiro, JB, Portugal},
series = {IMX '22}
}

@inproceedings{10.1145/3332167.3356878,
author = {Subramonyam, Hariharan},
title = {Designing Interactive Intelligent Systems for Human Learning, Creativity, and Sensemaking},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356878},
doi = {10.1145/3332167.3356878},
abstract = {The focus of intelligent systems is on "making things easy'' through automation. However, for many cognitive tasks---such as learning, creativity, or sensemaking---there is such a thing as too easy or too automated. Current human-AI design principles, as well as general usability guidelines, prioritize automation, and efficient task execution over human effort. However, this type of advice may not be suitable for designing systems that need to balance automation with other cognitive goals. In these cases, designers lack the necessary tools that will allow them to consider the trade-offs between automation, AI assistance, and human-effort. My dissertation looks at using models from cognitive psychology to inform the design of intelligent systems. The first system, Florum, looks at automation after human-effort as a strategy to facilitate learning from science text. The second system, TakeToons, explores automation as a complementary strategy to human-effort to support creative animation tasks. A third set, SmartCues and Affinity Lens use AI as a last-mile optimization strategy for human sensemaking tasks. Based on these systems, I am looking to develop a design framework that (1) classifies threats across different levels of design including automation, user interface, expectations from AI, and cognition and (2) offers ways to validate design decisions.},
booktitle = {Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {158–161},
numpages = {4},
keywords = {interaction design, human-ai, cognitive models},
location = {New Orleans, LA, USA},
series = {UIST '19 Adjunct}
}

@inproceedings{10.1145/3591196.3593361,
author = {Baharin, Hanif and Abd Manan, Mohd Shahrudin and Abdul Shukor, Shureen Faris and Harun, Afdallyna Fathiyah and Baharuddin, Nasir and Ibrahim, Nazrita and Nohuddin, Puteri Nor Ellyza and Bin Mastro, Muhammad Hafiz and Jing, Heng Tsui and Ahmad Saffian, Khatriza},
title = {ChoreoGraphiComfort: Human Thermal Comfort in Vernacular Architectural Spaces},
year = {2023},
isbn = {9798400701801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591196.3593361},
doi = {10.1145/3591196.3593361},
abstract = {‘ChoreoGraphiComfort’ is inspired by a critical reflection on the organicity and corporeality that (e)merge in the thermal space of vernacular architecture. Abstracting the creative notions of choreography, graphics, and comfort, the artwork uses an ethnographic survey of living conditions in low-cost flats in Malaysia during the recent Covid-19 pandemic and an architectural representation of a 300-year-old traditional Malay house called Istana Puteri Bongsu. Centering the use on the issue of thermal comfort, a series of thermal images of the human body in various choreographed postures were filmed against the background of a traditional Malay house. The images were then fed into an AI platform to generate a new set of images that were morphed into each other. This creates a surrealist abstraction of what we term ‘thermal choreography’ that complicates the polemics of organic architecture and cultural wisdom in adapting human thermal comfort to the natural environment.},
booktitle = {Proceedings of the 15th Conference on Creativity and Cognition},
pages = {211–216},
numpages = {6},
location = {Virtual Event, USA},
series = {C&amp;C '23}
}

@proceedings{10.1145/3394486,
title = {KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are very excited to welcome you to the twenty-sixth meeting of KDD - arguably the most unique KDD experience in history. The world changed drastically due to the spread of COVID-19 disease at pandemic proportions. As of this writing, all gatherings are ruled out, social distancing and masking are now the mandated norms. Yet, our community has been drawn closer than ever, especially in response to the ongoing public health emergency to detect and mitigate the spreading disease. As to the signature meeting event itself, the organizing community and KDD steering committee took on the challenge and worked together so that KDD 2020 would be a brand new experience of a virtual conference with innovations that enable the community to socialize, make acquaintance and strike conversations among the presenters and attendees. Not surprisingly, among the program highlights are work by the KDD community on combating COVID-19 showcased through a number of sessions consisting of themed workshops, health day, a special call for papers and a research panel on data science for preventative measures, contact tracing, treatment recommendation, and so on. Sprinkled throughout the program, there are many papers that address Data Science for best social distancing, for epidemiological modeling of the spread and for optimized operation of healthcare facilities. The program includes keynotes by Alessandor Vespignani who leads national effort on modeling the spatial spread of epidemics, and Emery Brown, a statistician, a neuroscientist and an anesthesiologist whose work has been recognized by election to all three branches of the National Academies of Sciences. Among our compelling keynote speakers are Yolanda Gil, researcher and president of the Association for the Advancement of Artificial Intelligence (AAAI); and Manuela Veloso, head of AI Research at JP Morgan and former head of Machine Learning at CMU and past president of AAAI. Likewise, our applied data science track features a compelling montage of twenty invited speakers to provoke thoughtful dialogue and to keep you engaged in the program.The pandemic crisis of the last five months has also found most of us locked in our homes and all movement cancelled. Amidst this stillness, we have been confronted with -- individually and as a society -- many moments of reflection on the issues that matter most. Deepest among these has been a realization that some amongst our fellow citizens face a different daily reality, of a deep racial divide that denies basic tenets of equal protection with heart-wrenching examples of their brutal treatment at the hands of the very law enforcement employed to protect them. As the realization of a society starkly divided in its basic guarantees of life and liberty to its black American citizens dawns, we can go beyond rejecting racism to bring meaningful change. We believe data science can serve as a power tool for transparency and support for systemic change. It can expose inequality and injustice while providing data-driven solutions for bringing lasting change. SIG-KDD as an organization brings together a community of researchers who can focus precisely on these important problems. KDD, as its main event, is the primary means by which KDD reaches out to this community of researchers, creates the context by curating a technical program that brings important technical and sociological challenges to the attention of a broader audience of researchers and practitioners.This year's KDD continues the trend of record high number of submissions, the highest we have seen ever in our applied data science and research tracks. A total of 2035 valid submissions were made, which is the highest number in KDD history (over 13% more than the second highest one): 1279 in the research track and 756 in applied data science track. After a careful review process, 338 papers (217 in research track and 121 in applied data science track) were accepted for publication in the proceedings. The selectivity and attention to detail by the program committees is reflected in the program where sessions range from deeply theoretical topics in statistical analysis, machine learning to practical applications in health, finance etc. As the sessions from refereed papers emerged, the chairs have carefully curated the overall program through keynotes and invited speakers. The first two days of the conference include tutorials by experts in their field and workshops and theme days, followed by three rigorous yet enjoyable days of research papers, applied data science papers and over 21 applied invited speakers sharing views from the trenches of industrial applications.Diversity and inclusion is a major theme in this year's KDD. For the first time we organized a dedicated full day event for diversity and inclusion. The day includes exciting invited talks by researchers in underrepresented groups or on diversity and inclusion research, as well as mentoring sessions for students raising awareness of diversity and inclusion and coaching career development. The final program composition itself reflects the diversity of talent in the KDD community: three out of four keynote speakers are in underrepresented groups, ten out of twenty-one ADS track invited speakers are female, close to fifty-percent of our organizing committee are female. We also host a women research panel, social impact program, all reflecting tremendous volunteer activities in support of the diversity and inclusion theme this year.},
location = {Virtual Event, CA, USA}
}

@inproceedings{10.1145/3680529.3688961,
author = {Hu, Rui and Chen, Susan Yingshu},
title = {Matrix Model and Uberbau},
year = {2024},
isbn = {9798400711329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3680529.3688961},
doi = {10.1145/3680529.3688961},
abstract = {"Matrix Model and Uberbau" is a real-time simulation reflecting on the synchronicity between the evolution of computer graphics and AI technology and the turbulence of financial markets. The year 2008 stands as a stark mark, when financial and housing markets globally were ravaged by a devastating crisis following a period of deceptive prosperity. Amidst this economic turmoil, Nvidia unveiled CUDA, making general-purpose computing on the Graphics Processing Unit (GPU) streamlined and accessible. Traditionally used to produce images rooted in military and entertainment, GPU has since become the essential apparatus for AI and infiltrated numerous domains including scientific computing and finance, due to its prowess in parallel computing and matrix operations. The artwork reflects on this history, with various scripted events driven by "predictions" of the 2007--08 Shanghai Stock Exchange Index using a deep learning neural network trained on the first-gen consumer GPU with CUDA support. In a space half sports stadium and half construction site, men and women in business suits running on the sports field are knocked over repeatedly by offscaled office items, spotlighted by GPU-drones equipped with searchlights. This scene of competition and violence is observed by automatic camera cuts and accompanied by the audio of Hong Kong comedian Wong Tze-wah's gigs about financial and housing markets and an AI-cloned Zizek discussing graphics technology and ideology. The work asks: could the widespread adoption of AI, powered by computer graphics technology, provide us with the foresight into disasters previously unforeseen, or will it only further bolster the self-perpetuating hollow nature of finance capitalism? This question cascades into multiple subtopics---basic needs versus luxury; the notion of "model" in graphics, sciences, and economics; the emotional aspects of the markets; the uncertainty of life; and the paradox that the only perfect model might be the one that embraces its inherent inaccuracy.},
booktitle = {SIGGRAPH Asia 2024 Art Gallery},
articleno = {8},
numpages = {1},
location = {Tokyo, Japan},
series = {SA Art Gallery '24}
}

@inproceedings{10.1145/3366423.3382669,
author = {Ma, Wei-Ying},
title = {Democratizing Content Creation and Dissemination through AI Technology},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3382669},
doi = {10.1145/3366423.3382669},
abstract = {With the rise of mobile video, user-generated content, and social networks, there is a massive opportunity for disruptive innovations in the media and content industry. It is now a fast-changing landscape with rapid advances in AI-powered content creation, dissemination and interaction technologies. I believe the current trends are leading us towards a world where everyone is equally empowered to produce high-quality content in video, music, augmented reality or more – and to share their information, knowledge, and stories with a large global audience. This new AI- powered content platform can further lead to innovations in advertising, e-commerce, online education, and productivity. I will share the current research efforts at ByteDance connected to this emerging new platform through products such as Douyin and TikTok, and discuss the challenges and the direction of our future research.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3143},
numpages = {1},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3531146.3533218,
author = {Schoeffer, Jakob and Kuehl, Niklas and Machowski, Yvette},
title = {“There Is Not Enough Information”: On the Effects of Explanations on Perceptions of Informational Fairness and Trustworthiness in Automated Decision-Making},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533218},
doi = {10.1145/3531146.3533218},
abstract = {Automated decision systems (ADS) are increasingly used for consequential decision-making. These systems often rely on sophisticated yet opaque machine learning models, which do not allow for understanding how a given decision was arrived at. In this work, we conduct a human subject study to assess people’s perceptions of informational fairness (i.e., whether people think they are given adequate information on and explanation of the process and its outcomes) and trustworthiness of an underlying ADS when provided with varying types of information about the system. More specifically, we instantiate an ADS in the area of automated loan approval and generate different explanations that are commonly used in the literature. We randomize the amount of information that study participants get to see by providing certain groups of people with the same explanations as others plus additional explanations. From our quantitative analyses, we observe that different amounts of information as well as people’s (self-assessed) AI literacy significantly influence the perceived informational fairness, which, in turn, positively relates to perceived trustworthiness of the ADS. A comprehensive analysis of qualitative feedback sheds light on people’s desiderata for explanations, among which are (i) consistency (both with people’s expectations and across different explanations), (ii) disclosure of monotonic relationships between features and outcome, and (iii) actionability of recommendations.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1616–1628},
numpages = {13},
keywords = {Automated decision-making, explanations, informational fairness, machine learning, perceptions, trustworthiness},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@proceedings{10.1145/3637528,
title = {KDD '24: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to welcome you to the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining---KDD'2024 in Barcelona, Catalonia. This year's conference continues its tradition of being the premier forum for presentation of research results and experience reports on knowledge discovery, data science, and machine learning. The mission of the conference is to provide the best venue for advancement, education, and adoption of the "science" of knowledge discovery and machine learning from all types of data; to share novel methodologies that fulfill the needs of heterogeneous applications and environments and identify new directions for future research and development. These ideas have the potential to shape and impact our society and environment, becoming particularly important with the emergence of AI in all fields. So, KDD provides researchers and practitioners a unique opportunity to share their perspectives with others interested in various aspects of data science and machine learning.KDD'24 has a program of three keynotes (Sanjeev Arora, Tanya Berger-Wolf and Xihong Lin), one panel on generative AI, 411 research track papers, 151 applied data science (ADS) track papers and eight invited talks, 30 workshops, 34 tutorials (nine of them hands-on), nine special days (one online for India), and three KDD cups. We have introduced two new special days, one in Responsible AI and another in European Data Science given the location of the conference. We also added one extra poster session to have more time for posters presentations. For second time we used Openreview for the research and ADS tracks with the goal to further improve the review quality and facilitate the interaction between reviewers and authors. We hope that you will find this program interesting and thought-provoking and that the conference will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.},
location = {Barcelona, Spain}
}

@inproceedings{10.1145/3537972.3538009,
author = {Cuykendall, Shannon and DiPaola, Steve},
title = {Floating Departures: Developing Quarantine Dance Technique as an Artistic Practice Beyond the Pandemic},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3538009},
doi = {10.1145/3537972.3538009},
abstract = {We describe our process of quarantine dance technique in making the dance film and meditation, Floating Departures. This work, created during lockdown in 2021, brings together dance movement, poetry, painterly styles, and sound to explore cyclical patterns and points of departure in movement and life. To create Floating Departures we used a broad range of technologies–from everyday objects to smartphones to AI art systems. We experiment with various techniques to record ourselves and bring our movement together in a shared digital space with post-production video editing techniques. Using a bricolage approach, we incorporate materials, such as bubble wrap and balloons, to transform our spaces and explore our personal experiences during lockdown. We construct multiple layers of reality that are further transformed in unanticipated directions with AI technologies. Through our creation process, we develop a collective physical body to explore a new realm, unbound by reason or logic, that was only made possible through our remote collaborative processes and technologically-mediated interactions.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {29},
numpages = {7},
keywords = {Screendance, Remote Creation, Dance Film, Choreography, Bricolage, AI Art Systems},
location = {Chicago, IL, USA},
series = {MOCO '22}
}

@inproceedings{10.1145/3589334.3645511,
author = {D\"{u}tting, Paul and Mirrokni, Vahab and Paes Leme, Renato and Xu, Haifeng and Zuo, Song},
title = {Mechanism Design for Large Language Models},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589334.3645511},
doi = {10.1145/3589334.3645511},
abstract = {We investigate auction mechanisms to support the emerging format of AI-generated content. We in particular study how to aggregate several LLMs in an incentive compatible manner. In this problem, the preferences of each agent over stochastically generated contents are described/encoded as an LLM. A key motivation is to design an auction format for AI-generated ad creatives to combine inputs from different advertisers. We argue that this problem, while generally falling under the umbrella of mechanism design, has several unique features. We propose a general formalism---the token auction model---for studying this problem. A key feature of this model is that it acts on a token-by-token basis and lets LLM agents influence generated contents through single dimensional bids.We first explore a robust auction design approach, in which all we assume is that agent preferences entail partial orders over outcome distributions. We formulate two natural incentive properties, and show that these are equivalent to a monotonicity condition on distribution aggregation. We also show that for such aggregation functions, it is possible to design a second-price auction, despite the absence of bidder valuation functions. We then move to designing concrete aggregation functions by focusing on specific valuation forms based on KL-divergence, a commonly used loss function in LLM. The welfare-maximizing aggregation rules turn out to be the weighted (log-space) convex combination of the target distributions from all participants. We conclude with experimental results in support of the token auction formulation.},
booktitle = {Proceedings of the ACM Web Conference 2024},
pages = {144–155},
numpages = {12},
keywords = {large language models, mechanism design, online advertising},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3329715.3338883,
author = {Cutler, Larry and Darnell, Eric and Dirksen, Nathaniel and Hutchinson, Michael and Peterson, Scott and Schiewe, Robert and Wang, Wei},
title = {Making you matter: creating interactive VR narratives through experimentation and learning},
year = {2019},
isbn = {9781450367998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3329715.3338883},
doi = {10.1145/3329715.3338883},
abstract = {Virtual Reality (VR) is a transformative medium for narrative storytelling where content creators can place an audience member inside the story, give them a role to play, and ultimately make them matter to the characters. Immersive storytelling is fundamentally different from film and games. It requires a new creative toolset that is still in its infancy compared to other entertainment mediums.We provide a behind the scenes look at our many experimentations, failures, and learnings in developing interactive VR animated narratives spanning four released projects: Invasion!, Asteroids!, Crow: The Legend, and Bonfire. We delve into cinematic techniques for VR, including staging, movement mechanics, and directing the viewer's eye. We explore the role the viewer plays in each of our pieces. Finally, we dive into how we make you matter through nonverbal communication, interactivity that supports the narrative structure, non-linear storytelling, and character AI.},
booktitle = {Proceedings of the 2019 Digital Production Symposium},
articleno = {5},
numpages = {9},
keywords = {virtual reality, user interaction, storytelling, interactivity, computer animation},
location = {Los Angeles, California},
series = {DigiPro '19}
}

@inproceedings{10.1145/3404835.3462905,
author = {Li, Yuanchun and Riva, Oriana},
title = {Glider: A Reinforcement Learning Approach to Extract UI Scripts from Websites},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462905},
doi = {10.1145/3404835.3462905},
abstract = {Web automation scripts (tasklets) are used by personal AI assistants to carry out human tasks such as reserving a car or buying movie tickets. Generating tasklets today is a tedious job which requires much manual effort. We propose Glider, an automated and scalable approach to generate tasklets from a natural language task query and a website URL. A major advantage of Glider is that it does not require any pre-training. Glider models tasklet extraction as a state space search, where agents can explore a website's UI and get rewarded when making progress towards task completion. The reward is computed based on the agent's navigating pattern and the similarity between its trajectory and the task query. A hierarchical reinforcement learning policy is used to efficiently find the action sequences that maximize the reward. To evaluate Glider, we used it to extract tasklets for tasks in various categories (shopping, real-estate, flights, etc.); in 79% of cases a correct tasklet was generated.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1420–1430},
numpages = {11},
keywords = {reinforcement learning, task completion, ui script, web interfaces},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3366423.3382668,
author = {Nigel Shadbolt, Sir},
title = {Architectures for Autonomy: Towards an Equitable Web of Data in the Age of AI},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3382668},
doi = {10.1145/3366423.3382668},
abstract = {Today, the Web connects over half the world's population, many of whom use it to stay connected to a multiplicity of vital digital public and private services, impacting every aspect of their lives. Access to the Web and underlying Internet is seen as essential for all—even a fundamental human right [7]. However, many contend that the power structure on large swaths of the Web has become inverted; they argue that instead of being run for and by users, it has been made to serve the platforms themselves, and the powerful actors that sponsor such platforms to run targeted advertising on their behalf. In such an ad-driven platform ecosystem, users, including their beliefs, data, and attention, have become traded commodities [13].There is concern that the emergence of powerful data analytics and AI techniques threaten to further entrench the power of these same platforms, by putting the control of powerful and valuable new capabilities in their hands rather than the users who produce the data [10]. The fear is that it is giving rise to data and AI monopolies [2,6]. Individuals have no long-term control or agency over their personal data or many of the decisions made using it.This may be one reason we are witnessing a so called Renaissance of Ethics - a plethora of initiatives and activities that call out the range of threats to individual autonomy, self-determination and privacy, the lack of transparency and accountability, a concern around bias and fairness, equity and access in our data driven ecosystem. This keynote will argue as the remaining half of the world's population comes online, we need digital infrastructures that will promote a plurality of methods of data sovereignty and governance instead of imposing a ’single policy fits-all’ platform governance model, which has strained and undermined the ability for governments to protect and support their citizens digital rights.This is an opportunity to re-imagine and re-architect elements of the Web, data, algorithms and institutions so as to ensure a more equitable distribution of these new digital potentialities. Based on our existing research we have been developing methods and tech-nologies pertaining to the following core principles: informational self-determination and autonomy, balanced and equitable access to AI and data, accountability and redress of AI/algorithmic decisions, and new models of ethical participation and contribution.The technology that underpins the modern web has seen exponential rates of change that have continuously improved the capabilities of the processors, memory and communications upon which it depends. This has enabled huge amounts of data to be linked and stored as well as providing for increasing use of AI. A variety of projects will be described where we sought to unlock the potential of this increasingly powerful infrastructure [1, 4, 5, 9]. The lessons learnt through various efforts to develop the Seman-tic Web [8] and the insights gained through the release of open data at scale will be reviewed [11]. We will review our attempts to understand how the blending of humans, algorithms and data at scale results in social machines whose emergent properties results in behaviour and problem solving which any of the individual elements would not have been able to achieve [12]. Understanding these emergent properties of the web was one of the motivating factors behind the establishment of Web Science [3]. We will briefly review the prospects for Web Science.The importance of data as infrastructure to enable wide spread innovation, accountability and trusted reproducible science will be stressed. Recent work will be described that seeks to promote an equitable and balanced Web environment in which privacy can be upheld and better mutualities realised. Developments in technical and institutional architectures that could underpin an Ethical Web of data will be outlined.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {3141–3142},
numpages = {2},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3637528.3671476,
author = {Bagherjeiran, Abraham and Djuric, Nemanja and Lee, Kuang-Chih and Pang, Linsey and Radosavljevic, Vladan and Rajan, Suju},
title = {AdKDD 2024},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671476},
doi = {10.1145/3637528.3671476},
abstract = {The digital advertising field has always had challenging ML problems, learning from petabytes of data that is highly imbalanced, reactivity times in the milliseconds, and more recently compounded with the complex user's path to purchase across devices, across platforms, and even online/real-world behavior. The AdKDD workshop continues to be a forum for researchers in advertising, during and after KDD. Our website which hosts slides and abstracts receives approximately 2,000 monthly visits and 1,800 active users during the KDD 2021. In surveys during AdKDD 2019 and 2020, over 60% agreed that AdKDD is the reason they attended KDD, and over 90% indicated they would attend next year. The 2024 edition is particularly timely because of the increasing application of Graph-based NN and Generative AI models in advertising. Coupled with privacy-preserving initiatives enforced by GDPR, CCPA the future of computational advertising is at an interesting crossroads. For this edition, we plan to solicit papers that span the spectrum of deep user understanding while remaining privacy-preserving. In addition, we will seek papers that discuss fairness in the context of advertising, to what extent does hyper-personalization work, and whether the ad industry as a whole needs to think through more effective business models such as incrementality. We have hosted several academic and industry luminaries as keynote speakers and have found our invited speaker series hosting expert practitioners to be an audience favorite. We will continue fielding a diverse set of keynote speakers and invited talks for this edition as well. As with past editions, we hope to motivate researchers in this space to think not only about the ML aspects but also to spark conversations about the societal impact of online advertising.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {6706–6707},
numpages = {2},
keywords = {ad targeting, computational advertising, user modeling},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3610537.3622963,
author = {Rieger, Uwe and Liu, Yinan and Kaluarachchi, Tharindu and Barde, Amit and Bai, Huidong and Nassani, Alaeddin and Nanayakkara, Suranga and Billinghurst, Mark},
title = {LightSense - Long Distance},
year = {2023},
isbn = {9798400703089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610537.3622963},
doi = {10.1145/3610537.3622963},
abstract = { 'LightSense - Long Distance' explores remote interaction with architectural space. It is a virtual extension of the project 'LightSense,' which is currently presented at the exhibition 'Cyber Physical: Architecture in Real Time' at EPFL Pavilions in Switzerland. Using numerous VR headsets, the setup at the Art Gallery at SIGGRAPH Asia establishes a direct connection between both exhibition sites in Sydney and Lausanne.'LightSense' at EPFL Pavilions is an immersive installation that allows the audience to engage in intimate interaction with a living architectural body. It consists of a 12-meter-long construction that combines a lightweight structure with projected 3D holographic animations. At its core sits a neural network, which has been trained on sixty thousand poems. This allows the structure to engage, lead, and sustain conversations with the visitor. Its responses are truly associative, unpredictable, meaningful, magical, and deeply emotional. Analysing the emotional tenor of the conversation, 'LightSense' can transform into a series of hybrid architectural volumes, immersing the visitors in Pavilions of Love, Anger, Curiosity, and Joy.'LightSense's' physical construction is linked to a digital twin. Movement, holographic animations, sound, and text responses are controlled by the cloud-based AI system. This combination creates a location-independent cyber-physical system. As such, the 'Long Distance' version, which premiered at SIGGRAPH Asia, enables the visitors in Sydney to directly engage with the physical setup in Lausanne. Using VR headsets with a new 360-degree 4K live streaming system, the visitors find themselves teleported to face 'LightSense', able to engage in a direct conversation with the structure on-site.'LightSense - Long Distance' leaves behind the notion of architecture being a place-bound and static environment. Instead, it points toward the next generation of responsive buildings that transcend space, are capable of dynamic behaviour, and able to accompany their visitors as creative partners.},
booktitle = {SIGGRAPH Asia 2023 Art Gallery},
articleno = {13},
numpages = {2},
location = {Sydney, NSW, Australia},
series = {SA '23}
}

@inproceedings{10.1145/3461702.3462580,
author = {Watkins, Elizabeth Anne and Moss, Emanuel and Metcalf, Jacob and Singh, Ranjit and Elish, Madeleine Clare},
title = {Governing Algorithmic Systems with Impact Assessments: Six Observations},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462580},
doi = {10.1145/3461702.3462580},
abstract = {Algorithmic decision-making and decision-support systems (ADS) are gaining influence over how society distributes resources, administers justice, and provides access to opportunities. Yet collectively we do not adequately study how these systems affect people or document the actual or potential harms resulting from their integration with important social functions. This is a significant challenge for computational justice efforts of measuring and governing AI systems. Impact assessments are often used as instruments to create accountability relationships and grant some measure of agency and voice to communities affected by projects with environmental, financial, and human rights ramifications. Applying these tools-through Algorithmic Impact Assessments (AIA)-is a plausible way to establish accountability relationships for ADSs. At the same time, what an AIA would entail remains under-specified; they raise as many questions as they answer. Choices about the methods, scope, and purpose of AIAs structure the conditions of possibility for AI governance. In this paper, we present our research on the history of impact assessments across diverse domains, through a sociotechnical lens, to present six observations on how they co-constitute accountability. Decisions about what type of effects count as an impact; when impacts are assessed; whose interests are considered; who is invited to participate; who conducts the assessment; how assessments are made publicly available, and what the outputs of the assessment might be; all shape the forms of accountability that AIAs engender. Because AlAs are still an incipient governance strategy, approaching them as social constructions that do not require a single or universal approach offers a chance to produce interventions that emerge from careful deliberation.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {1010–1022},
numpages = {13},
keywords = {impact, harm, governance, algorithmic impact assessment, accountability},
location = {Virtual Event, USA},
series = {AIES '21}
}

@inproceedings{10.1145/3450507.3457439,
author = {Rice, Tivon},
title = {Models for environmental literacy},
year = {2021},
isbn = {9781450383608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450507.3457439},
doi = {10.1145/3450507.3457439},
abstract = {Models for Environmental Literacy is a series of experimental digital animations combining drone photogrammetry with A.I. generated narratives. These videos creatively and critically explore the challenges of describing a landscape, an ecosystem, or the specter of environmental collapse through human language. The project further explores how language and vision are impacted by the mediating agency of new technologies. How do we see, feel, imagine, and talk about the environment in this post-digital era, when there are indeed non-human/machine agents similarly trained to perceive "natural" spaces? This project explores these questions, as well as emerging relationships with environmental surveillance, drone/computer vision, and A.I.},
booktitle = {ACM SIGGRAPH 2021 Art Gallery},
articleno = {8},
numpages = {3},
location = {Virtual Event, USA},
series = {SIGGRAPH '21}
}

@inproceedings{10.1145/3442381.3449848,
author = {Ghazimatin, Azin and Pramanik, Soumajit and Saha Roy, Rishiraj and Weikum, Gerhard},
title = {ELIXIR: Learning from User Feedback on Explanations to&nbsp;Improve&nbsp;Recommender Models},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449848},
doi = {10.1145/3442381.3449848},
abstract = {System-provided explanations for recommendations are an important component towards transparent and trustworthy AI. In state-of-the-art research, this is a one-way signal, though, to improve user acceptance. In this paper, we turn the role of explanations around and investigate how they can contribute to enhancing the quality of generated recommendations themselves. We devise a human-in-the-loop framework, called Elixir, where user feedback on explanations is leveraged for pairwise learning of user preferences. Elixir leverages feedback on pairs of recommendations and explanations to learn user-specific latent preference vectors, overcoming sparseness by label propagation with item-similarity-based neighborhoods. Our framework is instantiated using generalized graph recommendation via Random Walk with Restart. Insightful experiments with a real user study show significant improvements in movie and book recommendations over item-level feedback.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3850–3860},
numpages = {11},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3649165.3703622,
author = {Alshaigy, Bedour and Grande, Virginia and Kiesler, Natalie and Settle, Amber},
title = {How Do You Solve A Problem Like Recruitment? On The Hiring and Retention of Computing Academics},
year = {2024},
isbn = {9798400705984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649165.3703622},
doi = {10.1145/3649165.3703622},
abstract = {This paper critically examines persistent inequities in existing computing faculty hiring and retention practices, which gravely impact computing educators from marginalized groups. Throughout these processes, applicants fight against multiple systemic barriers, including but not limited to, biased job ads and discriminatory interview practices. The increasing use of generative AI tools to aid in tasks connected to the hiring process, such as writing recommendation letters, exacerbates these biases. The inequities persist despite global initiatives and legal mandates and serve as a direct contradiction to widespread institutional commitments to diversity and inclusion. By building on literature and the lived experiences of the SIGCSE community represented in a recent Technical Symposium session, we raise concerns about the different stages of this process, highlighting the importance of clear expectations and adequate support. The paper concludes with a call to align hiring practices with inclusive institutional values, requiring the academic community to reflect on and revise hiring policies for a more equitable future. It is of paramount importance to address the role of these practices in the erosion of marginalized communities from the computing education community, a marginalization that occurs in many different contexts and negatively impacts everyone involved.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1},
pages = {263–266},
numpages = {4},
keywords = {CS academics, recruitment, retention},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@inproceedings{10.1145/3476123.3487879,
author = {Marynowsky, Wade},
title = {From our deceased bodies flowers will grow, we are in them and that is eternity},
year = {2022},
isbn = {9781450386869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3476123.3487879},
doi = {10.1145/3476123.3487879},
abstract = {Wade Marynowsky, 'From our deceased bodies, flowers will grow we are in them and that is eternity', high definition video, sound, 3mins, 2021.Artbreeder (Simon 2020) is a collaborative tool for discovering new images and generating animations. Images are 'bred' by having 'children', or by mixing an images 'genes' with other images. The lineage of the hybrid image may be traced through a collaborative family tree, thus breeding and sharing can be used as methods for exploring highly complex spaces. The main technologies enabling the creative tool are Generative Adversarial Networks (GANs), (Goodfellow, 2014). In particular BigGAN which was trained on ImageNet, a large visual database of more than 14 million images, designed for use in visual object recognition software research.Marynowsky has applied experimental methods in an attempt to generate unexpected outcomes from the image generating network. In the work, "From our deceased bodies flowers will grow, we are in them and that is eternity", the artist has purposely input photography of local flowers into the Portrait AI engine, forcing it to find faces where there are none. Some of the local flower images include: Acacia trees; Nasturtium; Camellia, Magnolia; Lavender; Callistemon, etc. The images are not bred any further and exist as the first translations of the Portrait AI algorithm. The most interesting images are then selected to become part of the animated morph sequence. The sequence flows from machine abstraction to the representation of faces, confusing the threshold of anthropomorphism, human's innate tendency to see faces in the world around us.},
booktitle = {SIGGRAPH Asia 2021 Art Gallery},
articleno = {5},
numpages = {1},
location = {Tokyo, Japan},
series = {SA '21}
}

@inproceedings{10.1145/3596947.3596959,
author = {Tiwari, Aniruddha and Dave, Rushit and Vanamala, Mounika},
title = {Leveraging Deep Learning Approaches for Deepfake Detection: A Review},
year = {2023},
isbn = {9781450399920},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3596947.3596959},
doi = {10.1145/3596947.3596959},
abstract = {Abstract— Conspicuous progression in the field of machine learning (ML) and deep learning (DL) have led the jump of highly realistic fake media, these media oftentimes referred as deepfakes. Deepfakes are fabricated media which are generated by sophisticated AI that are at times very difficult to set apart from the real media. So far, this media can be uploaded to the various social media platforms, hence advertising it to the world got easy, calling for an efficacious countermeasure. Thus, one of the optimistic counter steps against deepfake would be deepfake detection. To undertake this threat, researchers in the past have created models to detect deepfakes based on ML/DL techniques like Convolutional Neural Networks (CNN). This paper aims to explore different methodologies with an intention to achieve a cost-effective model with a higher accuracy with different types of the datasets, which is to address the generalizability of the dataset. },
booktitle = {Proceedings of the 2023 7th International Conference on Intelligent Systems, Metaheuristics &amp; Swarm Intelligence},
pages = {12–19},
numpages = {8},
keywords = {convolutional neural network, Machine Learning, Long Short-Term Memory, Fake Image Detection, Deepfake, Deep Neural Network, Deep Learning},
location = {Virtual Event, Malaysia},
series = {ISMSI '23}
}

@inproceedings{10.1145/3411109.3411138,
author = {Laurent, Christie and Overholt, Dan},
title = {ReVoice: a control interface for 1d transmission-line analog vowel synthesis},
year = {2020},
isbn = {9781450375634},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411109.3411138},
doi = {10.1145/3411109.3411138},
abstract = {As an integrated part of the human experience, the voice has naturally raised interest amongst audio computing researchers. This has especially been true since the development of modern imaging techniques and increased computational capacities. Voice synthesis is now an active area of investigation, as much for AI design, as for the film industry or pure research purposes. Although various digital models of the vocal tract exist, few authors have proposed a simple physical device allowing a tangible approach to the process of vocalic synthesis. The work presented here is a control interface that gives the users the ability to interact with a voice synthesis algorithm in a tactile manner. ReVoice is an artificial and manipulable vocal tract, whose shape, controlled by the user's hands, determines the nature of the vowel that is synthesised. More specifically, this can help the user to understand how changes in the vocal tract impact the nature of the vowel. This paper elucidates all designs and specifications that characterise the device and interaction: hardware, software, mapping and use.},
booktitle = {Proceedings of the 15th International Audio Mostly Conference},
pages = {152–159},
numpages = {8},
keywords = {vowels, vocalic synthesis, vocal tract, source-filter, sound shaping, sound processing, sonic interaction design, prototyping, physical modelling},
location = {Graz, Austria},
series = {AM '20}
}

@article{10.1145/3680471,
author = {Russo, Daniel},
title = {Navigating the Complexity of Generative AI Adoption in Software Engineering—RCR Report},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3680471},
doi = {10.1145/3680471},
abstract = {This Replicated Computational Results (RCR) report complements the study “Navigating the Complexity of Generative AI Adoption in Software Engineering,” which examines the factors influencing the integration of AI tools in software engineering practices. Employing a mixed-methods approach grounded in the Technology Acceptance Model, Diffusion of Innovation Theory, and Social Cognitive Theory, the study introduces the Human-AI Collaboration and Adaptation Framework (HACAF), validated through PLS-SEM analysis. The replication package detailed herein includes survey instruments, raw data, and analysis scripts essential for reproducing the study's findings. By providing these artifacts, the RCR report aims to support transparency, enable replication, and encourage further research on effective AI tool adoption strategies in software engineering.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {221},
numpages = {5},
keywords = {Generative AI, Large Language Models, Technology Adaption, Empirical Software Engineering}
}

@inproceedings{10.1145/3580305.3599582,
author = {Bagherjeiran, Abraham and Djuric, Nemanja and Lee, Kuang-Chih and Pang, Linsey and Radosavljevic, Vladan and Rajan, Suju},
title = {AdKDD 2023},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599582},
doi = {10.1145/3580305.3599582},
abstract = {The digital advertising field has always had challenging ML problems, learning from petabytes of data that is highly imbalanced, reactivity times in the milliseconds, and more recently compounded with the complex user's path to purchase across devices, across platforms, and even online/real-world behavior. The AdKDD workshop continues to be a forum for researchers in advertising, during and after KDD. Our website which hosts slides and abstracts receives approximately 2,000 monthly visits and 1,800 active users during the KDD 2021. In surveys during AdKDD 2019 and 2020, over 60% agreed that AdKDD is the reason they attended KDD, and over 90% indicated they would attend next year. The 2023 edition is particularly timely because of the increasing application of Graph-based NN and Generative AI models in advertising. Coupled with privacy-preserving initiatives enforced by GDPR, CCPA the future of computational advertising is at an interesting crossroads. For this edition, we plan to solicit papers that span the spectrum of deep user understanding while remaining privacy-preserving. In addition, we will seek papers that discuss fairness in the context of advertising, to what extent does hyper-personalization work, and whether the ad industry as a whole needs to think through more effective business models such as incrementality. We have hosted several academic and industry luminaries as keynote speakers and have found our invited speaker series hosting expert practitioners to be an audience favorite. We will continue fielding a diverse set of keynote speakers and invited talks for this edition as well. As with past editions, we hope to motivate researchers in this space to think not only about the ML aspects but also to spark conversations about the societal impact of online advertising.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5849–5850},
numpages = {2},
keywords = {user modeling, computational advertising, ad targeting},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3555776.3577831,
author = {Narayan, Abhay and Galve, Disha and Chacko, Anu},
title = {AI Enabled Cloud Service to detect Conversion Fraud in E-commerce},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577831},
doi = {10.1145/3555776.3577831},
abstract = {Digital advertising has become very crucial in E-commerce. Artificial display requests or clicks created by malicious codes, bot-nets, and click-firms, pose a significant threat to the advertising ecosystem. Conversion fraud done by attackers directly influences Ad rates, campaign planning, and many other critical components of the Advertising ecosystem as they create fake impressions. Hence conversions need to be monitored and transparent. This research work analyzes different ML Classifiers to detect conversion fraud from log data as an AI Enabled Cloud Service. It was observed that ensemble techniques and Random Forest gives the best results.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {45–48},
numpages = {4},
keywords = {machine learning, conversion fraud, online advertising, cloud services, cloud computing},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3657604.3664719,
author = {Schmucker, Robin and Xia, Meng and Azaria, Amos and Mitchell, Tom},
title = {Ruffle&amp;Riley: From Lesson Text to Conversational Tutoring},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3664719},
doi = {10.1145/3657604.3664719},
abstract = {Conversational tutoring systems (CTSs) offer learning experiences driven by natural language interactions. They are recognized for promoting cognitive engagement and improving learning outcomes, especially in reasoning tasks. Ruffle&amp;Riley is a novel type of CTS that explores the potential of LLMs for efficient AI-assisted content authoring and for facilitating structured free-form conversational tutoring. This interactive event enables participants to engage with the LLM-based CTS introduced in our recent AIED2024 paper in two ways: (1) Attendees will interact with the web application using their personal devices. (2) Attendees will learn how to import learning materials into the system and generate custom tutoring scripts through a detailed tutorial. Ruffle&amp;Riley is an extendable, open-source framework that promotes research on effective instructional design of LLM-based learning technologies. The interactive event will foster related discussions.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {547–549},
numpages = {3},
keywords = {authoring tools, conversational tutoring systems, intelligent tutoring systems, large language models},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@inproceedings{10.1145/3642921.3642930,
author = {Henriques, Miguel and Bispo, Jo\~{a}o and Paulino, Nuno},
title = {Using Source-to-Source to Target RISC-V Custom Extensions: UVE Case-Study},
year = {2024},
isbn = {9798400717918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3642921.3642930},
doi = {10.1145/3642921.3642930},
abstract = {Hardware specialization is seen as a promising venue for improving computing efficiency, with reconfigurable devices as excellent deployment platforms for application-specific architectures. One approach to hardware specialization is via the popular RISC-V, where Instruction Set Architecture (ISA) extensions for domains such as Edge Artifical Intelligence (AI) are already appearing. However, to use the custom instructions while maintaining a high (e.g., C/C++) abstraction level, the assembler and compiler must be modified. Alternatively, inline assembly can be manually introduced by a software developer with expert knowledge of the hardware modifications in the RISC-V core. In this paper, we consider a RISC-V core with a vectorization and streaming engine to support the Unlimited Vector Extension (UVE), and propose an approach to automatically transform annotated C loops into UVE compatible code, via automatic insertion of inline assembly. We rely on a source-to-source transformation tool, Clava, to perform sophisticated code analysis and transformations via scripts. We use pragmas to identify code sections amenable for vectorization and/or streaming, and use Clava to automatically insert inline UVE instructions, avoiding extensive modifications of existing compiler projects. We produce UVE binaries which are functionally correct, when compared to handwritten versions with inline assembly, and achieve equal and sometimes improved number of executed instructions, for a set of six benchmarks from the Polybench suite. These initial results are evidence towards that this kind of translation is feasible, and we consider that it is possible in future work to target more complex transformations or other ISA extensions, accelerating the adoption of hardware/software co-design flows for generic application cases.},
booktitle = {Proceedings of the 16th Workshop on Rapid Simulation and Performance Evaluation for Design},
pages = {42–50},
numpages = {9},
keywords = {Hardware/Software co-design, Instruction Set Extension, Source-to-Source Compilation},
location = {Munich, Germany},
series = {RAPIDO '24}
}

@inproceedings{10.1145/3649921.3659853,
author = {Moradi Karkaj, Arash and Nelson, Mark J. and Koutis, Ioannis and Hoover, Amy K.},
title = {Prompt Wrangling: On Replication and Generalization in Large Language Models for PCG Levels},
year = {2024},
isbn = {9798400709555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649921.3659853},
doi = {10.1145/3649921.3659853},
abstract = {The ChatGPT4PCG competition calls for participants to submit inputs to ChatGPT or prompts that guide its output toward instructions to generate levels as sequences of Tetris-like block drops. Prompts submitted to the competition are queried by ChatGPT to generate levels that resemble letters of the English alphabet. Levels are evaluated based on their similarity to the target letter and physical stability in the game engine. This provides a quantitative evaluation setting for prompt-based procedural content generation (PCG), an approach that has been gaining popularity in PCG, as in other areas of generative AI. This paper focuses on replicating and generalizing the competition results. The replication experiments in the paper first aim to test whether the number of responses gathered from ChatGPT is sufficient to account for the stochasticity requery the original prompt submissions to rerun the original scripts from the competition on different machines about six months after the competition organizers. We re-run the competition, using the original scripts, but on our own machines, several months later, and with varying sample sizes. We find that results largely replicate, except that two of the 15 submissions do much better in our replication, for reasons we can only partly determine. When it comes to generalization, we notice that the top-performing prompt has instructions for all 26 target levels hardcoded, which is at odds with the PCGML goal of generating new, previously unseen content from examples. We perform experiments in a more restricted few-shot prompting scenario, and find that generalization remains a challenge for current approaches.},
booktitle = {Proceedings of the 19th International Conference on the Foundations of Digital Games},
articleno = {76},
numpages = {8},
keywords = {Evaluating Generalization, Generalizability, Large Language Models (LLMs), Procedural content generation (PCG), Science Birds},
location = {Worcester, MA, USA},
series = {FDG '24}
}

@inproceedings{10.1145/3637528.3671526,
author = {Guo, Jiayan and Huo, Yusen and Zhang, Zhilin and Wang, Tianyu and Yu, Chuan and Xu, Jian and Zheng, Bo and Zhang, Yan},
title = {Generative Auto-bidding via Conditional Diffusion Modeling},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671526},
doi = {10.1145/3637528.3671526},
abstract = {Auto-bidding plays a crucial role in facilitating online advertising by automatically providing bids for advertisers. Reinforcement learning (RL) has gained popularity for auto-bidding. However, most current RL auto-bidding methods are modeled through the Markovian Decision Process (MDP), which assumes the Markovian state transition. This assumption restricts the ability to perform in long horizon scenarios and makes the model unstable when dealing with highly random online advertising environments. To tackle this issue, this paper introduces AI-Generated Bidding (AIGB), a novel paradigm for auto-bidding through generative modeling. In this paradigm, we propose DiffBid, a conditional diffusion modeling approach for bid generation. DiffBid directly models the correlation between the return and the entire trajectory, effectively avoiding error propagation across time steps in long horizons. Additionally, DiffBid offers a versatile approach for generating trajectories that maximize given targets while adhering to specific constraints. Extensive experiments conducted on the real-world dataset and online A/B test on Alibaba advertising platform demonstrate the effectiveness of DiffBid, achieving 2.81% increase in GMV and 3.36% increase in ROI.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5038–5049},
numpages = {12},
keywords = {automated bidding, bid optimization, e-commerce advertising, generative learning},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3613904.3642738,
author = {Haque, MD Romael and Saxena, Devansh and Weathington, Katy and Chudzik, Joseph and Guha, Shion},
title = {Are We Asking the Right Questions?: Designing for Community Stakeholders’ Interactions with AI in Policing},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642738},
doi = {10.1145/3613904.3642738},
abstract = {Research into recidivism risk prediction in the criminal justice system has garnered significant attention from HCI, critical algorithm studies, and the emerging field of human-AI decision-making. This study focuses on algorithmic crime mapping, a prevalent yet underexplored form of algorithmic decision support (ADS) in this context. We conducted experiments and follow-up interviews with 60 participants, including community members, technical experts, and law enforcement agents (LEAs), to explore how lived experiences, technical knowledge, and domain expertise shape interactions with the ADS, impacting human-AI decision-making. Surprisingly, we found that domain experts (LEAs) often exhibited anchoring bias, readily accepting and engaging with the first crime map presented to them. Conversely, community members and technical experts were more inclined to engage with the tool, adjust controls, and generate different maps. Our findings highlight that all three stakeholders were able to provide critical feedback regarding AI design and use - community members questioned the core motivation of the tool, technical experts drew attention to the elastic nature of data science practice, and LEAs suggested redesign pathways such that the tool could complement their domain expertise.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {301},
numpages = {20},
keywords = {algorithmic crime mapping, human-AI decision-making, problem formulation, public sector algorithms},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3706599.3719775,
author = {Shimizu, Shinya and Ota, Airi and Nakane, Ai},
title = {Reviving Intentional Facial Expressions: an Interface for ALS Patients using Brain Decoding and Image-Generative AI},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719775},
doi = {10.1145/3706599.3719775},
abstract = {This study proposes an interface to enable non-verbal emotional communication for amyotrophic lateral sclerosis (ALS) patients with impaired face movement. By integrating EEG decoding and image-generative AI, the interface generates facial expressions based on decoding voluntary intentions rather than emotions, addressing privacy concerns. To enhance training data acquisition for decoding 17 action units (AUs), we developed a personalized approach using latent facial expression spaces and six transition animations for imitation. Experiments with participants without ALS validated the proposed method, and subsequent tests with one ALS patient showed an average AU decoding accuracy of 0.20, comparable to participants without ALS (0.23). In real-time generation tests, the ALS patient achieved a correlation of 0.40 between intended and decoded facial expressions, outperforming participants without ALS (0.30). While less accurate than invasive methods, this non-invasive, task-driven approach offers an effective solution for Augmentative and Alternative Communication in patients with severe neurological conditions.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {464},
numpages = {10},
keywords = {facial expression, emotional communication, brain computer interface (BCI), electroencephalogram (EEG), brain decoding, amyotrophic lateral sclerosis (ALS)},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3383313.3418436,
author = {Zhou, Michelle},
title = {“You Really Get Me”: Conversational AI Agents That Can Truly Understand and Help Users},
year = {2020},
isbn = {9781450375832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383313.3418436},
doi = {10.1145/3383313.3418436},
abstract = {Have you watched the movie Her? Have you ever wondered or wished to have an AI companion like Samantha, who could tell you what you really are, whom your best teammate may be, and which career path would be best for you? In this talk, Michelle will present a framework for building hyper-personalized, conversational Artificial Intelligent (AI) agents who can deeply understand users and responsibly guide user behavior in both virtual and real world. Through live demos, she will highlight two technical advances of the framework: (1) evidence-based personality inference and (2) model-based conversation generation. Michelle will discuss real-world applications of these agents and the wider implications of enabling hyper-personalized conversational AI agents for businesses and individuals.},
booktitle = {Proceedings of the 14th ACM Conference on Recommender Systems},
pages = {3},
numpages = {1},
location = {Virtual Event, Brazil},
series = {RecSys '20}
}

@inproceedings{10.1145/3550340.3564227,
author = {Voleti, Vikram and Oreshkin, Boris and Bocquelet, Florent and Harvey, F\'{e}lix and M\'{e}nard, Louis-Simon and Pal, Christopher},
title = {SMPL-IK: Learned Morphology-Aware Inverse Kinematics for AI Driven Artistic Workflows},
year = {2022},
isbn = {9781450394659},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550340.3564227},
doi = {10.1145/3550340.3564227},
abstract = {Inverse Kinematics (IK) systems are often rigid with respect to their input character, thus requiring user intervention to be adapted to new skeletons. In this paper we aim at creating a flexible, learned IK solver applicable to a wide variety of human morphologies. We extend a state-of-the-art machine learning IK solver to operate on the well known Skinned Multi-Person Linear model (SMPL). We call our model SMPL-IK, and show that when integrated into real-time 3D software, this extended system opens up opportunities for defining novel AI-assisted animation workflows. For example, when chained with existing pose estimation algorithms, SMPL-IK accelerates posing by allowing users to bootstrap 3D scenes from 2D images while allowing for further editing. Additionally, we propose a novel SMPL Shape Inversion mechanism (SMPL-SI) to map arbitrary humanoid characters to the SMPL space, allowing artists to leverage SMPL-IK on custom characters. In addition to qualitative demos showing proposed tools, we present quantitative SMPL-IK baselines on the H36M and AMASS datasets. Our code is publicly available https://github.com/boreshkinai/smpl-ik.},
booktitle = {SIGGRAPH Asia 2022 Technical Communications},
articleno = {1},
numpages = {7},
keywords = {pose authoring, learned inverse kinematics, SMPL, 3D animation},
location = {Daegu, Republic of Korea},
series = {SA '22}
}

@inproceedings{10.1145/3526113.3545613,
author = {Chang, Ruei-Che and Ting, Chao-Hsien and Hung, Chia-Sheng and Lee, Wan-Chen and Chen, Liang-Jin and Chao, Yu-Tzu and Chen, Bing-Yu and Guo, Anhong},
title = {OmniScribe: Authoring Immersive Audio Descriptions for 360° Videos},
year = {2022},
isbn = {9781450393201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526113.3545613},
doi = {10.1145/3526113.3545613},
abstract = {Blind people typically access videos via audio descriptions (AD) crafted by sighted describers who comprehend, select, and describe crucial visual content in the videos. 360° video is an emerging storytelling medium that enables immersive experiences that people may not possibly reach in everyday life. However, the omnidirectional nature of 360° videos makes it challenging for describers to perceive the holistic visual content and interpret spatial information that is essential to create immersive ADs for blind people. Through a formative study with a professional describer, we identified key challenges in describing 360° videos and iteratively designed OmniScribe, a system that supports the authoring of immersive ADs for 360° videos. OmniScribe uses AI-generated content-awareness overlays for describers to better grasp 360° video content. Furthermore, OmniScribe enables describers to author spatial AD and immersive labels for blind users to consume the videos immersively with our mobile prototype. In a study with 11 professional and novice describers, we demonstrated the value of OmniScribe in the authoring workflow; and a study with 8 blind participants revealed the promise of immersive AD over standard AD for 360° videos. Finally, we discuss the implications of promoting 360° video accessibility.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {15},
numpages = {14},
keywords = {visual impairment, virtual reality, sonification, multimedia, mobile, computer vision, audio description, accessibility, Blind, 360° video},
location = {Bend, OR, USA},
series = {UIST '22}
}

@proceedings{10.1145/3701551,
title = {WSDM '25: Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining},
year = {2025},
isbn = {9798400713293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 18th ACM International Conference on Web and Data Mining, this time taking place in Hannover, Germany.Many volunteers have helped to provide excellent content for the conference, most of all the 3 PC Chairs, Meeyoung Cha (Max Planck Institute for Security and Privacy in Germany), Francine Moens (KU Leuven, Belgium) and Marc Najork (Google DeepMind, USA), who selected more than 100 papers from over 600 submissions for presentation at the conference.This exciting WSDM 2025 main conference program was extended by interesting demonstrations like Lightning IR for fine-tuning and inference of transformer-based language models for information retrieval, WildlifeLookup, a chatbot designed to facilitate wildlife management and Ventana a la Verdad, a chatbot for navigating Colombian civil conflict archives. Several workshops ranging from language models to trust and verification on the Web complemented this program and provided important opportunities for the discussion of exciting new ideas and approaches.WSDM Cup 2025 challenged more than 1000 competitors to create useful and efficient auto-rater models that can accurately reflect human evaluations. This competition focused on the creation of multilingual auto-rater models using chatbot arena data, with exciting solutions. The WSDM 2025 Industry Day featured talks from leading companies on practical applications of web search and data mining. Some notable examples included zero-shot image moderation in Google Ads, advancing Voice AI for E-commerce at Amazon, and Fact-checking of multilingual podcasts. Finally, WSDM Day talks focused on medical research questions and topics, applying language models, knowledge graphs and other AI based approaches.For participants aiming to enter new research areas, the WSDM 2025 tutorials covered a broad range of topics, including Robust Information Retrieval, Building Trustworthy AI Models for Medicine, Unifying Bias and Unfairness in Information Retrieval in the LLM Era, Advances in Vector Search and quite a few more. And finally, three excellent invited speakers (Slav Petrov from Google DeepMind and co-lead on Gemini, Roberto Navigli from Sapienza University and Babelscape and Mario Fritz from CISPA) provided exciting insights into the state of AI, about what large language models really understand, as well as privacy and security aspects of neural models.},
location = {Hannover, Germany}
}

@article{10.1145/3303706,
author = {Mubin, Omar and Wadibhasme, Kewal and Jordan, Philipp and Obaid, Mohammad},
title = {Reflecting on the Presence of Science Fiction Robots in Computing Literature},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
url = {https://doi.org/10.1145/3303706},
doi = {10.1145/3303706},
abstract = {Depictions of robots and AIs in popular science fiction movies and shows have the potential to showcase visions of Human-Robot Interaction (HRI) to the general public and computer science researchers alike. In contrast, studies on the referral, usage, and appropriation of these portrayals by computer scientists in their research publications is an academic void at present. However, such investigations are critical to better understand the potential utility and latent shortcomings of science fiction robots for future HRI research, innovation, and education. To address this research gap, this study investigates the overall presence, nature, and frequency of referrals of 18 popular science fiction robots in the Association for Computing Machinery (ACM) Digital Library. These robots were either portrayed in various movies and subsequently inducted into the Robot Hall of Fame, created by Carnegie Mellon University in 2003, or were top-ranked in a user-curated Internet Movie Database (IMDB) list. To do so, we performed full-text search and retrieval queries of all 18 robots in the ACM Digital Library. In total, we identified 121 relevant mentions, across 102 individual publications, in a time span from 1973 to 2017. These 121 mentions were then qualitatively analysed to determine the nature of the robot mentions. Our results indicate that the robot attributes of voice or dialogue were emerging as a popularly mentioned element. In addition, we find that research papers of philosophical nature mention sci-fi robots more frequently than papers of technical or theoretical nature. We also observe that the dystopian element of science fiction is under-utilised, with the majority of robot mentions exhibiting neutral or utopian characteristics. In conclusion, we speculate on our results and present possible avenues of future HRI research on the topic.},
journal = {J. Hum.-Robot Interact.},
month = mar,
articleno = {5},
numpages = {25},
keywords = {visions of the future, text mining, science fiction, robots, computing literature, AI, ACM digital library}
}

@inproceedings{10.1145/3313831.3376701,
author = {Cheema, Noshaba and Frey-Law, Laura A. and Naderi, Kourosh and Lehtinen, Jaakko and Slusallek, Philipp and H\"{a}m\"{a}l\"{a}inen, Perttu},
title = {Predicting Mid-Air Interaction Movements and Fatigue Using Deep Reinforcement Learning},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376701},
doi = {10.1145/3313831.3376701},
abstract = {A common problem of mid-air interaction is excessive arm fatigue, known as the "Gorilla arm" effect. To predict and prevent such problems at a low cost, we investigate user testing of mid-air interaction without real users, utilizing biomechanically simulated AI agents trained using deep Reinforcement Learning (RL). We implement this in a pointing task and four experimental conditions, demonstrating that the simulated fatigue data matches human fatigue data. We also compare two effort models: 1) instantaneous joint torques commonly used in computer animation and robotics, and 2) the recent Three Compartment Controller (3CC-) model from biomechanical literature. 3CC- yields movements that are both more efficient and relaxed, whereas with instantaneous joint torques, the RL agent can easily generate movements that are quickly tiring or only reach the targets slowly and inaccurately. Our work demonstrates that deep RL combined with the 3CC- provides a viable tool for predicting both interaction movements and user experiencein silico, without users.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {biomechanical simulation, computational interaction, reinforcement learning, user modeling},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3379336.3381478,
author = {Aylett, Matthew P. and Vazquez-Alvarez, Yolanda},
title = {Voice Puppetry: Speech Synthesis Adventures in Human Centred AI},
year = {2020},
isbn = {9781450375139},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379336.3381478},
doi = {10.1145/3379336.3381478},
abstract = {State-of-the-art speech synthesis owes much to modern AI machine learning, with recurrent neural networks becoming the new standard. However, how you say something is just as important as what you say. If we draw inspiration from human dramatic performance, ideas such as artistic direction can help us design interactive speech synthesis systems which can be finely controlled by a human voice. This "voice puppetry" has many possible applications from film dubbing to the pre-creation of prompts for a conversational agent. Previous work in voice puppetry has raised the question of how such a system should work and how we might interact with it. Here, we share the results of a focus group discussing voice puppetry and responding to a voice puppetry demo. Results highlight a main challenge in user-centred AI: where is the trade-off between control and automation? and how may users control this trade-off?},
booktitle = {Companion Proceedings of the 25th International Conference on Intelligent User Interfaces},
pages = {108–109},
numpages = {2},
keywords = {personification, social robots, speech synthesis},
location = {Cagliari, Italy},
series = {IUI '20 Companion}
}

@inproceedings{10.1145/3664647.3681697,
author = {Liu, Rui and Hu, Yifan and Ren, Yi and Yin, Xiang and Li, Haizhou},
title = {Generative Expressive Conversational Speech Synthesis},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681697},
doi = {10.1145/3664647.3681697},
abstract = {Conversational Speech Synthesis (CSS) aims to express a target utterance with the proper speaking style in a user-agent conversation setting. Existing CSS methods employ effective multi-modal context modeling techniques to achieve empathy understanding and expression. However, they often need to design complex network architectures and meticulously optimize the modules within them. In addition, due to the limitations of small-scale datasets containing scripted recording styles, they often fail to simulate real natural conversational styles. To address the above issues, we propose a novel generative expressive CSS system, termed GPT-Talker.We transform the multimodal information of the multi-turn dialogue history into discrete token sequences and seamlessly integrate them to form a comprehensive user-agent dialogue context. Leveraging the power of GPT, we predict the token sequence, that includes both semantic and style knowledge, of response for the agent. After that, the expressive conversational speech is synthesized by the conversation-enriched VITS to deliver feedback to the user.Furthermore, we propose a large-scale Natural CSS Dataset called NCSSD, that includes both naturally recorded conversational speech in improvised styles and dialogues extracted from TV shows. It encompasses both Chinese and English languages, with a total duration of 236 hours. We conducted comprehensive experiments on the reliability of the NCSSD and the effectiveness of our GPT-Talker. Both subjective and objective evaluations demonstrate that our model outperforms other state-of-the-art CSS systems significantly in terms of naturalness and expressiveness. The Code, Dataset, and Pre-trained Model are available at: https://github.com/AI-S2-Lab/GPT-Talker.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {4187–4196},
numpages = {10},
keywords = {conversational speech synthesis (css), expressiveness, gpt, user-agent conversation},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/3498361.3538784,
author = {Jangid, Nitesh Kumar and Gupta, Mukesh Kumar},
title = {Protecting software design in cloud using AWS IoT},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538784},
doi = {10.1145/3498361.3538784},
abstract = {Emerging technologies like Cloud Computing, Mobile Computing, IoT, AI, etc. are gaining momentum in the IT community. These novel technologies have changed the way to design secure and expert IT Systems for the future. Over the last decade scripting languages like PHP, Python, etc. are being used to develop applications due to ease of product development with moderate IT skills. Software design is not well protected for scripting languages due to the absence of source code translation. This risk is multifold if the IT System is deployed in a public cloud. The IoT Cloud services like AWS IoT may be used to redesign the security of IT Sytems by incorporating cost-effective and powerful IoT Computers like Raspberry Pi. We are proposing here a solution to protect software design by decoupling application code design and database design using hashing for database-name, table-name, and attribute-name of the application database. Security keys used in hashing are stored on a credit card size IoT Computer on business premises. IoT computer uses a separate AWS IoT secure channel to share the security keys on-demand in the cloud IT System when user logged in and keys are saved in session variable only. This proposal may be considered as adding client-trusted hardware into the public cloud.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {561–562},
numpages = {2},
keywords = {software design, raspberry Pi, hashing, IoT computer, IT system, AWS IoT},
location = {Portland, Oregon},
series = {MobiSys '22}
}

@inproceedings{10.1145/3441852.3471211,
author = {Ghai, Bhavya and Mueller, Klaus},
title = {Fluent: An AI Augmented Writing Tool for People who Stutter},
year = {2021},
isbn = {9781450383066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3441852.3471211},
doi = {10.1145/3441852.3471211},
abstract = {Stuttering is a speech disorder which impacts the personal and professional lives of millions of people worldwide. To save themselves from stigma and discrimination, people who stutter (PWS) may adopt different strategies to conceal their stuttering. One of the common strategies is word substitution where an individual avoids saying a word they might stutter on and use an alternative instead. This process itself can cause stress and add more burden. In this work, we present Fluent, an AI augmented writing tool which assists PWS in writing scripts which they can speak more fluently. Fluent embodies a novel active learning based method of identifying words an individual might struggle pronouncing. Such words are highlighted in the interface. On hovering over any such word, Fluent presents a set of alternative words which have similar meaning but are easier to speak. The user is free to accept or ignore these suggestions. Based on such user interaction (feedback), Fluent continuously evolves its classifier to better suit the personalized needs of each user. We evaluated our tool by measuring its ability to identify difficult words for 10 simulated users. We found that our tool can identify difficult words with a mean accuracy of over 80% in under 20 interactions and it keeps improving with more feedback. Our tool can be beneficial for certain important life situations like giving a talk, presentation, etc. The source code for this tool has been made publicly accessible at github.com/bhavyaghai/Fluent.},
booktitle = {Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {26},
numpages = {8},
keywords = {Accessible computing, Active learning, Stammering, Stuttering},
location = {Virtual Event, USA},
series = {ASSETS '21}
}

@inproceedings{10.1145/3434073.3444657,
author = {Das, Devleena and Banerjee, Siddhartha and Chernova, Sonia},
title = {Explainable AI for Robot Failures: Generating Explanations that Improve User Assistance in Fault Recovery},
year = {2021},
isbn = {9781450382892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434073.3444657},
doi = {10.1145/3434073.3444657},
abstract = {With the growing capabilities of intelligent systems, the integration of robots in our everyday life is increasing. However, when interacting in such complex human environments, the occasional failure of robotic systems is inevitable. The field of explainable AI has sought to make complex-decision making systems more interpretable but most existing techniques target domain experts. On the contrary, in many failure cases, robots will require recovery assistance from non-expert users. In this work, we introduce a new type of explanation, εerr, that explains the cause of an unexpected failure during an agent's plan execution to non-experts. In order for error explanations to be meaningful, we investigate what types of information within a set of hand-scripted explanations are most helpful to non-experts for failure and solution identification. Additionally, we investigate how such explanations can be autonomously generated, extending an existing encoder-decoder model, and generalized across environments. We investigate such questions in the context of a robot performing a pick-and-place manipulation task in the home environment. Our results show that explanations capturing the context of a failure and history of past actions, are the most effective for failure and solution identification among non-experts. Furthermore, through a second user evaluation, we verify that our model-generated explanations can generalize to an unseen office environment, and are just as effective as the hand-scripted explanations.},
booktitle = {Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {351–360},
numpages = {10},
keywords = {fault recovery, explainable ai},
location = {Boulder, CO, USA},
series = {HRI '21}
}

@inproceedings{10.1145/3534678.3539188,
author = {Li, Zhuliu and Wang, Yiming and Yan, Xiao and Meng, Weizhi and Li, Yanen and Yang, Jaewon},
title = {TaxoTrans: Taxonomy-Guided Entity Translation},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539188},
doi = {10.1145/3534678.3539188},
abstract = {Taxonomies describe the definitions of entities, entities' attributes and the relations among the entities, and thus play an important role in building a knowledge graph. In this paper, we tackle the task of taxonomy entity translation, which is to translate the names of taxonomy entities in a source language to a target language. The translations then can be utilized to build a knowledge graph in the target language. Despite its importance, taxonomy entity translation remains a hard problem for AI models due to two major challenges. One challenge is understanding the semantic context in very short entity names. Another challenge is having deep understanding for the domain where the knowledge graph is built.  We present TaxoTrans, a novel method for taxonomy entity translation that can capture the context in entity names and the domain knowledge in taxonomy. To achieve this, TaxoTrans creates a heterogeneous graph to connect entities, and formulates the entity name translation problem as link prediction in the heterogeneous graph: given a pair of entity names across two languages, TaxoTrans applies a graph neural network to determine whether they form a translation pair or not. Because of this graph, TaxoTrans can capture both the semantic context and the domain knowledge. Our offline experiments on LinkedIn's skill and title taxonomies show that by modeling semantic information and domain knowledge in the heterogeneous graph, TaxoTrans outperforms the state-of-the-art translation methods by ∼10%. Human annotation and A/B test results further demonstrate that the accurately translated entities significantly improves user engagements and advertising revenue at LinkedIn.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3279–3287},
numpages = {9},
keywords = {entity translation, graph neural network, knowledge graph, taxonomy},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3375627.3375839,
author = {Pasquale, Frank},
title = {Machines Judging Humans: The Promise and Perils of Formalizing Evaluative Criteria},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375839},
doi = {10.1145/3375627.3375839},
abstract = {Over the past decade, algorithmic accountability has become an important concern for social scientists, computer scientists, journalists, and lawyers [1]. Expos\'{e}s have sparked vibrant debates about algorithmic sentencing. Researchers have exposed tech giants showing women ads for lower-paying jobs, discriminating against the aged, deploying deceptive dark patterns to trick consumers into buying things, and manipulating users toward rabbit holes of extremist content. Public-spirited regulators have begun to address algorithmic transparency and online fairness, building on the work of legal scholars who have called for technological due process, platform neutrality, and nondiscrimination principles.This policy work is just beginning, as experts translate academic research and activist demands into statutes and regulations. Lawmakers are proposing bills requiring basic standards of algorithmic transparency and auditing. We are starting down on a long road toward ensuring that AI-based hiring practices and financial underwriting are not used if they have a disparate impact on historically marginalized communities. And just as this "first wave" of algorithmic accountability research and activism has targeted existing systems, an emerging "second wave" of algorithmic accountability has begun to address more structural concerns. Both waves will be essential to ensure a fairer, and more genuinely emancipatory, political economy of technology. Second wave work is particularly important when it comes to illuminating the promise &amp; perils of formalizing evaluative criteria.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {7},
numpages = {1},
keywords = {opportunity, inequality, fatml, emancipatory computing, egalitarianism, bias, anticipatory social research, ai for good, ai, accountability},
location = {New York, NY, USA},
series = {AIES '20}
}

